{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f848dbd5-0b55-41e0-8f22-a4d9234f6a54",
   "metadata": {},
   "source": [
    "## Train discriminator and generator at each epoch test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53854181-58fa-47d4-850a-9cde0a2ac179",
   "metadata": {},
   "source": [
    "## cAvatar architecture with cAvatar dataset starts from here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce68642c-1056-461f-9a7b-484a073ecbca",
   "metadata": {},
   "source": [
    "## Dataset read and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d98bc62a-f8a6-46c0-a4fa-d112907ecbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import math\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "# from smpl_torch import SMPLModel  # Import the SMPL model from your saved file\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "# # Define parameters for a smaller dataset\n",
    "window_size = 20  # Reduced window size for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0b232eea-6883-46cc-a161-870decc97fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = '/home/mkeya/cAvatar_dataset/touch_normalized.p'\n",
    "\n",
    "with open(folder_path, 'rb') as f:\n",
    "        data = pickle.load(f)  # Load the entire .p file\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d7667ec6-a6ca-474f-ab13-86233b41d129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type: <class 'numpy.ndarray'>\n",
      "data shape: (11075, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "print(f\"data type: {type(data)}\")\n",
    "\n",
    "print(f\"data shape: {data.shape}\")\n",
    "\n",
    "# frames = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e212f29b-486e-462c-a225-7ce4a4cf82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_frames_file = '/home/mkeya/VIBE/missing_frames.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6808297b-408c-4141-b38a-ba05148426d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read missing frames\n",
    "with open(missing_frames_file, 'r') as f:\n",
    "    missing_frames = set(int(line.strip()) for line in f)\n",
    "\n",
    "# Filter out missing frames from the data\n",
    "filtered_data = [frame for i, frame in enumerate(data) if i not in missing_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b67f7af8-d0b2-4550-9a52-2aff17cc71a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_data type: <class 'list'>\n",
      "filtered_data_array shape: (9143, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "print(f\"filtered_data type: {type(filtered_data)}\")\n",
    "\n",
    "filtered_data_array = np.array(filtered_data)\n",
    "\n",
    "print(f\"filtered_data_array shape: {filtered_data_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a3aee27d-c4ef-4f3c-963a-c2700515cdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9143"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = filtered_data_array\n",
    "\n",
    "len(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441ef83-4297-4b31-ae28-0bbbe1b18a55",
   "metadata": {},
   "source": [
    "## Teacher Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "70952ef1-962c-4582-bb84-1dc13c58aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "ground_truth_pose shape: torch.Size([9145, 72])\n",
      "ground_truth_shape shape: torch.Size([9145, 10])\n",
      "ground_truth_camera shape: torch.Size([9145, 3])\n",
      "ground_truth_vertices shape: torch.Size([9145, 6890, 3])\n",
      "ground_truth_joints shape: torch.Size([9145, 49, 3])\n"
     ]
    }
   ],
   "source": [
    "# Path to the JSON file saved by demo.py\n",
    "json_file_path = '/home/mkeya/VIBE/merged_vibe_results.json'  # Replace with the actual path to the JSON file\n",
    "\n",
    "# Load the JSON file with ground truth values\n",
    "with open(json_file_path, 'r') as f:\n",
    "    vibe_results = json.load(f)\n",
    "print('---------------')\n",
    "\n",
    "\n",
    "\n",
    "ground_truth_pose = torch.tensor(vibe_results['pose'], dtype=torch.float64)\n",
    "ground_truth_shape = torch.tensor(vibe_results['shape'], dtype=torch.float64)\n",
    "ground_truth_camera = torch.tensor(vibe_results['camera'], dtype=torch.float64)\n",
    "ground_truth_joints = torch.tensor(vibe_results['joints'], dtype=torch.float64)\n",
    "ground_truth_vertices = torch.tensor(vibe_results['vertices'], dtype=torch.float64)\n",
    "\n",
    "print(f\"ground_truth_pose shape: {ground_truth_pose.shape}\") # theta\n",
    "print(f\"ground_truth_shape shape: {ground_truth_shape.shape}\") # beta\n",
    "print(f\"ground_truth_camera shape: {ground_truth_camera.shape}\") # translation\n",
    "print(f\"ground_truth_vertices shape: {ground_truth_vertices.shape}\") # vertices\n",
    "print(f\"ground_truth_joints shape: {ground_truth_joints.shape}\") # joints taking all joints this time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc6ea3-05bd-44d6-b153-291bb5e9ca98",
   "metadata": {},
   "source": [
    "## Create Training input and target pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab2343-a1a3-41cc-a9ab-0fb5a208f48f",
   "metadata": {},
   "source": [
    "## Sliding window + Splitting the dataset into training, validation and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c6b573f1-a869-4331-b8d7-7c283f69e054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sequences: 9124\n",
      "train_sequences1 shape: 7299\n",
      "train_sequences len: 7299\n",
      "Training set shape: (7299, 20, 96, 96)\n",
      "Validation set shape: (912, 20, 96, 96)\n",
      "Testing set shape: (913, 20, 96, 96)\n",
      "ground_truth_pose_test shape: torch.Size([913, 72])\n"
     ]
    }
   ],
   "source": [
    "def extract_from_list(key, sequences): \n",
    "    sequences_list = [item[key] for item in sequences]\n",
    "    return sequences_list\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Create overlapping sequences using a reduced sliding window\n",
    "sequences = []\n",
    "for i in range(len(frames) - window_size + 1):\n",
    "    sequence = frames[i:i + window_size]  # Get 20 frames\n",
    "    \n",
    "\n",
    "    samples_X_y = {} # it's very important to make it empty here.\n",
    "    keys = ['sequence', 'pose', 'shape', 'camera', 'vertices', 'coco']\n",
    "    samples_X_y = dict.fromkeys(keys)\n",
    "    \n",
    "    # the following data is the learning target for 20th frame from VIBE output\n",
    "    # in python a[i] is the (i+1)th element in list a\n",
    "\n",
    "    samples_X_y['sequence']=np.array(sequence)\n",
    "    samples_X_y['pose']=ground_truth_pose[i+window_size-1:i+window_size]\n",
    "    samples_X_y['shape']=ground_truth_shape[i+window_size-1:i+window_size]\n",
    "    samples_X_y['camera']=ground_truth_camera[i+window_size-1:i+window_size]\n",
    "    samples_X_y['vertices']=ground_truth_vertices[i+window_size-1:i+window_size]\n",
    "    samples_X_y['coco']=ground_truth_joints[i+window_size-1:i+window_size]\n",
    "\n",
    "    sequences.append(samples_X_y) \n",
    "\n",
    "# # Step: Shuffle the dataset \n",
    "# random.seed(42)\n",
    "# random.shuffle(sequences)\n",
    "\n",
    "# Step 4: Split sequences into training, validation, and testing sets (80-10-10 split)\n",
    "num_sequences = len(sequences)\n",
    "print(f\"num_sequences: {num_sequences}\")\n",
    "train_end = int(num_sequences * 0.8)\n",
    "val_end = train_end + int(num_sequences * 0.1)\n",
    "\n",
    "# you are not randomizing the samples, just following the original orders of the continues frames\n",
    "# each sample in the list, is a dictionary\n",
    "train_sequences1 = sequences[:train_end]\n",
    "\n",
    "print(f\"train_sequences1 shape: {len(train_sequences1)}\")\n",
    "# print(f\"train_sequences1 [0]: {train_sequences1[0]}\")\n",
    "train_sequences = extract_from_list('sequence', train_sequences1)\n",
    "\n",
    "print(f\"train_sequences len: {len(train_sequences)}\")\n",
    "\n",
    "val_sequences1 = sequences[train_end:val_end]\n",
    "val_sequences = extract_from_list('sequence', val_sequences1)\n",
    "\n",
    "test_sequences1 = sequences[val_end:]\n",
    "test_sequences = extract_from_list('sequence', test_sequences1)\n",
    "\n",
    "\n",
    "# Dynamic extraction using globals()\n",
    "VIBE_keys = ['pose', 'shape', 'camera', 'vertices', 'coco']\n",
    "name_list = ['ground_truth_pose1','ground_truth_shape1','ground_truth_camera1','ground_truth_vertices1','ground_truth_coco1']\n",
    "\n",
    "for index, key in enumerate(VIBE_keys):\n",
    "    # print(index)\n",
    "    # print(key)\n",
    "    name1 = name_list[index]\n",
    "    name2 = name1 \n",
    "\n",
    "    # print(name2)\n",
    "    globals()[name2] = extract_from_list(key, train_sequences1)\n",
    "\n",
    "new_name_list = [item+'_train' for item in name_list]\n",
    "# Step 5: Print shapes to verify\n",
    "print(f\"Training set shape: {np.array(train_sequences).shape}\") \n",
    "print(f\"Validation set shape: {np.array(val_sequences).shape}\")\n",
    "print(f\"Testing set shape: {np.array(test_sequences).shape}\")\n",
    "\n",
    "# 4. Extract ground truth for training, validation, and testing\n",
    "ground_truth_pose_train = extract_from_list('pose', train_sequences1)\n",
    "ground_truth_pose_val = extract_from_list('pose', val_sequences1)\n",
    "ground_truth_pose_test = extract_from_list('pose', test_sequences1)\n",
    "\n",
    "ground_truth_shape_train = extract_from_list('shape', train_sequences1)\n",
    "ground_truth_shape_val = extract_from_list('shape', val_sequences1)\n",
    "ground_truth_shape_test = extract_from_list('shape', test_sequences1)\n",
    "\n",
    "ground_truth_camera_train = extract_from_list('camera', train_sequences1)\n",
    "ground_truth_camera_val = extract_from_list('camera', val_sequences1)\n",
    "ground_truth_camera_test = extract_from_list('camera', test_sequences1)\n",
    "\n",
    "ground_truth_vertices_train = extract_from_list('vertices', train_sequences1)\n",
    "ground_truth_vertices_val = extract_from_list('vertices', val_sequences1)\n",
    "ground_truth_vertices_test = extract_from_list('vertices', test_sequences1)\n",
    "\n",
    "ground_truth_coco_train = extract_from_list('coco', train_sequences1)\n",
    "ground_truth_coco_val = extract_from_list('coco', val_sequences1)\n",
    "ground_truth_coco_test = extract_from_list('coco', test_sequences1)\n",
    "\n",
    "# Convert to tensors if needed\n",
    "ground_truth_pose_train = torch.tensor(np.vstack(ground_truth_pose_train))\n",
    "ground_truth_pose_val = torch.tensor(np.vstack(ground_truth_pose_val))\n",
    "ground_truth_pose_test = torch.tensor(np.vstack(ground_truth_pose_test))\n",
    "\n",
    "ground_truth_shape_train = torch.tensor(np.vstack(ground_truth_shape_train))\n",
    "ground_truth_shape_val = torch.tensor(np.vstack(ground_truth_shape_val))\n",
    "ground_truth_shape_test = torch.tensor(np.vstack(ground_truth_shape_test))\n",
    "\n",
    "ground_truth_camera_train = torch.tensor(np.vstack(ground_truth_camera_train))\n",
    "ground_truth_camera_val = torch.tensor(np.vstack(ground_truth_camera_val))\n",
    "ground_truth_camera_test = torch.tensor(np.vstack(ground_truth_camera_test))\n",
    "\n",
    "ground_truth_vertices_train = torch.tensor(np.vstack(ground_truth_vertices_train))\n",
    "ground_truth_vertices_val = torch.tensor(np.vstack(ground_truth_vertices_val))\n",
    "ground_truth_vertices_test = torch.tensor(np.vstack(ground_truth_vertices_test))\n",
    "\n",
    "ground_truth_coco_train = torch.tensor(np.vstack(ground_truth_coco_train))\n",
    "ground_truth_coco_val = torch.tensor(np.vstack(ground_truth_coco_val))\n",
    "ground_truth_coco_test = torch.tensor(np.vstack(ground_truth_coco_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "328eaf66-0272-4409-9441-cbaa787da004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth_pose_train shape: torch.Size([7299, 72])\n",
      "ground_truth_coco_test shape: torch.Size([913, 49, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"ground_truth_pose_train shape: {ground_truth_pose_train.shape}\")\n",
    "\n",
    "print(f\"ground_truth_coco_test shape: {ground_truth_coco_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40ed17-aa8d-4cc7-a8ae-afebb9132e11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d63c6-b846-43fb-83d3-b75b80f78fbb",
   "metadata": {},
   "source": [
    "## Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "af1dcae8-9177-4268-9b1d-e1f5644ed288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ResNet-18 Modification\n",
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        # Adjusting the first convolution layer for 1 channel input (grayscale or single channel images)\n",
    "        resnet.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-2])  # Removing fully connected layers\n",
    "        self.fc = nn.Linear(512, output_dim)  # The output from ResNet is reduced to the required output dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = torch.mean(x, dim=(2, 3))  # Global average pooling to reduce spatial dimensions\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Generates sinusoidal positional encoding matrix of shape (seq_len, d_model).\n",
    "    \"\"\"\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1).float()  # Shape: (seq_len, 1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe  # Shape: (seq_len, d_model)\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.norm1(src)\n",
    "        attn_output, _ = self.self_attn(src2, src2, src2)\n",
    "        src = src + self.dropout(attn_output)\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout(self.linear2(F.relu(self.linear1(src2))))\n",
    "        return src\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, nhead, dim_feedforward, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "        return src\n",
    "\n",
    "# The main Student Model that includes all components\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, output_dim=86, num_layers=6, d_model=256, nhead=4, dim_feedforward=2048, dropout=0.1, max_seq_len=20):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.resnet_model = CustomResNet18(d_model)\n",
    "        self.positional_encoding = positional_encoding(seq_len=max_seq_len, d_model=d_model)\n",
    "        self.transformer_encoder = TransformerEncoder(num_layers, d_model, nhead, dim_feedforward, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)  # Output layer to generate joint angles, shapes, etc.\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=d_model)  # Connect ResNet18 and Transformer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: ResNet for each frame in the sequence\n",
    "        # x is of shape (batch_size, seq_len, 96, 96), reshape to (batch_size * seq_len, 1, 96, 96)\n",
    "        batch_size, seq_len, h, w = x.shape\n",
    "        x_reshaped = x.view(batch_size * seq_len, 1, h, w)\n",
    "        resnet_output = self.resnet_model(x_reshaped)  # Output shape: (batch_size * seq_len, 256)\n",
    "        \n",
    "        # Reshape the output back to (batch_size, seq_len, 256)\n",
    "        resnet_output = resnet_output.view(batch_size, seq_len, -1)\n",
    "\n",
    "        # Step 2: Linear layer after ResNet to match the transformer input size\n",
    "        final_output = self.fc_layer(resnet_output)  # Shape: (batch_size, seq_len, 256)\n",
    "\n",
    "        # Step 3: Add positional encoding\n",
    "        batch_size, seq_len, d_model = final_output.shape\n",
    "        positional_encoding = self.positional_encoding[:seq_len, :].unsqueeze(0).repeat(batch_size, 1, 1)  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        final_output = final_output.to('cuda:0')\n",
    "        positional_encoding = positional_encoding.to('cuda:0')\n",
    "        \n",
    "        final_output_with_pe = final_output + positional_encoding  # Add positional encoding to the final output\n",
    "\n",
    "        # Step 4: Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(final_output_with_pe)  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Step 5: Use the last time step's embedding to predict the 21st frame\n",
    "        last_embedding = transformer_output[:, -1, :]  # Shape: (batch_size, d_model)\n",
    "        output_21st_frame = self.output_layer(last_embedding)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # Step 6: Separate Beta, Theta, and Translation\n",
    "        beta_student = output_21st_frame[..., :10]\n",
    "        theta_student = output_21st_frame[..., 10:82]\n",
    "        translation_student = output_21st_frame[..., 82:85]\n",
    "\n",
    "        return beta_student, theta_student, translation_student\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583a2be-047d-4b68-bcba-5767fc6c8688",
   "metadata": {},
   "source": [
    "## Initiate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4fa004bf-a697-429d-819e-5b5ae7b67cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mean Absolute Error loss function\n",
    "mae_loss = nn.L1Loss()\n",
    "\n",
    "# Instantiate the model\n",
    "student_model = StudentModel()\n",
    "\n",
    "# Move model to device (e.g., GPU if available)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "student_model = student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "56c6041b-b1b1-48cc-8c7b-817b0c084d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sequences shape: torch.Size([7299, 20, 96, 96])\n",
      "test_sequences shape: torch.Size([913, 20, 96, 96])\n",
      "num_samples: 7299\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_sequences is already a tensor\n",
    "train_sequences = torch.tensor(train_sequences, dtype=torch.float32) # (num_windows, window_size, height, width) = 7299, 20, 96, 96\n",
    "test_sequences = torch.tensor(test_sequences, dtype=torch.float32)\n",
    "\n",
    "batch_size = 32\n",
    "num_samples = train_sequences.shape[0]\n",
    "\n",
    "print(f\"train_sequences shape: {train_sequences.shape}\")\n",
    "print(f\"test_sequences shape: {test_sequences.shape}\")\n",
    "\n",
    "print(f\"num_samples: {num_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4867ee-8f9a-47ba-9c8a-2c11e57d8985",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "45b0fcf9-6c53-46a7-b8bf-96455f995879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator hidden layer weights dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_joints=49, hidden_dim=256, dropout_prob=0.4): # num of joints is 49 not 24   \n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(num_joints * 3, hidden_dim),  # Flattened input size\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_prob)  # Apply dropout here\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),  # Outputs a single value per input\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, joints):\n",
    "        # Flatten the input tensor\n",
    "        joints_flat = joints.view(joints.size(0), -1)  # Shape: [batch_size, num_joints * 3]\n",
    "        hidden_output = self.hidden_layer(joints_flat)\n",
    "        output = self.output_layer(hidden_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the discriminator\n",
    "discriminator = Discriminator(num_joints=49, dropout_prob=0.4).to(device) # working with 49 joints\n",
    "\n",
    "d_lr = 8e-5\n",
    "# Discriminator training optimizer\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=d_lr, weight_decay=1e-4)\n",
    "\n",
    "# Define the Binary Cross-Entropy Loss\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958522a0-e8c9-4a05-ad29-1418a2dd6fd4",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2afd8cb0-1bb5-4a68-9c65-e1fc997893c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, student_model, discriminator, test_sequences, ground_truth_shape_test, \n",
    "               ground_truth_pose_test, ground_truth_camera_test, ground_truth_vertices_test, \n",
    "               ground_truth_coco_test, batch_start, batch_end, batch_size, lambda_adv=0.05, lambda_ca=1.0):\n",
    "    \"\"\"\n",
    "    Function to test the model and return total loss and discriminator loss for a single batch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    student_model.eval()\n",
    "    discriminator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract batch of sliding windows for testing\n",
    "        batch_windows = test_sequences[batch_start:batch_end].to(device)\n",
    "        \n",
    "        # Get the ground truth for the current batch\n",
    "        ground_truth_shape_batch = ground_truth_shape_test[batch_start:batch_end].to(device)\n",
    "        ground_truth_pose_batch = ground_truth_pose_test[batch_start:batch_end].to(device)\n",
    "        ground_truth_camera_batch = ground_truth_camera_test[batch_start:batch_end].to(device)\n",
    "        ground_truth_vertices_batch = ground_truth_vertices_test[batch_start:batch_end].to(device)\n",
    "        ground_truth_joints_batch = ground_truth_coco_test[batch_start:batch_end].to(device)\n",
    "\n",
    "        if batch_windows.shape[0] != batch_size:\n",
    "            return None, None  # Skip if batch size mismatch\n",
    "\n",
    "        # Get the beta, theta, and translation from the student model\n",
    "        beta_student, theta_student, translation_student = student_model(batch_windows)\n",
    "\n",
    "\n",
    "        # Forward pass through the SMPL model\n",
    "        all_vertices = []\n",
    "        all_joints = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            beta = beta_student[b].to(device).to(torch.float32)          # Shape: (10,)\n",
    "            theta = theta_student[b].to(device).to(torch.float32)        # Shape: (72,)\n",
    "            translation = translation_student[b].to(device).to(torch.float32)  # Shape: (3,) \n",
    "\n",
    "            # Ensure shapes of inputs are correct\n",
    "            betas = beta.unsqueeze(0) if beta.dim() == 1 else beta  # Shape: (1, 10)\n",
    "            body_pose = theta[3:].unsqueeze(0) if theta[3:].dim() == 1 else theta[3:]  # Shape: (1, 69)\n",
    "            global_orient = theta[:3].unsqueeze(0) if theta[:3].dim() == 1 else theta[:3]  # Shape: (1, 3)\n",
    "            transl = translation.unsqueeze(0) if translation.dim() == 1 else translation  # Shape: (1, 3)\n",
    "\n",
    "            \n",
    "            # Forward pass through SMPL model\n",
    "            output = smpl_model(\n",
    "                betas=betas,\n",
    "                body_pose=body_pose,\n",
    "                global_orient=global_orient,\n",
    "                transl=transl\n",
    "            )\n",
    "        \n",
    "            # Extract vertices and joints\n",
    "            vertices = output.vertices  # Shape: (1, 6890, 3)\n",
    "            joints = output.joints      # Shape: (1, 49, 3)\n",
    "            \n",
    "            # Collect vertices and joints for this frame\n",
    "            all_vertices.append(vertices.cpu().detach().numpy())\n",
    "            all_joints.append(joints.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        all_vertices = np.array(all_vertices)  # Shape: (batch_size, 1, num_vertices, 3)\n",
    "        all_joints = np.array(all_joints)      # Shape: (batch_size, 1, 49, 3)\n",
    "        \n",
    "\n",
    "\n",
    "        all_vertices = all_vertices.squeeze(1)  # Shape: (batch_size, num_vertices, 3)\n",
    "        all_joints = all_joints.squeeze(1)      # Shape: (batch_size, 49, 3)\n",
    "\n",
    "\n",
    "        \n",
    "        # Convert SMPL output to torch tensors (if needed)\n",
    "        all_vertices_torch = torch.tensor(all_vertices, device=device)  # Shape: (32, 6890, 3)\n",
    "        all_joints_torch = torch.tensor(all_joints, device=device)      # Shape: (32, 24, 3)\n",
    "  \n",
    "\n",
    "        # Adversarial loss labels\n",
    "        real_outputs = discriminator(ground_truth_joints_batch.to(device).to(torch.float32))\n",
    "        fake_outputs = discriminator(all_joints_torch.detach().to(device).to(torch.float32))\n",
    "        d_real_loss = -torch.mean(torch.log(real_outputs + 1e-8))\n",
    "        d_fake_loss = -torch.mean(torch.log(1 - fake_outputs + 1e-8))\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "        discriminator_loss_test.append(d_loss.item())\n",
    "\n",
    "        # Calculate MAE losses\n",
    "        beta_loss = mae_loss(beta_student, ground_truth_shape_batch)\n",
    "        camera_loss = mae_loss(translation_student, ground_truth_camera_batch)\n",
    "        pose_loss = mae_loss(theta_student, ground_truth_pose_batch)\n",
    "        vertices_loss = mae_loss(all_vertices_torch, ground_truth_vertices_batch)\n",
    "        joints_loss = mae_loss(all_joints_torch, ground_truth_joints_batch)\n",
    "\n",
    "        # Compute total loss for the batch\n",
    "        loss_ca = (torch.mean(beta_loss) + torch.mean(camera_loss) + \n",
    "                   torch.mean(pose_loss) + torch.mean(vertices_loss) + torch.mean(joints_loss))\n",
    "        \n",
    "\n",
    "        adv_loss = -torch.mean(torch.log(discriminator(all_joints_torch.to(torch.float32)) + 1e-8))  # L_adv\n",
    "        \n",
    "        # total_loss = lambda_ca * loss_ca + lambda_adv * adv_loss\n",
    "        total_loss = loss_ca\n",
    "\n",
    "        # return d_loss, total_loss\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666c8d6-954a-4e1f-b102-6a80b4a48fab",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a701de1b-312f-4e1e-9386-8306ac3f213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the VIBE directory to the Python path\n",
    "vibe_root = os.path.abspath(\"/home/mkeya/VIBE\")\n",
    "if vibe_root not in sys.path:\n",
    "    sys.path.append(vibe_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "88c24a69-becd-487e-9e90-dbc70bfecff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_windows: 7299\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Epoch 1, Batch 1, loss_ca: 3.2931, adv_loss: 0.2464\n",
      "Epoch 1, Batch 2, loss_ca: 9.8247, adv_loss: 0.2368\n",
      "Epoch 1, Batch 3, loss_ca: 9.2984, adv_loss: 0.1594\n",
      "Epoch 1, Batch 4, loss_ca: 5.4381, adv_loss: 0.2189\n",
      "Epoch 1, Batch 5, loss_ca: 4.5873, adv_loss: 0.2260\n",
      "Epoch 1, Batch 6, loss_ca: 4.6332, adv_loss: 0.2717\n",
      "Epoch 1, Batch 7, loss_ca: 3.8030, adv_loss: 0.2275\n",
      "Epoch 1, Batch 8, loss_ca: 3.9623, adv_loss: 0.2216\n",
      "Epoch 1, Batch 9, loss_ca: 3.3026, adv_loss: 0.2185\n",
      "Epoch 1, Batch 10, loss_ca: 3.5805, adv_loss: 0.2532\n",
      "Epoch 1, Batch 11, loss_ca: 3.4051, adv_loss: 0.2518\n",
      "Epoch 1, Batch 12, loss_ca: 3.0822, adv_loss: 0.2451\n",
      "Epoch 1, Batch 13, loss_ca: 2.5051, adv_loss: 0.2614\n",
      "Epoch 1, Batch 14, loss_ca: 2.5838, adv_loss: 0.2705\n",
      "Epoch 1, Batch 15, loss_ca: 2.8796, adv_loss: 0.2717\n",
      "Epoch 1, Batch 16, loss_ca: 3.0299, adv_loss: 0.2490\n",
      "Epoch 1, Batch 17, loss_ca: 3.2745, adv_loss: 0.2570\n",
      "Epoch 1, Batch 18, loss_ca: 2.7380, adv_loss: 0.2874\n",
      "Epoch 1, Batch 19, loss_ca: 2.5439, adv_loss: 0.2835\n",
      "Epoch 1, Batch 20, loss_ca: 2.9565, adv_loss: 0.2764\n",
      "Epoch 1, Batch 21, loss_ca: 2.6864, adv_loss: 0.2809\n",
      "Epoch 1, Batch 22, loss_ca: 3.0345, adv_loss: 0.3035\n",
      "Epoch 1, Batch 23, loss_ca: 3.0311, adv_loss: 0.3297\n",
      "Epoch 1, Batch 24, loss_ca: 2.5970, adv_loss: 0.3290\n",
      "Epoch 1, Batch 25, loss_ca: 2.9899, adv_loss: 0.3023\n",
      "Epoch 1, Batch 26, loss_ca: 3.5401, adv_loss: 0.2967\n",
      "Epoch 1, Batch 27, loss_ca: 3.4163, adv_loss: 0.3384\n",
      "Epoch 1, Batch 28, loss_ca: 2.7761, adv_loss: 0.3341\n",
      "Epoch 1, Batch 29, loss_ca: 2.8574, adv_loss: 0.3559\n",
      "Epoch 1, Batch 30, loss_ca: 3.0372, adv_loss: 0.3488\n",
      "Epoch 1, Batch 31, loss_ca: 2.5758, adv_loss: 0.3305\n",
      "Epoch 1, Batch 32, loss_ca: 2.3148, adv_loss: 0.3452\n",
      "Epoch 1, Batch 33, loss_ca: 2.5283, adv_loss: 0.3578\n",
      "Epoch 1, Batch 34, loss_ca: 2.4619, adv_loss: 0.3255\n",
      "Epoch 1, Batch 35, loss_ca: 2.4406, adv_loss: 0.3493\n",
      "Epoch 1, Batch 36, loss_ca: 2.7473, adv_loss: 0.3977\n",
      "Epoch 1, Batch 37, loss_ca: 2.8835, adv_loss: 0.3942\n",
      "Epoch 1, Batch 38, loss_ca: 2.5203, adv_loss: 0.3765\n",
      "Epoch 1, Batch 39, loss_ca: 2.3727, adv_loss: 0.3447\n",
      "Epoch 1, Batch 40, loss_ca: 2.3760, adv_loss: 0.3572\n",
      "Epoch 1, Batch 41, loss_ca: 2.5519, adv_loss: 0.3714\n",
      "Epoch 1, Batch 42, loss_ca: 2.6473, adv_loss: 0.4148\n",
      "Epoch 1, Batch 43, loss_ca: 2.4777, adv_loss: 0.4043\n",
      "Epoch 1, Batch 44, loss_ca: 2.4284, adv_loss: 0.3440\n",
      "Epoch 1, Batch 45, loss_ca: 2.1608, adv_loss: 0.3695\n",
      "Epoch 1, Batch 46, loss_ca: 2.3387, adv_loss: 0.3851\n",
      "Epoch 1, Batch 47, loss_ca: 2.5414, adv_loss: 0.4117\n",
      "Epoch 1, Batch 48, loss_ca: 2.2771, adv_loss: 0.4236\n",
      "Epoch 1, Batch 49, loss_ca: 2.2271, adv_loss: 0.3958\n",
      "Epoch 1, Batch 50, loss_ca: 2.2594, adv_loss: 0.4003\n",
      "Epoch 1, Batch 51, loss_ca: 2.4240, adv_loss: 0.4161\n",
      "Epoch 1, Batch 52, loss_ca: 2.5878, adv_loss: 0.4272\n",
      "Epoch 1, Batch 53, loss_ca: 2.3959, adv_loss: 0.4154\n",
      "Epoch 1, Batch 54, loss_ca: 2.1842, adv_loss: 0.4054\n",
      "Epoch 1, Batch 55, loss_ca: 2.2289, adv_loss: 0.4352\n",
      "Epoch 1, Batch 56, loss_ca: 2.2774, adv_loss: 0.4269\n",
      "Epoch 1, Batch 57, loss_ca: 2.4632, adv_loss: 0.4590\n",
      "Epoch 1, Batch 58, loss_ca: 2.6437, adv_loss: 0.4453\n",
      "Epoch 1, Batch 59, loss_ca: 2.3466, adv_loss: 0.4295\n",
      "Epoch 1, Batch 60, loss_ca: 2.1654, adv_loss: 0.4050\n",
      "Epoch 1, Batch 61, loss_ca: 2.1545, adv_loss: 0.4092\n",
      "Epoch 1, Batch 62, loss_ca: 2.3644, adv_loss: 0.4070\n",
      "Epoch 1, Batch 63, loss_ca: 2.6608, adv_loss: 0.4232\n",
      "Epoch 1, Batch 64, loss_ca: 2.5067, adv_loss: 0.4107\n",
      "Epoch 1, Batch 65, loss_ca: 2.1785, adv_loss: 0.4186\n",
      "Epoch 1, Batch 66, loss_ca: 2.0617, adv_loss: 0.3847\n",
      "Epoch 1, Batch 67, loss_ca: 2.0872, adv_loss: 0.4361\n",
      "Epoch 1, Batch 68, loss_ca: 2.1976, adv_loss: 0.4399\n",
      "Epoch 1, Batch 69, loss_ca: 2.3125, adv_loss: 0.4662\n",
      "Epoch 1, Batch 70, loss_ca: 2.3549, adv_loss: 0.4368\n",
      "Epoch 1, Batch 71, loss_ca: 2.1638, adv_loss: 0.4295\n",
      "Epoch 1, Batch 72, loss_ca: 2.0320, adv_loss: 0.4205\n",
      "Epoch 1, Batch 73, loss_ca: 1.8794, adv_loss: 0.4303\n",
      "Epoch 1, Batch 74, loss_ca: 1.9605, adv_loss: 0.4303\n",
      "Epoch 1, Batch 75, loss_ca: 2.1883, adv_loss: 0.4396\n",
      "Epoch 1, Batch 76, loss_ca: 2.1088, adv_loss: 0.4499\n",
      "Epoch 1, Batch 77, loss_ca: 1.9447, adv_loss: 0.4161\n",
      "Epoch 1, Batch 78, loss_ca: 1.9289, adv_loss: 0.4048\n",
      "Epoch 1, Batch 79, loss_ca: 1.9438, adv_loss: 0.4158\n",
      "Epoch 1, Batch 80, loss_ca: 2.1049, adv_loss: 0.4360\n",
      "Epoch 1, Batch 81, loss_ca: 2.0844, adv_loss: 0.4173\n",
      "Epoch 1, Batch 82, loss_ca: 1.9722, adv_loss: 0.4061\n",
      "Epoch 1, Batch 83, loss_ca: 1.9346, adv_loss: 0.4113\n",
      "Epoch 1, Batch 84, loss_ca: 2.0323, adv_loss: 0.4101\n",
      "Epoch 1, Batch 85, loss_ca: 2.1174, adv_loss: 0.4034\n",
      "Epoch 1, Batch 86, loss_ca: 2.0818, adv_loss: 0.3945\n",
      "Epoch 1, Batch 87, loss_ca: 2.0533, adv_loss: 0.4026\n",
      "Epoch 1, Batch 88, loss_ca: 1.9457, adv_loss: 0.4063\n",
      "Epoch 1, Batch 89, loss_ca: 2.1103, adv_loss: 0.4178\n",
      "Epoch 1, Batch 90, loss_ca: 2.1443, adv_loss: 0.4201\n",
      "Epoch 1, Batch 91, loss_ca: 2.1883, adv_loss: 0.4383\n",
      "Epoch 1, Batch 92, loss_ca: 2.0749, adv_loss: 0.4466\n",
      "Epoch 1, Batch 93, loss_ca: 2.1756, adv_loss: 0.4555\n",
      "Epoch 1, Batch 94, loss_ca: 2.0799, adv_loss: 0.4405\n",
      "Epoch 1, Batch 95, loss_ca: 2.2127, adv_loss: 0.4582\n",
      "Epoch 1, Batch 96, loss_ca: 2.2343, adv_loss: 0.4816\n",
      "Epoch 1, Batch 97, loss_ca: 2.2097, adv_loss: 0.4613\n",
      "Epoch 1, Batch 98, loss_ca: 2.1220, adv_loss: 0.4533\n",
      "Epoch 1, Batch 99, loss_ca: 2.0141, adv_loss: 0.4438\n",
      "Epoch 1, Batch 100, loss_ca: 2.0758, adv_loss: 0.4404\n",
      "Epoch 1, Batch 101, loss_ca: 2.2348, adv_loss: 0.4334\n",
      "Epoch 1, Batch 102, loss_ca: 2.2561, adv_loss: 0.4245\n",
      "Epoch 1, Batch 103, loss_ca: 2.1837, adv_loss: 0.4338\n",
      "Epoch 1, Batch 104, loss_ca: 2.3479, adv_loss: 0.3973\n",
      "Epoch 1, Batch 105, loss_ca: 2.2026, adv_loss: 0.3898\n",
      "Epoch 1, Batch 106, loss_ca: 2.1449, adv_loss: 0.3947\n",
      "Epoch 1, Batch 107, loss_ca: 2.2843, adv_loss: 0.4136\n",
      "Epoch 1, Batch 108, loss_ca: 2.3365, adv_loss: 0.4246\n",
      "Epoch 1, Batch 109, loss_ca: 2.3680, adv_loss: 0.4216\n",
      "Epoch 1, Batch 110, loss_ca: 2.1642, adv_loss: 0.4770\n",
      "Epoch 1, Batch 111, loss_ca: 2.1025, adv_loss: 0.4777\n",
      "Epoch 1, Batch 112, loss_ca: 2.1095, adv_loss: 0.5094\n",
      "Epoch 1, Batch 113, loss_ca: 2.1502, adv_loss: 0.4760\n",
      "Epoch 1, Batch 114, loss_ca: 2.2378, adv_loss: 0.4185\n",
      "Epoch 1, Batch 115, loss_ca: 2.0526, adv_loss: 0.4094\n",
      "Epoch 1, Batch 116, loss_ca: 1.9417, adv_loss: 0.3772\n",
      "Epoch 1, Batch 117, loss_ca: 1.8764, adv_loss: 0.3682\n",
      "Epoch 1, Batch 118, loss_ca: 2.0390, adv_loss: 0.3639\n",
      "Epoch 1, Batch 119, loss_ca: 2.0492, adv_loss: 0.3545\n",
      "Epoch 1, Batch 120, loss_ca: 1.9103, adv_loss: 0.3941\n",
      "Epoch 1, Batch 121, loss_ca: 2.0356, adv_loss: 0.4038\n",
      "Epoch 1, Batch 122, loss_ca: 2.0643, adv_loss: 0.4054\n",
      "Epoch 1, Batch 123, loss_ca: 2.2369, adv_loss: 0.4506\n",
      "Epoch 1, Batch 124, loss_ca: 2.2930, adv_loss: 0.4769\n",
      "Epoch 1, Batch 125, loss_ca: 2.1563, adv_loss: 0.5148\n",
      "Epoch 1, Batch 126, loss_ca: 2.1906, adv_loss: 0.5076\n",
      "Epoch 1, Batch 127, loss_ca: 2.2440, adv_loss: 0.4967\n",
      "Epoch 1, Batch 128, loss_ca: 2.3020, adv_loss: 0.4847\n",
      "Epoch 1, Batch 129, loss_ca: 2.2671, adv_loss: 0.4903\n",
      "Epoch 1, Batch 130, loss_ca: 2.0913, adv_loss: 0.4692\n",
      "Epoch 1, Batch 131, loss_ca: 2.0523, adv_loss: 0.4452\n",
      "Epoch 1, Batch 132, loss_ca: 2.1826, adv_loss: 0.4249\n",
      "Epoch 1, Batch 133, loss_ca: 2.2072, adv_loss: 0.4191\n",
      "Epoch 1, Batch 134, loss_ca: 2.1197, adv_loss: 0.4586\n",
      "Epoch 1, Batch 135, loss_ca: 2.0599, adv_loss: 0.4280\n",
      "Epoch 1, Batch 136, loss_ca: 2.0778, adv_loss: 0.4745\n",
      "Epoch 1, Batch 137, loss_ca: 2.0037, adv_loss: 0.4685\n",
      "Epoch 1, Batch 138, loss_ca: 2.1332, adv_loss: 0.4457\n",
      "Epoch 1, Batch 139, loss_ca: 2.1783, adv_loss: 0.4169\n",
      "Epoch 1, Batch 140, loss_ca: 2.0473, adv_loss: 0.3752\n",
      "Epoch 1, Batch 141, loss_ca: 1.9055, adv_loss: 0.3583\n",
      "Epoch 1, Batch 142, loss_ca: 1.9563, adv_loss: 0.3505\n",
      "Epoch 1, Batch 143, loss_ca: 1.9932, adv_loss: 0.3634\n",
      "Epoch 1, Batch 144, loss_ca: 2.0408, adv_loss: 0.3415\n",
      "Epoch 1, Batch 145, loss_ca: 1.9441, adv_loss: 0.3335\n",
      "Epoch 1, Batch 146, loss_ca: 1.8829, adv_loss: 0.3550\n",
      "Epoch 1, Batch 147, loss_ca: 1.9752, adv_loss: 0.3586\n",
      "Epoch 1, Batch 148, loss_ca: 1.8843, adv_loss: 0.3862\n",
      "Epoch 1, Batch 149, loss_ca: 1.9277, adv_loss: 0.3466\n",
      "Epoch 1, Batch 150, loss_ca: 2.0208, adv_loss: 0.3540\n",
      "Epoch 1, Batch 151, loss_ca: 1.9427, adv_loss: 0.3893\n",
      "Epoch 1, Batch 152, loss_ca: 2.0706, adv_loss: 0.4163\n",
      "Epoch 1, Batch 153, loss_ca: 2.1805, adv_loss: 0.4424\n",
      "Epoch 1, Batch 154, loss_ca: 2.0415, adv_loss: 0.4283\n",
      "Epoch 1, Batch 155, loss_ca: 2.1160, adv_loss: 0.4136\n",
      "Epoch 1, Batch 156, loss_ca: 2.2606, adv_loss: 0.4093\n",
      "Epoch 1, Batch 157, loss_ca: 2.3210, adv_loss: 0.4152\n",
      "Epoch 1, Batch 158, loss_ca: 2.3501, adv_loss: 0.3822\n",
      "Epoch 1, Batch 159, loss_ca: 2.6836, adv_loss: 0.3512\n",
      "Epoch 1, Batch 160, loss_ca: 2.7817, adv_loss: 0.3419\n",
      "Epoch 1, Batch 161, loss_ca: 3.3303, adv_loss: 0.3850\n",
      "Epoch 1, Batch 162, loss_ca: 2.9843, adv_loss: 0.4161\n",
      "Epoch 1, Batch 163, loss_ca: 3.1558, adv_loss: 0.3919\n",
      "Epoch 1, Batch 164, loss_ca: 2.6131, adv_loss: 0.3675\n",
      "Epoch 1, Batch 165, loss_ca: 2.5088, adv_loss: 0.3591\n",
      "Epoch 1, Batch 166, loss_ca: 2.3297, adv_loss: 0.3432\n",
      "Epoch 1, Batch 167, loss_ca: 2.6031, adv_loss: 0.3635\n",
      "Epoch 1, Batch 168, loss_ca: 2.5679, adv_loss: 0.3529\n",
      "Epoch 1, Batch 169, loss_ca: 2.6935, adv_loss: 0.3599\n",
      "Epoch 1, Batch 170, loss_ca: 2.8283, adv_loss: 0.3341\n",
      "Epoch 1, Batch 171, loss_ca: 2.5718, adv_loss: 0.3480\n",
      "Epoch 1, Batch 172, loss_ca: 2.3880, adv_loss: 0.3323\n",
      "Epoch 1, Batch 173, loss_ca: 2.6713, adv_loss: 0.3776\n",
      "Epoch 1, Batch 174, loss_ca: 3.0215, adv_loss: 0.3814\n",
      "Epoch 1, Batch 175, loss_ca: 2.4867, adv_loss: 0.3313\n",
      "Epoch 1, Batch 176, loss_ca: 2.0933, adv_loss: 0.2970\n",
      "Epoch 1, Batch 177, loss_ca: 1.8051, adv_loss: 0.2802\n",
      "Epoch 1, Batch 178, loss_ca: 1.8495, adv_loss: 0.3473\n",
      "Epoch 1, Batch 179, loss_ca: 2.0610, adv_loss: 0.4518\n",
      "Epoch 1, Batch 180, loss_ca: 2.1004, adv_loss: 0.4701\n",
      "Epoch 1, Batch 181, loss_ca: 2.1247, adv_loss: 0.4715\n",
      "Epoch 1, Batch 182, loss_ca: 2.2595, adv_loss: 0.4179\n",
      "Epoch 1, Batch 183, loss_ca: 2.3000, adv_loss: 0.4407\n",
      "Epoch 1, Batch 184, loss_ca: 2.0487, adv_loss: 0.4797\n",
      "Epoch 1, Batch 185, loss_ca: 2.0259, adv_loss: 0.4522\n",
      "Epoch 1, Batch 186, loss_ca: 1.9247, adv_loss: 0.4224\n",
      "Epoch 1, Batch 187, loss_ca: 1.9135, adv_loss: 0.4253\n",
      "Epoch 1, Batch 188, loss_ca: 2.0218, adv_loss: 0.4103\n",
      "Epoch 1, Batch 189, loss_ca: 2.1214, adv_loss: 0.4470\n",
      "Epoch 1, Batch 190, loss_ca: 2.2258, adv_loss: 0.4448\n",
      "Epoch 1, Batch 191, loss_ca: 2.2371, adv_loss: 0.4111\n",
      "Epoch 1, Batch 192, loss_ca: 2.4014, adv_loss: 0.3767\n",
      "Epoch 1, Batch 193, loss_ca: 2.5329, adv_loss: 0.3661\n",
      "Epoch 1, Batch 194, loss_ca: 2.6599, adv_loss: 0.4023\n",
      "Epoch 1, Batch 195, loss_ca: 2.4202, adv_loss: 0.3879\n",
      "Epoch 1, Batch 196, loss_ca: 2.4885, adv_loss: 0.3363\n",
      "Epoch 1, Batch 197, loss_ca: 2.6145, adv_loss: 0.3433\n",
      "Epoch 1, Batch 198, loss_ca: 2.9509, adv_loss: 0.2780\n",
      "Epoch 1, Batch 199, loss_ca: 2.4358, adv_loss: 0.2292\n",
      "Epoch 1, Batch 200, loss_ca: 2.3369, adv_loss: 0.1932\n",
      "Epoch 1, Batch 201, loss_ca: 2.3184, adv_loss: 0.2311\n",
      "Epoch 1, Batch 202, loss_ca: 2.5111, adv_loss: 0.2527\n",
      "Epoch 1, Batch 203, loss_ca: 2.8217, adv_loss: 0.3224\n",
      "Epoch 1, Batch 204, loss_ca: 2.9112, adv_loss: 0.3722\n",
      "Epoch 1, Batch 205, loss_ca: 3.1433, adv_loss: 0.4227\n",
      "Epoch 1, Batch 206, loss_ca: 2.8488, adv_loss: 0.4310\n",
      "Epoch 1, Batch 207, loss_ca: 2.5670, adv_loss: 0.4008\n",
      "Epoch 1, Batch 208, loss_ca: 2.2896, adv_loss: 0.3648\n",
      "Epoch 1, Batch 209, loss_ca: 2.1261, adv_loss: 0.3544\n",
      "Epoch 1, Batch 210, loss_ca: 2.0806, adv_loss: 0.3901\n",
      "Epoch 1, Batch 211, loss_ca: 2.1682, adv_loss: 0.4370\n",
      "Epoch 1, Batch 212, loss_ca: 2.0439, adv_loss: 0.4797\n",
      "Epoch 1, Batch 213, loss_ca: 2.4719, adv_loss: 0.4821\n",
      "Epoch 1, Batch 214, loss_ca: 2.3134, adv_loss: 0.5178\n",
      "Epoch 1, Batch 215, loss_ca: 2.0656, adv_loss: 0.5264\n",
      "Epoch 1, Batch 216, loss_ca: 2.1511, adv_loss: 0.5091\n",
      "Epoch 1, Batch 217, loss_ca: 2.1873, adv_loss: 0.5042\n",
      "Epoch 1, Batch 218, loss_ca: 2.2317, adv_loss: 0.4841\n",
      "Epoch 1, Batch 219, loss_ca: 2.7086, adv_loss: 0.4880\n",
      "Epoch 1, Batch 220, loss_ca: 2.7642, adv_loss: 0.4393\n",
      "Epoch 1, Batch 221, loss_ca: 2.6633, adv_loss: 0.4111\n",
      "Epoch 1, Batch 222, loss_ca: 2.3398, adv_loss: 0.3719\n",
      "Epoch 1, Batch 223, loss_ca: 2.1173, adv_loss: 0.3126\n",
      "Epoch 1, Batch 224, loss_ca: 2.0501, adv_loss: 0.2966\n",
      "Epoch 1, Batch 225, loss_ca: 1.9696, adv_loss: 0.2728\n",
      "Epoch 1, Batch 226, loss_ca: 2.0953, adv_loss: 0.2309\n",
      "Epoch 1, Batch 227, loss_ca: 2.3193, adv_loss: 0.2071\n",
      "Epoch 1, Batch 228, loss_ca: 2.2811, adv_loss: 0.2060\n",
      "Epoch 2, Batch 2, loss_ca: 2.1651, adv_loss: 0.2179\n",
      "Epoch 2, Batch 3, loss_ca: 2.0383, adv_loss: 0.2452\n",
      "Epoch 2, Batch 4, loss_ca: 1.9327, adv_loss: 0.2989\n",
      "Epoch 2, Batch 5, loss_ca: 1.9394, adv_loss: 0.3411\n",
      "Epoch 2, Batch 6, loss_ca: 1.9274, adv_loss: 0.3859\n",
      "Epoch 2, Batch 7, loss_ca: 2.0588, adv_loss: 0.4247\n",
      "Epoch 2, Batch 8, loss_ca: 2.0285, adv_loss: 0.4368\n",
      "Epoch 2, Batch 9, loss_ca: 1.9093, adv_loss: 0.4525\n",
      "Epoch 2, Batch 10, loss_ca: 1.9194, adv_loss: 0.5087\n",
      "Epoch 2, Batch 11, loss_ca: 1.9623, adv_loss: 0.5218\n",
      "Epoch 2, Batch 12, loss_ca: 1.9876, adv_loss: 0.4869\n",
      "Epoch 2, Batch 13, loss_ca: 2.0546, adv_loss: 0.4653\n",
      "Epoch 2, Batch 14, loss_ca: 2.1207, adv_loss: 0.4600\n",
      "Epoch 2, Batch 15, loss_ca: 2.0292, adv_loss: 0.4625\n",
      "Epoch 2, Batch 16, loss_ca: 1.8616, adv_loss: 0.4534\n",
      "Epoch 2, Batch 17, loss_ca: 1.8769, adv_loss: 0.4471\n",
      "Epoch 2, Batch 18, loss_ca: 1.8045, adv_loss: 0.3990\n",
      "Epoch 2, Batch 19, loss_ca: 1.9940, adv_loss: 0.4082\n",
      "Epoch 2, Batch 20, loss_ca: 2.2108, adv_loss: 0.3942\n",
      "Epoch 2, Batch 21, loss_ca: 2.0713, adv_loss: 0.4246\n",
      "Epoch 2, Batch 22, loss_ca: 1.9514, adv_loss: 0.4385\n",
      "Epoch 2, Batch 23, loss_ca: 1.9662, adv_loss: 0.4411\n",
      "Epoch 2, Batch 24, loss_ca: 2.1685, adv_loss: 0.4122\n",
      "Epoch 2, Batch 25, loss_ca: 2.0070, adv_loss: 0.3740\n",
      "Epoch 2, Batch 26, loss_ca: 2.2002, adv_loss: 0.3596\n",
      "Epoch 2, Batch 27, loss_ca: 2.2124, adv_loss: 0.3798\n",
      "Epoch 2, Batch 28, loss_ca: 2.0869, adv_loss: 0.3871\n",
      "Epoch 2, Batch 29, loss_ca: 2.0667, adv_loss: 0.4393\n",
      "Epoch 2, Batch 30, loss_ca: 2.2021, adv_loss: 0.4451\n",
      "Epoch 2, Batch 31, loss_ca: 2.1347, adv_loss: 0.4929\n",
      "Epoch 2, Batch 32, loss_ca: 2.0177, adv_loss: 0.4786\n",
      "Epoch 2, Batch 33, loss_ca: 2.2479, adv_loss: 0.4201\n",
      "Epoch 2, Batch 34, loss_ca: 2.2015, adv_loss: 0.3947\n",
      "Epoch 2, Batch 35, loss_ca: 2.1852, adv_loss: 0.3827\n",
      "Epoch 2, Batch 36, loss_ca: 2.2144, adv_loss: 0.3611\n",
      "Epoch 2, Batch 37, loss_ca: 2.2943, adv_loss: 0.3587\n",
      "Epoch 2, Batch 38, loss_ca: 2.3073, adv_loss: 0.3446\n",
      "Epoch 2, Batch 39, loss_ca: 2.2137, adv_loss: 0.3573\n",
      "Epoch 2, Batch 40, loss_ca: 2.1264, adv_loss: 0.3694\n",
      "Epoch 2, Batch 41, loss_ca: 2.0712, adv_loss: 0.4067\n",
      "Epoch 2, Batch 42, loss_ca: 2.1048, adv_loss: 0.4247\n",
      "Epoch 2, Batch 43, loss_ca: 2.0061, adv_loss: 0.4186\n",
      "Epoch 2, Batch 44, loss_ca: 1.9634, adv_loss: 0.4478\n",
      "Epoch 2, Batch 45, loss_ca: 1.9726, adv_loss: 0.4110\n",
      "Epoch 2, Batch 46, loss_ca: 2.0573, adv_loss: 0.4160\n",
      "Epoch 2, Batch 47, loss_ca: 2.0699, adv_loss: 0.3919\n",
      "Epoch 2, Batch 48, loss_ca: 1.9474, adv_loss: 0.3907\n",
      "Epoch 2, Batch 49, loss_ca: 1.8963, adv_loss: 0.3919\n",
      "Epoch 2, Batch 50, loss_ca: 1.9320, adv_loss: 0.4042\n",
      "Epoch 2, Batch 51, loss_ca: 2.0155, adv_loss: 0.3889\n",
      "Epoch 2, Batch 52, loss_ca: 2.0177, adv_loss: 0.3873\n",
      "Epoch 2, Batch 53, loss_ca: 1.9433, adv_loss: 0.3636\n",
      "Epoch 2, Batch 54, loss_ca: 1.9081, adv_loss: 0.3364\n",
      "Epoch 2, Batch 55, loss_ca: 1.8548, adv_loss: 0.3244\n",
      "Epoch 2, Batch 56, loss_ca: 1.8537, adv_loss: 0.3240\n",
      "Epoch 2, Batch 57, loss_ca: 1.8926, adv_loss: 0.3434\n",
      "Epoch 2, Batch 58, loss_ca: 1.9636, adv_loss: 0.3763\n",
      "Epoch 2, Batch 59, loss_ca: 1.9497, adv_loss: 0.4087\n",
      "Epoch 2, Batch 60, loss_ca: 1.9847, adv_loss: 0.4742\n",
      "Epoch 2, Batch 61, loss_ca: 2.0769, adv_loss: 0.5312\n",
      "Epoch 2, Batch 62, loss_ca: 2.1208, adv_loss: 0.5208\n",
      "Epoch 2, Batch 63, loss_ca: 2.0062, adv_loss: 0.4953\n",
      "Epoch 2, Batch 64, loss_ca: 1.9554, adv_loss: 0.4868\n",
      "Epoch 2, Batch 65, loss_ca: 1.8890, adv_loss: 0.4566\n",
      "Epoch 2, Batch 66, loss_ca: 1.9189, adv_loss: 0.4716\n",
      "Epoch 2, Batch 67, loss_ca: 2.0912, adv_loss: 0.4589\n",
      "Epoch 2, Batch 68, loss_ca: 2.1075, adv_loss: 0.4073\n",
      "Epoch 2, Batch 69, loss_ca: 1.9384, adv_loss: 0.3770\n",
      "Epoch 2, Batch 70, loss_ca: 1.9515, adv_loss: 0.3579\n",
      "Epoch 2, Batch 71, loss_ca: 1.8557, adv_loss: 0.3825\n",
      "Epoch 2, Batch 72, loss_ca: 1.9573, adv_loss: 0.4004\n",
      "Epoch 2, Batch 73, loss_ca: 1.9162, adv_loss: 0.4154\n",
      "Epoch 2, Batch 74, loss_ca: 1.8715, adv_loss: 0.4271\n",
      "Epoch 2, Batch 75, loss_ca: 1.7839, adv_loss: 0.4270\n",
      "Epoch 2, Batch 76, loss_ca: 1.7068, adv_loss: 0.4154\n",
      "Epoch 2, Batch 77, loss_ca: 1.7683, adv_loss: 0.3700\n",
      "Epoch 2, Batch 78, loss_ca: 1.8284, adv_loss: 0.3690\n",
      "Epoch 2, Batch 79, loss_ca: 1.8155, adv_loss: 0.3779\n",
      "Epoch 2, Batch 80, loss_ca: 1.8115, adv_loss: 0.3958\n",
      "Epoch 2, Batch 81, loss_ca: 1.7765, adv_loss: 0.3741\n",
      "Epoch 2, Batch 82, loss_ca: 1.7516, adv_loss: 0.3451\n",
      "Epoch 2, Batch 83, loss_ca: 1.8155, adv_loss: 0.3765\n",
      "Epoch 2, Batch 84, loss_ca: 1.8464, adv_loss: 0.3714\n",
      "Epoch 2, Batch 85, loss_ca: 1.9050, adv_loss: 0.3832\n",
      "Epoch 2, Batch 86, loss_ca: 1.9138, adv_loss: 0.3615\n",
      "Epoch 2, Batch 87, loss_ca: 1.8912, adv_loss: 0.3690\n",
      "Epoch 2, Batch 88, loss_ca: 1.9072, adv_loss: 0.3804\n",
      "Epoch 2, Batch 89, loss_ca: 2.0217, adv_loss: 0.3965\n",
      "Epoch 2, Batch 90, loss_ca: 1.9541, adv_loss: 0.4391\n",
      "Epoch 2, Batch 91, loss_ca: 1.9496, adv_loss: 0.4672\n",
      "Epoch 2, Batch 92, loss_ca: 1.9023, adv_loss: 0.4684\n",
      "Epoch 2, Batch 93, loss_ca: 2.0305, adv_loss: 0.4724\n",
      "Epoch 2, Batch 94, loss_ca: 1.9306, adv_loss: 0.5142\n",
      "Epoch 2, Batch 95, loss_ca: 2.0973, adv_loss: 0.5541\n",
      "Epoch 2, Batch 96, loss_ca: 2.0524, adv_loss: 0.5211\n",
      "Epoch 2, Batch 97, loss_ca: 2.0583, adv_loss: 0.5089\n",
      "Epoch 2, Batch 98, loss_ca: 2.0142, adv_loss: 0.4600\n",
      "Epoch 2, Batch 99, loss_ca: 1.9554, adv_loss: 0.4365\n",
      "Epoch 2, Batch 100, loss_ca: 2.0101, adv_loss: 0.4390\n",
      "Epoch 2, Batch 101, loss_ca: 2.1236, adv_loss: 0.4283\n",
      "Epoch 2, Batch 102, loss_ca: 2.1353, adv_loss: 0.4112\n",
      "Epoch 2, Batch 103, loss_ca: 2.1486, adv_loss: 0.4218\n",
      "Epoch 2, Batch 104, loss_ca: 2.3571, adv_loss: 0.4395\n",
      "Epoch 2, Batch 105, loss_ca: 2.2297, adv_loss: 0.4214\n",
      "Epoch 2, Batch 106, loss_ca: 2.1468, adv_loss: 0.3902\n",
      "Epoch 2, Batch 107, loss_ca: 2.2559, adv_loss: 0.3659\n",
      "Epoch 2, Batch 108, loss_ca: 2.2324, adv_loss: 0.3819\n",
      "Epoch 2, Batch 109, loss_ca: 2.2073, adv_loss: 0.4139\n",
      "Epoch 2, Batch 110, loss_ca: 1.9957, adv_loss: 0.4471\n",
      "Epoch 2, Batch 111, loss_ca: 1.9006, adv_loss: 0.4391\n",
      "Epoch 2, Batch 112, loss_ca: 1.9676, adv_loss: 0.4562\n",
      "Epoch 2, Batch 113, loss_ca: 1.8912, adv_loss: 0.4678\n",
      "Epoch 2, Batch 114, loss_ca: 1.9489, adv_loss: 0.5110\n",
      "Epoch 2, Batch 115, loss_ca: 2.0222, adv_loss: 0.5293\n",
      "Epoch 2, Batch 116, loss_ca: 1.9289, adv_loss: 0.5048\n",
      "Epoch 2, Batch 117, loss_ca: 1.8601, adv_loss: 0.4295\n",
      "Epoch 2, Batch 118, loss_ca: 1.9512, adv_loss: 0.4188\n",
      "Epoch 2, Batch 119, loss_ca: 1.9228, adv_loss: 0.4310\n",
      "Epoch 2, Batch 120, loss_ca: 1.9267, adv_loss: 0.4383\n",
      "Epoch 2, Batch 121, loss_ca: 2.0733, adv_loss: 0.4499\n",
      "Epoch 2, Batch 122, loss_ca: 2.0408, adv_loss: 0.4222\n",
      "Epoch 2, Batch 123, loss_ca: 1.9999, adv_loss: 0.4100\n",
      "Epoch 2, Batch 124, loss_ca: 1.9818, adv_loss: 0.3877\n",
      "Epoch 2, Batch 125, loss_ca: 2.0336, adv_loss: 0.4430\n",
      "Epoch 2, Batch 126, loss_ca: 2.1174, adv_loss: 0.4772\n",
      "Epoch 2, Batch 127, loss_ca: 2.1629, adv_loss: 0.4872\n",
      "Epoch 2, Batch 128, loss_ca: 2.1305, adv_loss: 0.5043\n",
      "Epoch 2, Batch 129, loss_ca: 2.0299, adv_loss: 0.5296\n",
      "Epoch 2, Batch 130, loss_ca: 1.9281, adv_loss: 0.5335\n",
      "Epoch 2, Batch 131, loss_ca: 1.9645, adv_loss: 0.5385\n",
      "Epoch 2, Batch 132, loss_ca: 2.1925, adv_loss: 0.5298\n",
      "Epoch 2, Batch 133, loss_ca: 2.2236, adv_loss: 0.5258\n",
      "Epoch 2, Batch 134, loss_ca: 2.1366, adv_loss: 0.5453\n",
      "Epoch 2, Batch 135, loss_ca: 1.9952, adv_loss: 0.5168\n",
      "Epoch 2, Batch 136, loss_ca: 1.9996, adv_loss: 0.4666\n",
      "Epoch 2, Batch 137, loss_ca: 1.9366, adv_loss: 0.4223\n",
      "Epoch 2, Batch 138, loss_ca: 1.9910, adv_loss: 0.3736\n",
      "Epoch 2, Batch 139, loss_ca: 1.9949, adv_loss: 0.3596\n",
      "Epoch 2, Batch 140, loss_ca: 1.9031, adv_loss: 0.3546\n",
      "Epoch 2, Batch 141, loss_ca: 1.8067, adv_loss: 0.3237\n",
      "Epoch 2, Batch 142, loss_ca: 1.9112, adv_loss: 0.3593\n",
      "Epoch 2, Batch 143, loss_ca: 1.9188, adv_loss: 0.3487\n",
      "Epoch 2, Batch 144, loss_ca: 2.0028, adv_loss: 0.4091\n",
      "Epoch 2, Batch 145, loss_ca: 1.9531, adv_loss: 0.4408\n",
      "Epoch 2, Batch 146, loss_ca: 1.8612, adv_loss: 0.4713\n",
      "Epoch 2, Batch 147, loss_ca: 1.9182, adv_loss: 0.4475\n",
      "Epoch 2, Batch 148, loss_ca: 1.9033, adv_loss: 0.4623\n",
      "Epoch 2, Batch 149, loss_ca: 1.9338, adv_loss: 0.4611\n",
      "Epoch 2, Batch 150, loss_ca: 1.9668, adv_loss: 0.4550\n",
      "Epoch 2, Batch 151, loss_ca: 1.8867, adv_loss: 0.4740\n",
      "Epoch 2, Batch 152, loss_ca: 1.8212, adv_loss: 0.4438\n",
      "Epoch 2, Batch 153, loss_ca: 2.0616, adv_loss: 0.4788\n",
      "Epoch 2, Batch 154, loss_ca: 2.0073, adv_loss: 0.4715\n",
      "Epoch 2, Batch 155, loss_ca: 2.0559, adv_loss: 0.5096\n",
      "Epoch 2, Batch 156, loss_ca: 2.1633, adv_loss: 0.5407\n",
      "Epoch 2, Batch 157, loss_ca: 2.1417, adv_loss: 0.4994\n",
      "Epoch 2, Batch 158, loss_ca: 2.2073, adv_loss: 0.4193\n",
      "Epoch 2, Batch 159, loss_ca: 2.6119, adv_loss: 0.3802\n",
      "Epoch 2, Batch 160, loss_ca: 2.5066, adv_loss: 0.3657\n",
      "Epoch 2, Batch 161, loss_ca: 2.7786, adv_loss: 0.4206\n",
      "Epoch 2, Batch 162, loss_ca: 2.7111, adv_loss: 0.4490\n",
      "Epoch 2, Batch 163, loss_ca: 2.7243, adv_loss: 0.5100\n",
      "Epoch 2, Batch 164, loss_ca: 2.5949, adv_loss: 0.5165\n",
      "Epoch 2, Batch 165, loss_ca: 2.6365, adv_loss: 0.5249\n",
      "Epoch 2, Batch 166, loss_ca: 2.3114, adv_loss: 0.4672\n",
      "Epoch 2, Batch 167, loss_ca: 2.5666, adv_loss: 0.4528\n",
      "Epoch 2, Batch 168, loss_ca: 2.3781, adv_loss: 0.4766\n",
      "Epoch 2, Batch 169, loss_ca: 2.3630, adv_loss: 0.4928\n",
      "Epoch 2, Batch 170, loss_ca: 2.7115, adv_loss: 0.5234\n",
      "Epoch 2, Batch 171, loss_ca: 2.4805, adv_loss: 0.5080\n",
      "Epoch 2, Batch 172, loss_ca: 2.2251, adv_loss: 0.4439\n",
      "Epoch 2, Batch 173, loss_ca: 2.3508, adv_loss: 0.3956\n",
      "Epoch 2, Batch 174, loss_ca: 2.7085, adv_loss: 0.3783\n",
      "Epoch 2, Batch 175, loss_ca: 2.4774, adv_loss: 0.3681\n",
      "Epoch 2, Batch 176, loss_ca: 2.2497, adv_loss: 0.3906\n",
      "Epoch 2, Batch 177, loss_ca: 1.9697, adv_loss: 0.3710\n",
      "Epoch 2, Batch 178, loss_ca: 1.9896, adv_loss: 0.3726\n",
      "Epoch 2, Batch 179, loss_ca: 1.9190, adv_loss: 0.4188\n",
      "Epoch 2, Batch 180, loss_ca: 1.8287, adv_loss: 0.3782\n",
      "Epoch 2, Batch 181, loss_ca: 1.9145, adv_loss: 0.4061\n",
      "Epoch 2, Batch 182, loss_ca: 2.0806, adv_loss: 0.4773\n",
      "Epoch 2, Batch 183, loss_ca: 2.1817, adv_loss: 0.5067\n",
      "Epoch 2, Batch 184, loss_ca: 1.9663, adv_loss: 0.4523\n",
      "Epoch 2, Batch 185, loss_ca: 1.8259, adv_loss: 0.4689\n",
      "Epoch 2, Batch 186, loss_ca: 1.8225, adv_loss: 0.4927\n",
      "Epoch 2, Batch 187, loss_ca: 1.8059, adv_loss: 0.4885\n",
      "Epoch 2, Batch 188, loss_ca: 1.9064, adv_loss: 0.5183\n",
      "Epoch 2, Batch 189, loss_ca: 1.9455, adv_loss: 0.5391\n",
      "Epoch 2, Batch 190, loss_ca: 2.1146, adv_loss: 0.5465\n",
      "Epoch 2, Batch 191, loss_ca: 2.2406, adv_loss: 0.5690\n",
      "Epoch 2, Batch 192, loss_ca: 2.4488, adv_loss: 0.5517\n",
      "Epoch 2, Batch 193, loss_ca: 2.3541, adv_loss: 0.5688\n",
      "Epoch 2, Batch 194, loss_ca: 2.3815, adv_loss: 0.5769\n",
      "Epoch 2, Batch 195, loss_ca: 2.2261, adv_loss: 0.5396\n",
      "Epoch 2, Batch 196, loss_ca: 2.4306, adv_loss: 0.5084\n",
      "Epoch 2, Batch 197, loss_ca: 2.4458, adv_loss: 0.4748\n",
      "Epoch 2, Batch 198, loss_ca: 2.8564, adv_loss: 0.4246\n",
      "Epoch 2, Batch 199, loss_ca: 2.5970, adv_loss: 0.3543\n",
      "Epoch 2, Batch 200, loss_ca: 2.2003, adv_loss: 0.3075\n",
      "Epoch 2, Batch 201, loss_ca: 2.1033, adv_loss: 0.2657\n",
      "Epoch 2, Batch 202, loss_ca: 2.3313, adv_loss: 0.2682\n",
      "Epoch 2, Batch 203, loss_ca: 2.5973, adv_loss: 0.3113\n",
      "Epoch 2, Batch 204, loss_ca: 2.3597, adv_loss: 0.4067\n",
      "Epoch 2, Batch 205, loss_ca: 2.6531, adv_loss: 0.4411\n",
      "Epoch 2, Batch 206, loss_ca: 2.6326, adv_loss: 0.4376\n",
      "Epoch 2, Batch 207, loss_ca: 2.5670, adv_loss: 0.4013\n",
      "Epoch 2, Batch 208, loss_ca: 2.3870, adv_loss: 0.4043\n",
      "Epoch 2, Batch 209, loss_ca: 2.0973, adv_loss: 0.4681\n",
      "Epoch 2, Batch 210, loss_ca: 2.0922, adv_loss: 0.5620\n",
      "Epoch 2, Batch 211, loss_ca: 2.2213, adv_loss: 0.5917\n",
      "Epoch 2, Batch 212, loss_ca: 1.9286, adv_loss: 0.6403\n",
      "Epoch 2, Batch 213, loss_ca: 2.4566, adv_loss: 0.6340\n",
      "Epoch 2, Batch 214, loss_ca: 2.2294, adv_loss: 0.5943\n",
      "Epoch 2, Batch 215, loss_ca: 1.9684, adv_loss: 0.5514\n",
      "Epoch 2, Batch 216, loss_ca: 2.0408, adv_loss: 0.4302\n",
      "Epoch 2, Batch 217, loss_ca: 2.0284, adv_loss: 0.3805\n",
      "Epoch 2, Batch 218, loss_ca: 2.0655, adv_loss: 0.4059\n",
      "Epoch 2, Batch 219, loss_ca: 2.4836, adv_loss: 0.4356\n",
      "Epoch 2, Batch 220, loss_ca: 2.6314, adv_loss: 0.4995\n",
      "Epoch 2, Batch 221, loss_ca: 2.6074, adv_loss: 0.5175\n",
      "Epoch 2, Batch 222, loss_ca: 2.3554, adv_loss: 0.5008\n",
      "Epoch 2, Batch 223, loss_ca: 2.1081, adv_loss: 0.4254\n",
      "Epoch 2, Batch 224, loss_ca: 2.0025, adv_loss: 0.4077\n",
      "Epoch 2, Batch 225, loss_ca: 1.9402, adv_loss: 0.3341\n",
      "Epoch 2, Batch 226, loss_ca: 1.9691, adv_loss: 0.2432\n",
      "Epoch 2, Batch 227, loss_ca: 2.1269, adv_loss: 0.2516\n",
      "Epoch 2, Batch 228, loss_ca: 2.0622, adv_loss: 0.2732\n",
      "Epoch 3, Batch 3, loss_ca: 2.0045, adv_loss: 0.3404\n",
      "Epoch 3, Batch 4, loss_ca: 1.9565, adv_loss: 0.3175\n",
      "Epoch 3, Batch 5, loss_ca: 1.8113, adv_loss: 0.3581\n",
      "Epoch 3, Batch 6, loss_ca: 1.9446, adv_loss: 0.3765\n",
      "Epoch 3, Batch 7, loss_ca: 2.0161, adv_loss: 0.4266\n",
      "Epoch 3, Batch 8, loss_ca: 2.0132, adv_loss: 0.4438\n",
      "Epoch 3, Batch 9, loss_ca: 1.8132, adv_loss: 0.4424\n",
      "Epoch 3, Batch 10, loss_ca: 1.7716, adv_loss: 0.4770\n",
      "Epoch 3, Batch 11, loss_ca: 1.7864, adv_loss: 0.4813\n",
      "Epoch 3, Batch 12, loss_ca: 1.7855, adv_loss: 0.5201\n",
      "Epoch 3, Batch 13, loss_ca: 1.8714, adv_loss: 0.5459\n",
      "Epoch 3, Batch 14, loss_ca: 2.0808, adv_loss: 0.5650\n",
      "Epoch 3, Batch 15, loss_ca: 2.0812, adv_loss: 0.5591\n",
      "Epoch 3, Batch 16, loss_ca: 1.8009, adv_loss: 0.5854\n",
      "Epoch 3, Batch 17, loss_ca: 1.7752, adv_loss: 0.5924\n",
      "Epoch 3, Batch 18, loss_ca: 1.7605, adv_loss: 0.5415\n",
      "Epoch 3, Batch 19, loss_ca: 1.8395, adv_loss: 0.4885\n",
      "Epoch 3, Batch 20, loss_ca: 2.0497, adv_loss: 0.4668\n",
      "Epoch 3, Batch 21, loss_ca: 1.9563, adv_loss: 0.4743\n",
      "Epoch 3, Batch 22, loss_ca: 1.8501, adv_loss: 0.5358\n",
      "Epoch 3, Batch 23, loss_ca: 1.8798, adv_loss: 0.5773\n",
      "Epoch 3, Batch 24, loss_ca: 2.2282, adv_loss: 0.5966\n",
      "Epoch 3, Batch 25, loss_ca: 2.1034, adv_loss: 0.5983\n",
      "Epoch 3, Batch 26, loss_ca: 2.2689, adv_loss: 0.6097\n",
      "Epoch 3, Batch 27, loss_ca: 2.2307, adv_loss: 0.6380\n",
      "Epoch 3, Batch 28, loss_ca: 2.1482, adv_loss: 0.6438\n",
      "Epoch 3, Batch 29, loss_ca: 2.1524, adv_loss: 0.6518\n",
      "Epoch 3, Batch 30, loss_ca: 2.2143, adv_loss: 0.6447\n",
      "Epoch 3, Batch 31, loss_ca: 2.1362, adv_loss: 0.6159\n",
      "Epoch 3, Batch 32, loss_ca: 1.9880, adv_loss: 0.5934\n",
      "Epoch 3, Batch 33, loss_ca: 2.0768, adv_loss: 0.5904\n",
      "Epoch 3, Batch 34, loss_ca: 2.2025, adv_loss: 0.5524\n",
      "Epoch 3, Batch 35, loss_ca: 2.1671, adv_loss: 0.5126\n",
      "Epoch 3, Batch 36, loss_ca: 2.1541, adv_loss: 0.4685\n",
      "Epoch 3, Batch 37, loss_ca: 2.2713, adv_loss: 0.3849\n",
      "Epoch 3, Batch 38, loss_ca: 2.2797, adv_loss: 0.3310\n",
      "Epoch 3, Batch 39, loss_ca: 2.1536, adv_loss: 0.3215\n",
      "Epoch 3, Batch 40, loss_ca: 2.0635, adv_loss: 0.3142\n",
      "Epoch 3, Batch 41, loss_ca: 2.0853, adv_loss: 0.3347\n",
      "Epoch 3, Batch 42, loss_ca: 2.0442, adv_loss: 0.3397\n",
      "Epoch 3, Batch 43, loss_ca: 1.9697, adv_loss: 0.2960\n",
      "Epoch 3, Batch 44, loss_ca: 1.9172, adv_loss: 0.2941\n",
      "Epoch 3, Batch 45, loss_ca: 1.9462, adv_loss: 0.3149\n",
      "Epoch 3, Batch 46, loss_ca: 1.9708, adv_loss: 0.3944\n",
      "Epoch 3, Batch 47, loss_ca: 2.0136, adv_loss: 0.4616\n",
      "Epoch 3, Batch 48, loss_ca: 1.9383, adv_loss: 0.4512\n",
      "Epoch 3, Batch 49, loss_ca: 1.8662, adv_loss: 0.4907\n",
      "Epoch 3, Batch 50, loss_ca: 1.8943, adv_loss: 0.4933\n",
      "Epoch 3, Batch 51, loss_ca: 1.9283, adv_loss: 0.5298\n",
      "Epoch 3, Batch 52, loss_ca: 1.9195, adv_loss: 0.5321\n",
      "Epoch 3, Batch 53, loss_ca: 1.8819, adv_loss: 0.5263\n",
      "Epoch 3, Batch 54, loss_ca: 1.8330, adv_loss: 0.4963\n",
      "Epoch 3, Batch 55, loss_ca: 1.8770, adv_loss: 0.5057\n",
      "Epoch 3, Batch 56, loss_ca: 1.8947, adv_loss: 0.5204\n",
      "Epoch 3, Batch 57, loss_ca: 1.9428, adv_loss: 0.5330\n",
      "Epoch 3, Batch 58, loss_ca: 1.9991, adv_loss: 0.5205\n",
      "Epoch 3, Batch 59, loss_ca: 1.9588, adv_loss: 0.5269\n",
      "Epoch 3, Batch 60, loss_ca: 1.9565, adv_loss: 0.5384\n",
      "Epoch 3, Batch 61, loss_ca: 1.9578, adv_loss: 0.5817\n",
      "Epoch 3, Batch 62, loss_ca: 2.0251, adv_loss: 0.6493\n",
      "Epoch 3, Batch 63, loss_ca: 2.0135, adv_loss: 0.6732\n",
      "Epoch 3, Batch 64, loss_ca: 1.9611, adv_loss: 0.6509\n",
      "Epoch 3, Batch 65, loss_ca: 1.9064, adv_loss: 0.6419\n",
      "Epoch 3, Batch 66, loss_ca: 1.9015, adv_loss: 0.6503\n",
      "Epoch 3, Batch 67, loss_ca: 1.9709, adv_loss: 0.5741\n",
      "Epoch 3, Batch 68, loss_ca: 1.9865, adv_loss: 0.5754\n",
      "Epoch 3, Batch 69, loss_ca: 1.8932, adv_loss: 0.5695\n",
      "Epoch 3, Batch 70, loss_ca: 1.9155, adv_loss: 0.5437\n",
      "Epoch 3, Batch 71, loss_ca: 1.7973, adv_loss: 0.5039\n",
      "Epoch 3, Batch 72, loss_ca: 1.7876, adv_loss: 0.4610\n",
      "Epoch 3, Batch 73, loss_ca: 1.7187, adv_loss: 0.4632\n",
      "Epoch 3, Batch 74, loss_ca: 1.7163, adv_loss: 0.4900\n",
      "Epoch 3, Batch 75, loss_ca: 1.7139, adv_loss: 0.4817\n",
      "Epoch 3, Batch 76, loss_ca: 1.6543, adv_loss: 0.4641\n",
      "Epoch 3, Batch 77, loss_ca: 1.6601, adv_loss: 0.4610\n",
      "Epoch 3, Batch 78, loss_ca: 1.7164, adv_loss: 0.4781\n",
      "Epoch 3, Batch 79, loss_ca: 1.7096, adv_loss: 0.4960\n",
      "Epoch 3, Batch 80, loss_ca: 1.7481, adv_loss: 0.5332\n",
      "Epoch 3, Batch 81, loss_ca: 1.7475, adv_loss: 0.5353\n",
      "Epoch 3, Batch 82, loss_ca: 1.7534, adv_loss: 0.5274\n",
      "Epoch 3, Batch 83, loss_ca: 1.8305, adv_loss: 0.5155\n",
      "Epoch 3, Batch 84, loss_ca: 1.8820, adv_loss: 0.5323\n",
      "Epoch 3, Batch 85, loss_ca: 1.9267, adv_loss: 0.5544\n",
      "Epoch 3, Batch 86, loss_ca: 1.9499, adv_loss: 0.5620\n",
      "Epoch 3, Batch 87, loss_ca: 1.8567, adv_loss: 0.5624\n",
      "Epoch 3, Batch 88, loss_ca: 1.8074, adv_loss: 0.5585\n",
      "Epoch 3, Batch 89, loss_ca: 1.8833, adv_loss: 0.5671\n",
      "Epoch 3, Batch 90, loss_ca: 1.8404, adv_loss: 0.5900\n",
      "Epoch 3, Batch 91, loss_ca: 1.9355, adv_loss: 0.5804\n",
      "Epoch 3, Batch 92, loss_ca: 1.9174, adv_loss: 0.5697\n",
      "Epoch 3, Batch 93, loss_ca: 1.9971, adv_loss: 0.5680\n",
      "Epoch 3, Batch 94, loss_ca: 1.8832, adv_loss: 0.6036\n",
      "Epoch 3, Batch 95, loss_ca: 1.9960, adv_loss: 0.6356\n",
      "Epoch 3, Batch 96, loss_ca: 1.9880, adv_loss: 0.6318\n",
      "Epoch 3, Batch 97, loss_ca: 1.9777, adv_loss: 0.6157\n",
      "Epoch 3, Batch 98, loss_ca: 2.0152, adv_loss: 0.6110\n",
      "Epoch 3, Batch 99, loss_ca: 1.9538, adv_loss: 0.6113\n",
      "Epoch 3, Batch 100, loss_ca: 2.0653, adv_loss: 0.6332\n",
      "Epoch 3, Batch 101, loss_ca: 2.1529, adv_loss: 0.6269\n",
      "Epoch 3, Batch 102, loss_ca: 2.1281, adv_loss: 0.5884\n",
      "Epoch 3, Batch 103, loss_ca: 2.1029, adv_loss: 0.5379\n",
      "Epoch 3, Batch 104, loss_ca: 2.2759, adv_loss: 0.4917\n",
      "Epoch 3, Batch 105, loss_ca: 2.1993, adv_loss: 0.4514\n",
      "Epoch 3, Batch 106, loss_ca: 2.1225, adv_loss: 0.4532\n",
      "Epoch 3, Batch 107, loss_ca: 2.2391, adv_loss: 0.4811\n",
      "Epoch 3, Batch 108, loss_ca: 2.2248, adv_loss: 0.5143\n",
      "Epoch 3, Batch 109, loss_ca: 2.2262, adv_loss: 0.4891\n",
      "Epoch 3, Batch 110, loss_ca: 2.0690, adv_loss: 0.4393\n",
      "Epoch 3, Batch 111, loss_ca: 1.9681, adv_loss: 0.3955\n",
      "Epoch 3, Batch 112, loss_ca: 1.9451, adv_loss: 0.4114\n",
      "Epoch 3, Batch 113, loss_ca: 1.8478, adv_loss: 0.3840\n",
      "Epoch 3, Batch 114, loss_ca: 1.8441, adv_loss: 0.3699\n",
      "Epoch 3, Batch 115, loss_ca: 1.8458, adv_loss: 0.3953\n",
      "Epoch 3, Batch 116, loss_ca: 1.9160, adv_loss: 0.4224\n",
      "Epoch 3, Batch 117, loss_ca: 1.8823, adv_loss: 0.4098\n",
      "Epoch 3, Batch 118, loss_ca: 1.8657, adv_loss: 0.3909\n",
      "Epoch 3, Batch 119, loss_ca: 1.8868, adv_loss: 0.3994\n",
      "Epoch 3, Batch 120, loss_ca: 1.9543, adv_loss: 0.4888\n",
      "Epoch 3, Batch 121, loss_ca: 2.1664, adv_loss: 0.4898\n",
      "Epoch 3, Batch 122, loss_ca: 2.1453, adv_loss: 0.5155\n",
      "Epoch 3, Batch 123, loss_ca: 2.0113, adv_loss: 0.5373\n",
      "Epoch 3, Batch 124, loss_ca: 1.9374, adv_loss: 0.5564\n",
      "Epoch 3, Batch 125, loss_ca: 1.9324, adv_loss: 0.5717\n",
      "Epoch 3, Batch 126, loss_ca: 2.0659, adv_loss: 0.5756\n",
      "Epoch 3, Batch 127, loss_ca: 2.1521, adv_loss: 0.5628\n",
      "Epoch 3, Batch 128, loss_ca: 2.1618, adv_loss: 0.5965\n",
      "Epoch 3, Batch 129, loss_ca: 2.0933, adv_loss: 0.6532\n",
      "Epoch 3, Batch 130, loss_ca: 1.9841, adv_loss: 0.6442\n",
      "Epoch 3, Batch 131, loss_ca: 1.9378, adv_loss: 0.6112\n",
      "Epoch 3, Batch 132, loss_ca: 2.0028, adv_loss: 0.6253\n",
      "Epoch 3, Batch 133, loss_ca: 2.0428, adv_loss: 0.6332\n",
      "Epoch 3, Batch 134, loss_ca: 2.1085, adv_loss: 0.6814\n",
      "Epoch 3, Batch 135, loss_ca: 1.9916, adv_loss: 0.6433\n",
      "Epoch 3, Batch 136, loss_ca: 1.9398, adv_loss: 0.6358\n",
      "Epoch 3, Batch 137, loss_ca: 1.8598, adv_loss: 0.6331\n",
      "Epoch 3, Batch 138, loss_ca: 1.8751, adv_loss: 0.5965\n",
      "Epoch 3, Batch 139, loss_ca: 1.9964, adv_loss: 0.5701\n",
      "Epoch 3, Batch 140, loss_ca: 2.0782, adv_loss: 0.5508\n",
      "Epoch 3, Batch 141, loss_ca: 2.0170, adv_loss: 0.4654\n",
      "Epoch 3, Batch 142, loss_ca: 1.9749, adv_loss: 0.4283\n",
      "Epoch 3, Batch 143, loss_ca: 1.8351, adv_loss: 0.3740\n",
      "Epoch 3, Batch 144, loss_ca: 1.8318, adv_loss: 0.3452\n",
      "Epoch 3, Batch 145, loss_ca: 1.8270, adv_loss: 0.3208\n",
      "Epoch 3, Batch 146, loss_ca: 1.8684, adv_loss: 0.3091\n",
      "Epoch 3, Batch 147, loss_ca: 2.0677, adv_loss: 0.3043\n",
      "Epoch 3, Batch 148, loss_ca: 2.1211, adv_loss: 0.3211\n",
      "Epoch 3, Batch 149, loss_ca: 1.9900, adv_loss: 0.3525\n",
      "Epoch 3, Batch 150, loss_ca: 1.8880, adv_loss: 0.3805\n",
      "Epoch 3, Batch 151, loss_ca: 1.8113, adv_loss: 0.4141\n",
      "Epoch 3, Batch 152, loss_ca: 1.7143, adv_loss: 0.4096\n",
      "Epoch 3, Batch 153, loss_ca: 1.9029, adv_loss: 0.3998\n",
      "Epoch 3, Batch 154, loss_ca: 1.9033, adv_loss: 0.4133\n",
      "Epoch 3, Batch 155, loss_ca: 1.9844, adv_loss: 0.4717\n",
      "Epoch 3, Batch 156, loss_ca: 2.1175, adv_loss: 0.5356\n",
      "Epoch 3, Batch 157, loss_ca: 2.1787, adv_loss: 0.5749\n",
      "Epoch 3, Batch 158, loss_ca: 2.3410, adv_loss: 0.6011\n",
      "Epoch 3, Batch 159, loss_ca: 2.6653, adv_loss: 0.6006\n",
      "Epoch 3, Batch 160, loss_ca: 2.4649, adv_loss: 0.5466\n",
      "Epoch 3, Batch 161, loss_ca: 2.4426, adv_loss: 0.5092\n",
      "Epoch 3, Batch 162, loss_ca: 2.5969, adv_loss: 0.4486\n",
      "Epoch 3, Batch 163, loss_ca: 2.6250, adv_loss: 0.4651\n",
      "Epoch 3, Batch 164, loss_ca: 2.4888, adv_loss: 0.5207\n",
      "Epoch 3, Batch 165, loss_ca: 2.5687, adv_loss: 0.6403\n",
      "Epoch 3, Batch 166, loss_ca: 2.6006, adv_loss: 0.6825\n",
      "Epoch 3, Batch 167, loss_ca: 2.7374, adv_loss: 0.6888\n",
      "Epoch 3, Batch 168, loss_ca: 2.3320, adv_loss: 0.6082\n",
      "Epoch 3, Batch 169, loss_ca: 2.2593, adv_loss: 0.5167\n",
      "Epoch 3, Batch 170, loss_ca: 2.6274, adv_loss: 0.5140\n",
      "Epoch 3, Batch 171, loss_ca: 2.4540, adv_loss: 0.4868\n",
      "Epoch 3, Batch 172, loss_ca: 2.2663, adv_loss: 0.4189\n",
      "Epoch 3, Batch 173, loss_ca: 2.5287, adv_loss: 0.4295\n",
      "Epoch 3, Batch 174, loss_ca: 2.8342, adv_loss: 0.4066\n",
      "Epoch 3, Batch 175, loss_ca: 2.5691, adv_loss: 0.3527\n",
      "Epoch 3, Batch 176, loss_ca: 2.2865, adv_loss: 0.3369\n",
      "Epoch 3, Batch 177, loss_ca: 2.0229, adv_loss: 0.3649\n",
      "Epoch 3, Batch 178, loss_ca: 2.0496, adv_loss: 0.4083\n",
      "Epoch 3, Batch 179, loss_ca: 1.9164, adv_loss: 0.3713\n",
      "Epoch 3, Batch 180, loss_ca: 1.7463, adv_loss: 0.3579\n",
      "Epoch 3, Batch 181, loss_ca: 1.9436, adv_loss: 0.3862\n",
      "Epoch 3, Batch 182, loss_ca: 2.0418, adv_loss: 0.4424\n",
      "Epoch 3, Batch 183, loss_ca: 1.9867, adv_loss: 0.5119\n",
      "Epoch 3, Batch 184, loss_ca: 1.8750, adv_loss: 0.5739\n",
      "Epoch 3, Batch 185, loss_ca: 1.8182, adv_loss: 0.5893\n",
      "Epoch 3, Batch 186, loss_ca: 1.8316, adv_loss: 0.5990\n",
      "Epoch 3, Batch 187, loss_ca: 1.8488, adv_loss: 0.5931\n",
      "Epoch 3, Batch 188, loss_ca: 1.8304, adv_loss: 0.6465\n",
      "Epoch 3, Batch 189, loss_ca: 1.8918, adv_loss: 0.6524\n",
      "Epoch 3, Batch 190, loss_ca: 2.1272, adv_loss: 0.6281\n",
      "Epoch 3, Batch 191, loss_ca: 2.0841, adv_loss: 0.5948\n",
      "Epoch 3, Batch 192, loss_ca: 2.3088, adv_loss: 0.5614\n",
      "Epoch 3, Batch 193, loss_ca: 2.2501, adv_loss: 0.5360\n",
      "Epoch 3, Batch 194, loss_ca: 2.2298, adv_loss: 0.5428\n",
      "Epoch 3, Batch 195, loss_ca: 2.1927, adv_loss: 0.5569\n",
      "Epoch 3, Batch 196, loss_ca: 2.5015, adv_loss: 0.5714\n",
      "Epoch 3, Batch 197, loss_ca: 2.6563, adv_loss: 0.5875\n",
      "Epoch 3, Batch 198, loss_ca: 3.0377, adv_loss: 0.4992\n",
      "Epoch 3, Batch 199, loss_ca: 2.7590, adv_loss: 0.4580\n",
      "Epoch 3, Batch 200, loss_ca: 2.2626, adv_loss: 0.3958\n",
      "Epoch 3, Batch 201, loss_ca: 2.1342, adv_loss: 0.3220\n",
      "Epoch 3, Batch 202, loss_ca: 2.2704, adv_loss: 0.3234\n",
      "Epoch 3, Batch 203, loss_ca: 2.5362, adv_loss: 0.3857\n",
      "Epoch 3, Batch 204, loss_ca: 2.1410, adv_loss: 0.4253\n",
      "Epoch 3, Batch 205, loss_ca: 2.5869, adv_loss: 0.3902\n",
      "Epoch 3, Batch 206, loss_ca: 2.5772, adv_loss: 0.3648\n",
      "Epoch 3, Batch 207, loss_ca: 2.7286, adv_loss: 0.3338\n",
      "Epoch 3, Batch 208, loss_ca: 2.5170, adv_loss: 0.2771\n",
      "Epoch 3, Batch 209, loss_ca: 2.1791, adv_loss: 0.2987\n",
      "Epoch 3, Batch 210, loss_ca: 2.0437, adv_loss: 0.4105\n",
      "Epoch 3, Batch 211, loss_ca: 2.0434, adv_loss: 0.4765\n",
      "Epoch 3, Batch 212, loss_ca: 1.7418, adv_loss: 0.5560\n",
      "Epoch 3, Batch 213, loss_ca: 2.4141, adv_loss: 0.5689\n",
      "Epoch 3, Batch 214, loss_ca: 2.3843, adv_loss: 0.5486\n",
      "Epoch 3, Batch 215, loss_ca: 2.3084, adv_loss: 0.5658\n",
      "Epoch 3, Batch 216, loss_ca: 2.2043, adv_loss: 0.5561\n",
      "Epoch 3, Batch 217, loss_ca: 2.0265, adv_loss: 0.5583\n",
      "Epoch 3, Batch 218, loss_ca: 2.0083, adv_loss: 0.5666\n",
      "Epoch 3, Batch 219, loss_ca: 2.4016, adv_loss: 0.5482\n",
      "Epoch 3, Batch 220, loss_ca: 2.3376, adv_loss: 0.5142\n",
      "Epoch 3, Batch 221, loss_ca: 2.2629, adv_loss: 0.5687\n",
      "Epoch 3, Batch 222, loss_ca: 2.1275, adv_loss: 0.5917\n",
      "Epoch 3, Batch 223, loss_ca: 2.0504, adv_loss: 0.5904\n",
      "Epoch 3, Batch 224, loss_ca: 1.9217, adv_loss: 0.5559\n",
      "Epoch 3, Batch 225, loss_ca: 1.8405, adv_loss: 0.5279\n",
      "Epoch 3, Batch 226, loss_ca: 1.9128, adv_loss: 0.5081\n",
      "Epoch 3, Batch 227, loss_ca: 1.9776, adv_loss: 0.5327\n",
      "Epoch 3, Batch 228, loss_ca: 1.8271, adv_loss: 0.5454\n",
      "Epoch 4, Batch 4, loss_ca: 1.9978, adv_loss: 0.5535\n",
      "Epoch 4, Batch 5, loss_ca: 1.9185, adv_loss: 0.5653\n",
      "Epoch 4, Batch 6, loss_ca: 1.8823, adv_loss: 0.5405\n",
      "Epoch 4, Batch 7, loss_ca: 1.9176, adv_loss: 0.5109\n",
      "Epoch 4, Batch 8, loss_ca: 1.9475, adv_loss: 0.4830\n",
      "Epoch 4, Batch 9, loss_ca: 1.9104, adv_loss: 0.4650\n",
      "Epoch 4, Batch 10, loss_ca: 1.8920, adv_loss: 0.5063\n",
      "Epoch 4, Batch 11, loss_ca: 1.9111, adv_loss: 0.5313\n",
      "Epoch 4, Batch 12, loss_ca: 1.9492, adv_loss: 0.5191\n",
      "Epoch 4, Batch 13, loss_ca: 1.8367, adv_loss: 0.4952\n",
      "Epoch 4, Batch 14, loss_ca: 1.8933, adv_loss: 0.5140\n",
      "Epoch 4, Batch 15, loss_ca: 1.9181, adv_loss: 0.5317\n",
      "Epoch 4, Batch 16, loss_ca: 1.8650, adv_loss: 0.5453\n",
      "Epoch 4, Batch 17, loss_ca: 1.7847, adv_loss: 0.5308\n",
      "Epoch 4, Batch 18, loss_ca: 1.7823, adv_loss: 0.5119\n",
      "Epoch 4, Batch 19, loss_ca: 1.8993, adv_loss: 0.4831\n",
      "Epoch 4, Batch 20, loss_ca: 2.0184, adv_loss: 0.4527\n",
      "Epoch 4, Batch 21, loss_ca: 1.9141, adv_loss: 0.4647\n",
      "Epoch 4, Batch 22, loss_ca: 1.8040, adv_loss: 0.4931\n",
      "Epoch 4, Batch 23, loss_ca: 1.8577, adv_loss: 0.5308\n",
      "Epoch 4, Batch 24, loss_ca: 2.2152, adv_loss: 0.5222\n",
      "Epoch 4, Batch 25, loss_ca: 2.1824, adv_loss: 0.5383\n",
      "Epoch 4, Batch 26, loss_ca: 2.2174, adv_loss: 0.5074\n",
      "Epoch 4, Batch 27, loss_ca: 2.1947, adv_loss: 0.4899\n",
      "Epoch 4, Batch 28, loss_ca: 2.0748, adv_loss: 0.4896\n",
      "Epoch 4, Batch 29, loss_ca: 2.0328, adv_loss: 0.5209\n",
      "Epoch 4, Batch 30, loss_ca: 2.1795, adv_loss: 0.5401\n",
      "Epoch 4, Batch 31, loss_ca: 2.1044, adv_loss: 0.5754\n",
      "Epoch 4, Batch 32, loss_ca: 1.9772, adv_loss: 0.6017\n",
      "Epoch 4, Batch 33, loss_ca: 2.1191, adv_loss: 0.5840\n",
      "Epoch 4, Batch 34, loss_ca: 2.1252, adv_loss: 0.5493\n",
      "Epoch 4, Batch 35, loss_ca: 2.1053, adv_loss: 0.5132\n",
      "Epoch 4, Batch 36, loss_ca: 2.0777, adv_loss: 0.5441\n",
      "Epoch 4, Batch 37, loss_ca: 2.1038, adv_loss: 0.5834\n",
      "Epoch 4, Batch 38, loss_ca: 2.2598, adv_loss: 0.6099\n",
      "Epoch 4, Batch 39, loss_ca: 2.2188, adv_loss: 0.6083\n",
      "Epoch 4, Batch 40, loss_ca: 2.1599, adv_loss: 0.6102\n",
      "Epoch 4, Batch 41, loss_ca: 2.1699, adv_loss: 0.6203\n",
      "Epoch 4, Batch 42, loss_ca: 2.0982, adv_loss: 0.6071\n",
      "Epoch 4, Batch 43, loss_ca: 2.0536, adv_loss: 0.5869\n",
      "Epoch 4, Batch 44, loss_ca: 1.9571, adv_loss: 0.5776\n",
      "Epoch 4, Batch 45, loss_ca: 1.8952, adv_loss: 0.5733\n",
      "Epoch 4, Batch 46, loss_ca: 1.9101, adv_loss: 0.5507\n",
      "Epoch 4, Batch 47, loss_ca: 1.9289, adv_loss: 0.5436\n",
      "Epoch 4, Batch 48, loss_ca: 1.9114, adv_loss: 0.5510\n",
      "Epoch 4, Batch 49, loss_ca: 1.8460, adv_loss: 0.5557\n",
      "Epoch 4, Batch 50, loss_ca: 1.8486, adv_loss: 0.5619\n",
      "Epoch 4, Batch 51, loss_ca: 1.8943, adv_loss: 0.5771\n",
      "Epoch 4, Batch 52, loss_ca: 1.9659, adv_loss: 0.5553\n",
      "Epoch 4, Batch 53, loss_ca: 1.9365, adv_loss: 0.5534\n",
      "Epoch 4, Batch 54, loss_ca: 1.8303, adv_loss: 0.5365\n",
      "Epoch 4, Batch 55, loss_ca: 1.8249, adv_loss: 0.5038\n",
      "Epoch 4, Batch 56, loss_ca: 1.8168, adv_loss: 0.5023\n",
      "Epoch 4, Batch 57, loss_ca: 1.8161, adv_loss: 0.5250\n",
      "Epoch 4, Batch 58, loss_ca: 1.9769, adv_loss: 0.5259\n",
      "Epoch 4, Batch 59, loss_ca: 1.9979, adv_loss: 0.5346\n",
      "Epoch 4, Batch 60, loss_ca: 1.9633, adv_loss: 0.5409\n",
      "Epoch 4, Batch 61, loss_ca: 1.9272, adv_loss: 0.5427\n",
      "Epoch 4, Batch 62, loss_ca: 2.0186, adv_loss: 0.5374\n",
      "Epoch 4, Batch 63, loss_ca: 2.0450, adv_loss: 0.5322\n",
      "Epoch 4, Batch 64, loss_ca: 1.9551, adv_loss: 0.5506\n",
      "Epoch 4, Batch 65, loss_ca: 1.8790, adv_loss: 0.5443\n",
      "Epoch 4, Batch 66, loss_ca: 1.9325, adv_loss: 0.5450\n",
      "Epoch 4, Batch 67, loss_ca: 1.9405, adv_loss: 0.5337\n",
      "Epoch 4, Batch 68, loss_ca: 1.9871, adv_loss: 0.5301\n",
      "Epoch 4, Batch 69, loss_ca: 1.8688, adv_loss: 0.4969\n",
      "Epoch 4, Batch 70, loss_ca: 1.8936, adv_loss: 0.4911\n",
      "Epoch 4, Batch 71, loss_ca: 1.8096, adv_loss: 0.4946\n",
      "Epoch 4, Batch 72, loss_ca: 1.7420, adv_loss: 0.5060\n",
      "Epoch 4, Batch 73, loss_ca: 1.7248, adv_loss: 0.5234\n",
      "Epoch 4, Batch 74, loss_ca: 1.7738, adv_loss: 0.5436\n",
      "Epoch 4, Batch 75, loss_ca: 1.7752, adv_loss: 0.5387\n",
      "Epoch 4, Batch 76, loss_ca: 1.7310, adv_loss: 0.5530\n",
      "Epoch 4, Batch 77, loss_ca: 1.6996, adv_loss: 0.5269\n",
      "Epoch 4, Batch 78, loss_ca: 1.7493, adv_loss: 0.5193\n",
      "Epoch 4, Batch 79, loss_ca: 1.7527, adv_loss: 0.4981\n",
      "Epoch 4, Batch 80, loss_ca: 1.7602, adv_loss: 0.4671\n",
      "Epoch 4, Batch 81, loss_ca: 1.7356, adv_loss: 0.4520\n",
      "Epoch 4, Batch 82, loss_ca: 1.6996, adv_loss: 0.4317\n",
      "Epoch 4, Batch 83, loss_ca: 1.7777, adv_loss: 0.4400\n",
      "Epoch 4, Batch 84, loss_ca: 1.8308, adv_loss: 0.4631\n",
      "Epoch 4, Batch 85, loss_ca: 1.8459, adv_loss: 0.4715\n",
      "Epoch 4, Batch 86, loss_ca: 1.9139, adv_loss: 0.4480\n",
      "Epoch 4, Batch 87, loss_ca: 1.8616, adv_loss: 0.4685\n",
      "Epoch 4, Batch 88, loss_ca: 1.7970, adv_loss: 0.5183\n",
      "Epoch 4, Batch 89, loss_ca: 1.8661, adv_loss: 0.5406\n",
      "Epoch 4, Batch 90, loss_ca: 1.8224, adv_loss: 0.5551\n",
      "Epoch 4, Batch 91, loss_ca: 1.9162, adv_loss: 0.5516\n",
      "Epoch 4, Batch 92, loss_ca: 1.8950, adv_loss: 0.5472\n",
      "Epoch 4, Batch 93, loss_ca: 2.0347, adv_loss: 0.5362\n",
      "Epoch 4, Batch 94, loss_ca: 1.9403, adv_loss: 0.5459\n",
      "Epoch 4, Batch 95, loss_ca: 2.0592, adv_loss: 0.5528\n",
      "Epoch 4, Batch 96, loss_ca: 2.0239, adv_loss: 0.5562\n",
      "Epoch 4, Batch 97, loss_ca: 1.9804, adv_loss: 0.5320\n",
      "Epoch 4, Batch 98, loss_ca: 1.9662, adv_loss: 0.5195\n",
      "Epoch 4, Batch 99, loss_ca: 1.8619, adv_loss: 0.5338\n",
      "Epoch 4, Batch 100, loss_ca: 1.9925, adv_loss: 0.5515\n",
      "Epoch 4, Batch 101, loss_ca: 2.1056, adv_loss: 0.5703\n",
      "Epoch 4, Batch 102, loss_ca: 2.0950, adv_loss: 0.5782\n",
      "Epoch 4, Batch 103, loss_ca: 2.0645, adv_loss: 0.5716\n",
      "Epoch 4, Batch 104, loss_ca: 2.1608, adv_loss: 0.5511\n",
      "Epoch 4, Batch 105, loss_ca: 2.1533, adv_loss: 0.5330\n",
      "Epoch 4, Batch 106, loss_ca: 2.1162, adv_loss: 0.5331\n",
      "Epoch 4, Batch 107, loss_ca: 2.2281, adv_loss: 0.5571\n",
      "Epoch 4, Batch 108, loss_ca: 2.2249, adv_loss: 0.5791\n",
      "Epoch 4, Batch 109, loss_ca: 2.2392, adv_loss: 0.5975\n",
      "Epoch 4, Batch 110, loss_ca: 2.1164, adv_loss: 0.5871\n",
      "Epoch 4, Batch 111, loss_ca: 1.9849, adv_loss: 0.5524\n",
      "Epoch 4, Batch 112, loss_ca: 1.9481, adv_loss: 0.5244\n",
      "Epoch 4, Batch 113, loss_ca: 1.8390, adv_loss: 0.4870\n",
      "Epoch 4, Batch 114, loss_ca: 1.8383, adv_loss: 0.4797\n",
      "Epoch 4, Batch 115, loss_ca: 1.8039, adv_loss: 0.5218\n",
      "Epoch 4, Batch 116, loss_ca: 1.8214, adv_loss: 0.5361\n",
      "Epoch 4, Batch 117, loss_ca: 1.7935, adv_loss: 0.5358\n",
      "Epoch 4, Batch 118, loss_ca: 1.7643, adv_loss: 0.5183\n",
      "Epoch 4, Batch 119, loss_ca: 1.7916, adv_loss: 0.5197\n",
      "Epoch 4, Batch 120, loss_ca: 1.8608, adv_loss: 0.5234\n",
      "Epoch 4, Batch 121, loss_ca: 2.0648, adv_loss: 0.5203\n",
      "Epoch 4, Batch 122, loss_ca: 2.0956, adv_loss: 0.5189\n",
      "Epoch 4, Batch 123, loss_ca: 2.0459, adv_loss: 0.5367\n",
      "Epoch 4, Batch 124, loss_ca: 2.0224, adv_loss: 0.5634\n",
      "Epoch 4, Batch 125, loss_ca: 2.0128, adv_loss: 0.5692\n",
      "Epoch 4, Batch 126, loss_ca: 2.0356, adv_loss: 0.5341\n",
      "Epoch 4, Batch 127, loss_ca: 2.0800, adv_loss: 0.5081\n",
      "Epoch 4, Batch 128, loss_ca: 2.0764, adv_loss: 0.5095\n",
      "Epoch 4, Batch 129, loss_ca: 1.9840, adv_loss: 0.4955\n",
      "Epoch 4, Batch 130, loss_ca: 1.9036, adv_loss: 0.4887\n",
      "Epoch 4, Batch 131, loss_ca: 1.8941, adv_loss: 0.4945\n",
      "Epoch 4, Batch 132, loss_ca: 1.9899, adv_loss: 0.5198\n",
      "Epoch 4, Batch 133, loss_ca: 2.0496, adv_loss: 0.5383\n",
      "Epoch 4, Batch 134, loss_ca: 2.0575, adv_loss: 0.5783\n",
      "Epoch 4, Batch 135, loss_ca: 1.9718, adv_loss: 0.6074\n",
      "Epoch 4, Batch 136, loss_ca: 1.9341, adv_loss: 0.6109\n",
      "Epoch 4, Batch 137, loss_ca: 1.8225, adv_loss: 0.6030\n",
      "Epoch 4, Batch 138, loss_ca: 1.8787, adv_loss: 0.6350\n",
      "Epoch 4, Batch 139, loss_ca: 1.9188, adv_loss: 0.6382\n",
      "Epoch 4, Batch 140, loss_ca: 1.9239, adv_loss: 0.6030\n",
      "Epoch 4, Batch 141, loss_ca: 1.9015, adv_loss: 0.5716\n",
      "Epoch 4, Batch 142, loss_ca: 1.9555, adv_loss: 0.5571\n",
      "Epoch 4, Batch 143, loss_ca: 1.8501, adv_loss: 0.5527\n",
      "Epoch 4, Batch 144, loss_ca: 1.8318, adv_loss: 0.5408\n",
      "Epoch 4, Batch 145, loss_ca: 1.8163, adv_loss: 0.5564\n",
      "Epoch 4, Batch 146, loss_ca: 1.8053, adv_loss: 0.5373\n",
      "Epoch 4, Batch 147, loss_ca: 2.0011, adv_loss: 0.5312\n",
      "Epoch 4, Batch 148, loss_ca: 2.0662, adv_loss: 0.5065\n",
      "Epoch 4, Batch 149, loss_ca: 2.0429, adv_loss: 0.5242\n",
      "Epoch 4, Batch 150, loss_ca: 1.9755, adv_loss: 0.5269\n",
      "Epoch 4, Batch 151, loss_ca: 1.8835, adv_loss: 0.4951\n",
      "Epoch 4, Batch 152, loss_ca: 1.7592, adv_loss: 0.4549\n",
      "Epoch 4, Batch 153, loss_ca: 1.8435, adv_loss: 0.4242\n",
      "Epoch 4, Batch 154, loss_ca: 1.8446, adv_loss: 0.4125\n",
      "Epoch 4, Batch 155, loss_ca: 1.9665, adv_loss: 0.4400\n",
      "Epoch 4, Batch 156, loss_ca: 2.0476, adv_loss: 0.4699\n",
      "Epoch 4, Batch 157, loss_ca: 2.1152, adv_loss: 0.5330\n",
      "Epoch 4, Batch 158, loss_ca: 2.3167, adv_loss: 0.5268\n",
      "Epoch 4, Batch 159, loss_ca: 2.6777, adv_loss: 0.5122\n",
      "Epoch 4, Batch 160, loss_ca: 2.4930, adv_loss: 0.5256\n",
      "Epoch 4, Batch 161, loss_ca: 2.2265, adv_loss: 0.4901\n",
      "Epoch 4, Batch 162, loss_ca: 2.6277, adv_loss: 0.4593\n",
      "Epoch 4, Batch 163, loss_ca: 2.5242, adv_loss: 0.4794\n",
      "Epoch 4, Batch 164, loss_ca: 2.5964, adv_loss: 0.5098\n",
      "Epoch 4, Batch 165, loss_ca: 2.6653, adv_loss: 0.5670\n",
      "Epoch 4, Batch 166, loss_ca: 2.4742, adv_loss: 0.6329\n",
      "Epoch 4, Batch 167, loss_ca: 2.7475, adv_loss: 0.6746\n",
      "Epoch 4, Batch 168, loss_ca: 2.6843, adv_loss: 0.6909\n",
      "Epoch 4, Batch 169, loss_ca: 2.5266, adv_loss: 0.6962\n",
      "Epoch 4, Batch 170, loss_ca: 2.6243, adv_loss: 0.6667\n",
      "Epoch 4, Batch 171, loss_ca: 2.4908, adv_loss: 0.6253\n",
      "Epoch 4, Batch 172, loss_ca: 2.3089, adv_loss: 0.5819\n",
      "Epoch 4, Batch 173, loss_ca: 2.3006, adv_loss: 0.5602\n",
      "Epoch 4, Batch 174, loss_ca: 2.6197, adv_loss: 0.5792\n",
      "Epoch 4, Batch 175, loss_ca: 2.4159, adv_loss: 0.5481\n",
      "Epoch 4, Batch 176, loss_ca: 2.1805, adv_loss: 0.5108\n",
      "Epoch 4, Batch 177, loss_ca: 1.9497, adv_loss: 0.4921\n",
      "Epoch 4, Batch 178, loss_ca: 2.0145, adv_loss: 0.5116\n",
      "Epoch 4, Batch 179, loss_ca: 1.9335, adv_loss: 0.5364\n",
      "Epoch 4, Batch 180, loss_ca: 1.7825, adv_loss: 0.5550\n",
      "Epoch 4, Batch 181, loss_ca: 1.8245, adv_loss: 0.5861\n",
      "Epoch 4, Batch 182, loss_ca: 1.9038, adv_loss: 0.5543\n",
      "Epoch 4, Batch 183, loss_ca: 2.0506, adv_loss: 0.5164\n",
      "Epoch 4, Batch 184, loss_ca: 2.0097, adv_loss: 0.5113\n",
      "Epoch 4, Batch 185, loss_ca: 1.8535, adv_loss: 0.5010\n",
      "Epoch 4, Batch 186, loss_ca: 1.7575, adv_loss: 0.4803\n",
      "Epoch 4, Batch 187, loss_ca: 1.7500, adv_loss: 0.4625\n",
      "Epoch 4, Batch 188, loss_ca: 1.7487, adv_loss: 0.4572\n",
      "Epoch 4, Batch 189, loss_ca: 1.8787, adv_loss: 0.4645\n",
      "Epoch 4, Batch 190, loss_ca: 2.0975, adv_loss: 0.4751\n",
      "Epoch 4, Batch 191, loss_ca: 2.1815, adv_loss: 0.4826\n",
      "Epoch 4, Batch 192, loss_ca: 2.3906, adv_loss: 0.4884\n",
      "Epoch 4, Batch 193, loss_ca: 2.3154, adv_loss: 0.5108\n",
      "Epoch 4, Batch 194, loss_ca: 2.3143, adv_loss: 0.5367\n",
      "Epoch 4, Batch 195, loss_ca: 2.1583, adv_loss: 0.5141\n",
      "Epoch 4, Batch 196, loss_ca: 2.4758, adv_loss: 0.4877\n",
      "Epoch 4, Batch 197, loss_ca: 2.2455, adv_loss: 0.4696\n",
      "Epoch 4, Batch 198, loss_ca: 2.6166, adv_loss: 0.4398\n",
      "Epoch 4, Batch 199, loss_ca: 2.3939, adv_loss: 0.3895\n",
      "Epoch 4, Batch 200, loss_ca: 2.0922, adv_loss: 0.3581\n",
      "Epoch 4, Batch 201, loss_ca: 2.1099, adv_loss: 0.4376\n",
      "Epoch 4, Batch 202, loss_ca: 2.2534, adv_loss: 0.3904\n",
      "Epoch 4, Batch 203, loss_ca: 2.5285, adv_loss: 0.5534\n",
      "Epoch 4, Batch 204, loss_ca: 2.4154, adv_loss: 0.6362\n",
      "Epoch 4, Batch 205, loss_ca: 2.8525, adv_loss: 0.6258\n",
      "Epoch 4, Batch 206, loss_ca: 2.4760, adv_loss: 0.6162\n",
      "Epoch 4, Batch 207, loss_ca: 2.4818, adv_loss: 0.6020\n",
      "Epoch 4, Batch 208, loss_ca: 2.2695, adv_loss: 0.5309\n",
      "Epoch 4, Batch 209, loss_ca: 2.0058, adv_loss: 0.5478\n",
      "Epoch 4, Batch 210, loss_ca: 2.1815, adv_loss: 0.5957\n",
      "Epoch 4, Batch 211, loss_ca: 2.0548, adv_loss: 0.6073\n",
      "Epoch 4, Batch 212, loss_ca: 1.6969, adv_loss: 0.6386\n",
      "Epoch 4, Batch 213, loss_ca: 2.3385, adv_loss: 0.6924\n",
      "Epoch 4, Batch 214, loss_ca: 2.2702, adv_loss: 0.7026\n",
      "Epoch 4, Batch 215, loss_ca: 2.1131, adv_loss: 0.6923\n",
      "Epoch 4, Batch 216, loss_ca: 2.1339, adv_loss: 0.6520\n",
      "Epoch 4, Batch 217, loss_ca: 2.0524, adv_loss: 0.6322\n",
      "Epoch 4, Batch 218, loss_ca: 2.1306, adv_loss: 0.6214\n",
      "Epoch 4, Batch 219, loss_ca: 2.5096, adv_loss: 0.5912\n",
      "Epoch 4, Batch 220, loss_ca: 2.4834, adv_loss: 0.6043\n",
      "Epoch 4, Batch 221, loss_ca: 2.4968, adv_loss: 0.6026\n",
      "Epoch 4, Batch 222, loss_ca: 2.3460, adv_loss: 0.5879\n",
      "Epoch 4, Batch 223, loss_ca: 2.2387, adv_loss: 0.5729\n",
      "Epoch 4, Batch 224, loss_ca: 2.1047, adv_loss: 0.5691\n",
      "Epoch 4, Batch 225, loss_ca: 1.9691, adv_loss: 0.5440\n",
      "Epoch 4, Batch 226, loss_ca: 1.9194, adv_loss: 0.5150\n",
      "Epoch 4, Batch 227, loss_ca: 1.9610, adv_loss: 0.4999\n",
      "Epoch 4, Batch 228, loss_ca: 2.0139, adv_loss: 0.5148\n",
      "Epoch 5, Batch 5, loss_ca: 1.8915, adv_loss: 0.4734\n",
      "Epoch 5, Batch 6, loss_ca: 1.8096, adv_loss: 0.4729\n",
      "Epoch 5, Batch 7, loss_ca: 1.7528, adv_loss: 0.4411\n",
      "Epoch 5, Batch 8, loss_ca: 1.8770, adv_loss: 0.4593\n",
      "Epoch 5, Batch 9, loss_ca: 1.8568, adv_loss: 0.4462\n",
      "Epoch 5, Batch 10, loss_ca: 1.8069, adv_loss: 0.5039\n",
      "Epoch 5, Batch 11, loss_ca: 1.8986, adv_loss: 0.5572\n",
      "Epoch 5, Batch 12, loss_ca: 1.9967, adv_loss: 0.5654\n",
      "Epoch 5, Batch 13, loss_ca: 1.8604, adv_loss: 0.5355\n",
      "Epoch 5, Batch 14, loss_ca: 1.9805, adv_loss: 0.5446\n",
      "Epoch 5, Batch 15, loss_ca: 1.9841, adv_loss: 0.5403\n",
      "Epoch 5, Batch 16, loss_ca: 1.7261, adv_loss: 0.5451\n",
      "Epoch 5, Batch 17, loss_ca: 1.7174, adv_loss: 0.5531\n",
      "Epoch 5, Batch 18, loss_ca: 1.7374, adv_loss: 0.5197\n",
      "Epoch 5, Batch 19, loss_ca: 1.8046, adv_loss: 0.5318\n",
      "Epoch 5, Batch 20, loss_ca: 2.0411, adv_loss: 0.5088\n",
      "Epoch 5, Batch 21, loss_ca: 1.9689, adv_loss: 0.4899\n",
      "Epoch 5, Batch 22, loss_ca: 1.8216, adv_loss: 0.4940\n",
      "Epoch 5, Batch 23, loss_ca: 1.8508, adv_loss: 0.4821\n",
      "Epoch 5, Batch 24, loss_ca: 2.1859, adv_loss: 0.5022\n",
      "Epoch 5, Batch 25, loss_ca: 2.0925, adv_loss: 0.5100\n",
      "Epoch 5, Batch 26, loss_ca: 2.2073, adv_loss: 0.4888\n",
      "Epoch 5, Batch 27, loss_ca: 2.1143, adv_loss: 0.4930\n",
      "Epoch 5, Batch 28, loss_ca: 1.9837, adv_loss: 0.4966\n",
      "Epoch 5, Batch 29, loss_ca: 2.0058, adv_loss: 0.5033\n",
      "Epoch 5, Batch 30, loss_ca: 2.1494, adv_loss: 0.4956\n",
      "Epoch 5, Batch 31, loss_ca: 2.1262, adv_loss: 0.5134\n",
      "Epoch 5, Batch 32, loss_ca: 1.9836, adv_loss: 0.5296\n",
      "Epoch 5, Batch 33, loss_ca: 2.1337, adv_loss: 0.5592\n",
      "Epoch 5, Batch 34, loss_ca: 2.1334, adv_loss: 0.5508\n",
      "Epoch 5, Batch 35, loss_ca: 2.1793, adv_loss: 0.5333\n",
      "Epoch 5, Batch 36, loss_ca: 2.1343, adv_loss: 0.5687\n",
      "Epoch 5, Batch 37, loss_ca: 2.0478, adv_loss: 0.5893\n",
      "Epoch 5, Batch 38, loss_ca: 2.1067, adv_loss: 0.5981\n",
      "Epoch 5, Batch 39, loss_ca: 2.1624, adv_loss: 0.6094\n",
      "Epoch 5, Batch 40, loss_ca: 2.1557, adv_loss: 0.6067\n",
      "Epoch 5, Batch 41, loss_ca: 2.2975, adv_loss: 0.6021\n",
      "Epoch 5, Batch 42, loss_ca: 2.1865, adv_loss: 0.5959\n",
      "Epoch 5, Batch 43, loss_ca: 2.0444, adv_loss: 0.5672\n",
      "Epoch 5, Batch 44, loss_ca: 2.0117, adv_loss: 0.5335\n",
      "Epoch 5, Batch 45, loss_ca: 1.9237, adv_loss: 0.5203\n",
      "Epoch 5, Batch 46, loss_ca: 1.8741, adv_loss: 0.5201\n",
      "Epoch 5, Batch 47, loss_ca: 1.9713, adv_loss: 0.5041\n",
      "Epoch 5, Batch 48, loss_ca: 1.9907, adv_loss: 0.4935\n",
      "Epoch 5, Batch 49, loss_ca: 1.8820, adv_loss: 0.4924\n",
      "Epoch 5, Batch 50, loss_ca: 1.8720, adv_loss: 0.5287\n",
      "Epoch 5, Batch 51, loss_ca: 1.8789, adv_loss: 0.5511\n",
      "Epoch 5, Batch 52, loss_ca: 1.9553, adv_loss: 0.5563\n",
      "Epoch 5, Batch 53, loss_ca: 1.9420, adv_loss: 0.5501\n",
      "Epoch 5, Batch 54, loss_ca: 1.8919, adv_loss: 0.5100\n",
      "Epoch 5, Batch 55, loss_ca: 1.8291, adv_loss: 0.5193\n",
      "Epoch 5, Batch 56, loss_ca: 1.8106, adv_loss: 0.5250\n",
      "Epoch 5, Batch 57, loss_ca: 1.8412, adv_loss: 0.5438\n",
      "Epoch 5, Batch 58, loss_ca: 2.0587, adv_loss: 0.5592\n",
      "Epoch 5, Batch 59, loss_ca: 1.9366, adv_loss: 0.5576\n",
      "Epoch 5, Batch 60, loss_ca: 1.9082, adv_loss: 0.5363\n",
      "Epoch 5, Batch 61, loss_ca: 1.8989, adv_loss: 0.4904\n",
      "Epoch 5, Batch 62, loss_ca: 1.9032, adv_loss: 0.5051\n",
      "Epoch 5, Batch 63, loss_ca: 1.9231, adv_loss: 0.5130\n",
      "Epoch 5, Batch 64, loss_ca: 1.9539, adv_loss: 0.5161\n",
      "Epoch 5, Batch 65, loss_ca: 1.9003, adv_loss: 0.5253\n",
      "Epoch 5, Batch 66, loss_ca: 1.8690, adv_loss: 0.5484\n",
      "Epoch 5, Batch 67, loss_ca: 1.8722, adv_loss: 0.5355\n",
      "Epoch 5, Batch 68, loss_ca: 1.9074, adv_loss: 0.5458\n",
      "Epoch 5, Batch 69, loss_ca: 1.8426, adv_loss: 0.5395\n",
      "Epoch 5, Batch 70, loss_ca: 1.8748, adv_loss: 0.5694\n",
      "Epoch 5, Batch 71, loss_ca: 1.8792, adv_loss: 0.5696\n",
      "Epoch 5, Batch 72, loss_ca: 1.8359, adv_loss: 0.5549\n",
      "Epoch 5, Batch 73, loss_ca: 1.7584, adv_loss: 0.5504\n",
      "Epoch 5, Batch 74, loss_ca: 1.7268, adv_loss: 0.5565\n",
      "Epoch 5, Batch 75, loss_ca: 1.7289, adv_loss: 0.5677\n",
      "Epoch 5, Batch 76, loss_ca: 1.7292, adv_loss: 0.5555\n",
      "Epoch 5, Batch 77, loss_ca: 1.7481, adv_loss: 0.5428\n",
      "Epoch 5, Batch 78, loss_ca: 1.7529, adv_loss: 0.5196\n",
      "Epoch 5, Batch 79, loss_ca: 1.7312, adv_loss: 0.5114\n",
      "Epoch 5, Batch 80, loss_ca: 1.7021, adv_loss: 0.4902\n",
      "Epoch 5, Batch 81, loss_ca: 1.6895, adv_loss: 0.4800\n",
      "Epoch 5, Batch 82, loss_ca: 1.7146, adv_loss: 0.4758\n",
      "Epoch 5, Batch 83, loss_ca: 1.7536, adv_loss: 0.4668\n",
      "Epoch 5, Batch 84, loss_ca: 1.8047, adv_loss: 0.4800\n",
      "Epoch 5, Batch 85, loss_ca: 1.8318, adv_loss: 0.4976\n",
      "Epoch 5, Batch 86, loss_ca: 1.8770, adv_loss: 0.5218\n",
      "Epoch 5, Batch 87, loss_ca: 1.8919, adv_loss: 0.5411\n",
      "Epoch 5, Batch 88, loss_ca: 1.8237, adv_loss: 0.5367\n",
      "Epoch 5, Batch 89, loss_ca: 1.8605, adv_loss: 0.5143\n",
      "Epoch 5, Batch 90, loss_ca: 1.8007, adv_loss: 0.5166\n",
      "Epoch 5, Batch 91, loss_ca: 1.9664, adv_loss: 0.5351\n",
      "Epoch 5, Batch 92, loss_ca: 1.9461, adv_loss: 0.5311\n",
      "Epoch 5, Batch 93, loss_ca: 2.0205, adv_loss: 0.5287\n",
      "Epoch 5, Batch 94, loss_ca: 1.9342, adv_loss: 0.5460\n",
      "Epoch 5, Batch 95, loss_ca: 2.0344, adv_loss: 0.5480\n",
      "Epoch 5, Batch 96, loss_ca: 1.9831, adv_loss: 0.5327\n",
      "Epoch 5, Batch 97, loss_ca: 1.9451, adv_loss: 0.5165\n",
      "Epoch 5, Batch 98, loss_ca: 1.9155, adv_loss: 0.4977\n",
      "Epoch 5, Batch 99, loss_ca: 1.8489, adv_loss: 0.5166\n",
      "Epoch 5, Batch 100, loss_ca: 1.9647, adv_loss: 0.5116\n",
      "Epoch 5, Batch 101, loss_ca: 2.0841, adv_loss: 0.5230\n",
      "Epoch 5, Batch 102, loss_ca: 2.0669, adv_loss: 0.5324\n",
      "Epoch 5, Batch 103, loss_ca: 2.0475, adv_loss: 0.5326\n",
      "Epoch 5, Batch 104, loss_ca: 2.0933, adv_loss: 0.5258\n",
      "Epoch 5, Batch 105, loss_ca: 2.1157, adv_loss: 0.5170\n",
      "Epoch 5, Batch 106, loss_ca: 2.0792, adv_loss: 0.5157\n",
      "Epoch 5, Batch 107, loss_ca: 2.1835, adv_loss: 0.5346\n",
      "Epoch 5, Batch 108, loss_ca: 2.1525, adv_loss: 0.5745\n",
      "Epoch 5, Batch 109, loss_ca: 2.2492, adv_loss: 0.6178\n",
      "Epoch 5, Batch 110, loss_ca: 2.1808, adv_loss: 0.6282\n",
      "Epoch 5, Batch 111, loss_ca: 2.1152, adv_loss: 0.6167\n",
      "Epoch 5, Batch 112, loss_ca: 2.0561, adv_loss: 0.5902\n",
      "Epoch 5, Batch 113, loss_ca: 1.9409, adv_loss: 0.5616\n",
      "Epoch 5, Batch 114, loss_ca: 1.9122, adv_loss: 0.5096\n",
      "Epoch 5, Batch 115, loss_ca: 1.8415, adv_loss: 0.5176\n",
      "Epoch 5, Batch 116, loss_ca: 1.9228, adv_loss: 0.5318\n",
      "Epoch 5, Batch 117, loss_ca: 1.9195, adv_loss: 0.5260\n",
      "Epoch 5, Batch 118, loss_ca: 1.8268, adv_loss: 0.5046\n",
      "Epoch 5, Batch 119, loss_ca: 1.7635, adv_loss: 0.4744\n",
      "Epoch 5, Batch 120, loss_ca: 1.7940, adv_loss: 0.4625\n",
      "Epoch 5, Batch 121, loss_ca: 2.0399, adv_loss: 0.4586\n",
      "Epoch 5, Batch 122, loss_ca: 2.0837, adv_loss: 0.4768\n",
      "Epoch 5, Batch 123, loss_ca: 2.0125, adv_loss: 0.4876\n",
      "Epoch 5, Batch 124, loss_ca: 2.0343, adv_loss: 0.5137\n",
      "Epoch 5, Batch 125, loss_ca: 1.9707, adv_loss: 0.5268\n",
      "Epoch 5, Batch 126, loss_ca: 1.9784, adv_loss: 0.5429\n",
      "Epoch 5, Batch 127, loss_ca: 2.1098, adv_loss: 0.5357\n",
      "Epoch 5, Batch 128, loss_ca: 2.0920, adv_loss: 0.5463\n",
      "Epoch 5, Batch 129, loss_ca: 2.0298, adv_loss: 0.5327\n",
      "Epoch 5, Batch 130, loss_ca: 1.9472, adv_loss: 0.5379\n",
      "Epoch 5, Batch 131, loss_ca: 1.9474, adv_loss: 0.5399\n",
      "Epoch 5, Batch 132, loss_ca: 1.9944, adv_loss: 0.5391\n",
      "Epoch 5, Batch 133, loss_ca: 1.9769, adv_loss: 0.5321\n",
      "Epoch 5, Batch 134, loss_ca: 1.9639, adv_loss: 0.5570\n",
      "Epoch 5, Batch 135, loss_ca: 1.9063, adv_loss: 0.5694\n",
      "Epoch 5, Batch 136, loss_ca: 1.9473, adv_loss: 0.5887\n",
      "Epoch 5, Batch 137, loss_ca: 1.9310, adv_loss: 0.5806\n",
      "Epoch 5, Batch 138, loss_ca: 1.9097, adv_loss: 0.5837\n",
      "Epoch 5, Batch 139, loss_ca: 1.9248, adv_loss: 0.5715\n",
      "Epoch 5, Batch 140, loss_ca: 1.9098, adv_loss: 0.5361\n",
      "Epoch 5, Batch 141, loss_ca: 1.8765, adv_loss: 0.5127\n",
      "Epoch 5, Batch 142, loss_ca: 1.9433, adv_loss: 0.5110\n",
      "Epoch 5, Batch 143, loss_ca: 1.8473, adv_loss: 0.4884\n",
      "Epoch 5, Batch 144, loss_ca: 1.8482, adv_loss: 0.4929\n",
      "Epoch 5, Batch 145, loss_ca: 1.8219, adv_loss: 0.4873\n",
      "Epoch 5, Batch 146, loss_ca: 1.8003, adv_loss: 0.4960\n",
      "Epoch 5, Batch 147, loss_ca: 1.9590, adv_loss: 0.5233\n",
      "Epoch 5, Batch 148, loss_ca: 2.0081, adv_loss: 0.5401\n",
      "Epoch 5, Batch 149, loss_ca: 1.9563, adv_loss: 0.5624\n",
      "Epoch 5, Batch 150, loss_ca: 1.9468, adv_loss: 0.5635\n",
      "Epoch 5, Batch 151, loss_ca: 1.8924, adv_loss: 0.5655\n",
      "Epoch 5, Batch 152, loss_ca: 1.8086, adv_loss: 0.5547\n",
      "Epoch 5, Batch 153, loss_ca: 1.8938, adv_loss: 0.5307\n",
      "Epoch 5, Batch 154, loss_ca: 1.8641, adv_loss: 0.5070\n",
      "Epoch 5, Batch 155, loss_ca: 1.9199, adv_loss: 0.4876\n",
      "Epoch 5, Batch 156, loss_ca: 1.9709, adv_loss: 0.4874\n",
      "Epoch 5, Batch 157, loss_ca: 1.9816, adv_loss: 0.5101\n",
      "Epoch 5, Batch 158, loss_ca: 2.2610, adv_loss: 0.4993\n",
      "Epoch 5, Batch 159, loss_ca: 2.6551, adv_loss: 0.4836\n",
      "Epoch 5, Batch 160, loss_ca: 2.4504, adv_loss: 0.4683\n",
      "Epoch 5, Batch 161, loss_ca: 2.2293, adv_loss: 0.4465\n",
      "Epoch 5, Batch 162, loss_ca: 2.5545, adv_loss: 0.4295\n",
      "Epoch 5, Batch 163, loss_ca: 2.4662, adv_loss: 0.4441\n",
      "Epoch 5, Batch 164, loss_ca: 2.5394, adv_loss: 0.4668\n",
      "Epoch 5, Batch 165, loss_ca: 2.6354, adv_loss: 0.5081\n",
      "Epoch 5, Batch 166, loss_ca: 2.4217, adv_loss: 0.5887\n",
      "Epoch 5, Batch 167, loss_ca: 2.6825, adv_loss: 0.6258\n",
      "Epoch 5, Batch 168, loss_ca: 2.5625, adv_loss: 0.6214\n",
      "Epoch 5, Batch 169, loss_ca: 2.3939, adv_loss: 0.6005\n",
      "Epoch 5, Batch 170, loss_ca: 2.5441, adv_loss: 0.5811\n",
      "Epoch 5, Batch 171, loss_ca: 2.4733, adv_loss: 0.5753\n",
      "Epoch 5, Batch 172, loss_ca: 2.2725, adv_loss: 0.5501\n",
      "Epoch 5, Batch 173, loss_ca: 2.2035, adv_loss: 0.5336\n",
      "Epoch 5, Batch 174, loss_ca: 2.4979, adv_loss: 0.5381\n",
      "Epoch 5, Batch 175, loss_ca: 2.3121, adv_loss: 0.4964\n",
      "Epoch 5, Batch 176, loss_ca: 2.0543, adv_loss: 0.4620\n",
      "Epoch 5, Batch 177, loss_ca: 1.8441, adv_loss: 0.4804\n",
      "Epoch 5, Batch 178, loss_ca: 1.9790, adv_loss: 0.4901\n",
      "Epoch 5, Batch 179, loss_ca: 1.9176, adv_loss: 0.5083\n",
      "Epoch 5, Batch 180, loss_ca: 1.7833, adv_loss: 0.5158\n",
      "Epoch 5, Batch 181, loss_ca: 1.8506, adv_loss: 0.5512\n",
      "Epoch 5, Batch 182, loss_ca: 1.8762, adv_loss: 0.5114\n",
      "Epoch 5, Batch 183, loss_ca: 1.9873, adv_loss: 0.5242\n",
      "Epoch 5, Batch 184, loss_ca: 1.9003, adv_loss: 0.5138\n",
      "Epoch 5, Batch 185, loss_ca: 1.8346, adv_loss: 0.5053\n",
      "Epoch 5, Batch 186, loss_ca: 1.7522, adv_loss: 0.4756\n",
      "Epoch 5, Batch 187, loss_ca: 1.7387, adv_loss: 0.4748\n",
      "Epoch 5, Batch 188, loss_ca: 1.7025, adv_loss: 0.4716\n",
      "Epoch 5, Batch 189, loss_ca: 1.8775, adv_loss: 0.4918\n",
      "Epoch 5, Batch 190, loss_ca: 2.1245, adv_loss: 0.5137\n",
      "Epoch 5, Batch 191, loss_ca: 2.1835, adv_loss: 0.5370\n",
      "Epoch 5, Batch 192, loss_ca: 2.4311, adv_loss: 0.5165\n",
      "Epoch 5, Batch 193, loss_ca: 2.4430, adv_loss: 0.5364\n",
      "Epoch 5, Batch 194, loss_ca: 2.3591, adv_loss: 0.5504\n",
      "Epoch 5, Batch 195, loss_ca: 2.2813, adv_loss: 0.5543\n",
      "Epoch 5, Batch 196, loss_ca: 2.5025, adv_loss: 0.5548\n",
      "Epoch 5, Batch 197, loss_ca: 2.2739, adv_loss: 0.5732\n",
      "Epoch 5, Batch 198, loss_ca: 2.6385, adv_loss: 0.5477\n",
      "Epoch 5, Batch 199, loss_ca: 2.4316, adv_loss: 0.4717\n",
      "Epoch 5, Batch 200, loss_ca: 2.1508, adv_loss: 0.3966\n",
      "Epoch 5, Batch 201, loss_ca: 1.9807, adv_loss: 0.3724\n",
      "Epoch 5, Batch 202, loss_ca: 2.1508, adv_loss: 0.3562\n",
      "Epoch 5, Batch 203, loss_ca: 2.3361, adv_loss: 0.4263\n",
      "Epoch 5, Batch 204, loss_ca: 2.1101, adv_loss: 0.6040\n",
      "Epoch 5, Batch 205, loss_ca: 2.4102, adv_loss: 0.6410\n",
      "Epoch 5, Batch 206, loss_ca: 2.4399, adv_loss: 0.6602\n",
      "Epoch 5, Batch 207, loss_ca: 2.4718, adv_loss: 0.5812\n",
      "Epoch 5, Batch 208, loss_ca: 2.4248, adv_loss: 0.5359\n",
      "Epoch 5, Batch 209, loss_ca: 2.1306, adv_loss: 0.5034\n",
      "Epoch 5, Batch 210, loss_ca: 1.9589, adv_loss: 0.4944\n",
      "Epoch 5, Batch 211, loss_ca: 2.0624, adv_loss: 0.5264\n",
      "Epoch 5, Batch 212, loss_ca: 1.7219, adv_loss: 0.5156\n",
      "Epoch 5, Batch 213, loss_ca: 2.2846, adv_loss: 0.5631\n",
      "Epoch 5, Batch 214, loss_ca: 2.2637, adv_loss: 0.5942\n",
      "Epoch 5, Batch 215, loss_ca: 2.0729, adv_loss: 0.6013\n",
      "Epoch 5, Batch 216, loss_ca: 2.0763, adv_loss: 0.6191\n",
      "Epoch 5, Batch 217, loss_ca: 1.9211, adv_loss: 0.6258\n",
      "Epoch 5, Batch 218, loss_ca: 2.0776, adv_loss: 0.6277\n",
      "Epoch 5, Batch 219, loss_ca: 2.4900, adv_loss: 0.5757\n",
      "Epoch 5, Batch 220, loss_ca: 2.4952, adv_loss: 0.5903\n",
      "Epoch 5, Batch 221, loss_ca: 2.5500, adv_loss: 0.5903\n",
      "Epoch 5, Batch 222, loss_ca: 2.5012, adv_loss: 0.5963\n",
      "Epoch 5, Batch 223, loss_ca: 2.3876, adv_loss: 0.5657\n",
      "Epoch 5, Batch 224, loss_ca: 2.1272, adv_loss: 0.5807\n",
      "Epoch 5, Batch 225, loss_ca: 1.8598, adv_loss: 0.5349\n",
      "Epoch 5, Batch 226, loss_ca: 1.8309, adv_loss: 0.4831\n",
      "Epoch 5, Batch 227, loss_ca: 1.8093, adv_loss: 0.4630\n",
      "Epoch 5, Batch 228, loss_ca: 1.8500, adv_loss: 0.4743\n",
      "Epoch 6, Batch 6, loss_ca: 1.9240, adv_loss: 0.4988\n",
      "Epoch 6, Batch 7, loss_ca: 1.9715, adv_loss: 0.5060\n",
      "Epoch 6, Batch 8, loss_ca: 1.8852, adv_loss: 0.4779\n",
      "Epoch 6, Batch 9, loss_ca: 1.8802, adv_loss: 0.4781\n",
      "Epoch 6, Batch 10, loss_ca: 1.7917, adv_loss: 0.5182\n",
      "Epoch 6, Batch 11, loss_ca: 1.8372, adv_loss: 0.5401\n",
      "Epoch 6, Batch 12, loss_ca: 1.8974, adv_loss: 0.5751\n",
      "Epoch 6, Batch 13, loss_ca: 1.9216, adv_loss: 0.5958\n",
      "Epoch 6, Batch 14, loss_ca: 1.9480, adv_loss: 0.5849\n",
      "Epoch 6, Batch 15, loss_ca: 1.9376, adv_loss: 0.5856\n",
      "Epoch 6, Batch 16, loss_ca: 1.7249, adv_loss: 0.5353\n",
      "Epoch 6, Batch 17, loss_ca: 1.6889, adv_loss: 0.5053\n",
      "Epoch 6, Batch 18, loss_ca: 1.7351, adv_loss: 0.4846\n",
      "Epoch 6, Batch 19, loss_ca: 1.8046, adv_loss: 0.4600\n",
      "Epoch 6, Batch 20, loss_ca: 1.9664, adv_loss: 0.4302\n",
      "Epoch 6, Batch 21, loss_ca: 1.8730, adv_loss: 0.4297\n",
      "Epoch 6, Batch 22, loss_ca: 1.7943, adv_loss: 0.4489\n",
      "Epoch 6, Batch 23, loss_ca: 1.8133, adv_loss: 0.4642\n",
      "Epoch 6, Batch 24, loss_ca: 2.1238, adv_loss: 0.4805\n",
      "Epoch 6, Batch 25, loss_ca: 2.0946, adv_loss: 0.4830\n",
      "Epoch 6, Batch 26, loss_ca: 2.3010, adv_loss: 0.4847\n",
      "Epoch 6, Batch 27, loss_ca: 2.1813, adv_loss: 0.5026\n",
      "Epoch 6, Batch 28, loss_ca: 2.0238, adv_loss: 0.5006\n",
      "Epoch 6, Batch 29, loss_ca: 2.0329, adv_loss: 0.5182\n",
      "Epoch 6, Batch 30, loss_ca: 2.1371, adv_loss: 0.5174\n",
      "Epoch 6, Batch 31, loss_ca: 2.1445, adv_loss: 0.5250\n",
      "Epoch 6, Batch 32, loss_ca: 2.0733, adv_loss: 0.5368\n",
      "Epoch 6, Batch 33, loss_ca: 2.1829, adv_loss: 0.5563\n",
      "Epoch 6, Batch 34, loss_ca: 2.1800, adv_loss: 0.5814\n",
      "Epoch 6, Batch 35, loss_ca: 2.2083, adv_loss: 0.5830\n",
      "Epoch 6, Batch 36, loss_ca: 2.1785, adv_loss: 0.6128\n",
      "Epoch 6, Batch 37, loss_ca: 2.0900, adv_loss: 0.6125\n",
      "Epoch 6, Batch 38, loss_ca: 2.1297, adv_loss: 0.6048\n",
      "Epoch 6, Batch 39, loss_ca: 2.1580, adv_loss: 0.5818\n",
      "Epoch 6, Batch 40, loss_ca: 2.1134, adv_loss: 0.5596\n",
      "Epoch 6, Batch 41, loss_ca: 2.2448, adv_loss: 0.5584\n",
      "Epoch 6, Batch 42, loss_ca: 2.0804, adv_loss: 0.5480\n",
      "Epoch 6, Batch 43, loss_ca: 1.9173, adv_loss: 0.5408\n",
      "Epoch 6, Batch 44, loss_ca: 1.8598, adv_loss: 0.5331\n",
      "Epoch 6, Batch 45, loss_ca: 1.8338, adv_loss: 0.5337\n",
      "Epoch 6, Batch 46, loss_ca: 1.7817, adv_loss: 0.5077\n",
      "Epoch 6, Batch 47, loss_ca: 1.8102, adv_loss: 0.5172\n",
      "Epoch 6, Batch 48, loss_ca: 1.8881, adv_loss: 0.5340\n",
      "Epoch 6, Batch 49, loss_ca: 1.9355, adv_loss: 0.5321\n",
      "Epoch 6, Batch 50, loss_ca: 1.8982, adv_loss: 0.5294\n",
      "Epoch 6, Batch 51, loss_ca: 1.8497, adv_loss: 0.5334\n",
      "Epoch 6, Batch 52, loss_ca: 1.9671, adv_loss: 0.5311\n",
      "Epoch 6, Batch 53, loss_ca: 1.8866, adv_loss: 0.5471\n",
      "Epoch 6, Batch 54, loss_ca: 1.8557, adv_loss: 0.5256\n",
      "Epoch 6, Batch 55, loss_ca: 1.8266, adv_loss: 0.5150\n",
      "Epoch 6, Batch 56, loss_ca: 1.8170, adv_loss: 0.5178\n",
      "Epoch 6, Batch 57, loss_ca: 1.7478, adv_loss: 0.5370\n",
      "Epoch 6, Batch 58, loss_ca: 1.9051, adv_loss: 0.5547\n",
      "Epoch 6, Batch 59, loss_ca: 1.8958, adv_loss: 0.5749\n",
      "Epoch 6, Batch 60, loss_ca: 1.8897, adv_loss: 0.5970\n",
      "Epoch 6, Batch 61, loss_ca: 1.9289, adv_loss: 0.5574\n",
      "Epoch 6, Batch 62, loss_ca: 1.9838, adv_loss: 0.5728\n",
      "Epoch 6, Batch 63, loss_ca: 1.9851, adv_loss: 0.5948\n",
      "Epoch 6, Batch 64, loss_ca: 1.9992, adv_loss: 0.5907\n",
      "Epoch 6, Batch 65, loss_ca: 1.9507, adv_loss: 0.5710\n",
      "Epoch 6, Batch 66, loss_ca: 1.8557, adv_loss: 0.5948\n",
      "Epoch 6, Batch 67, loss_ca: 1.8043, adv_loss: 0.5304\n",
      "Epoch 6, Batch 68, loss_ca: 1.9144, adv_loss: 0.5271\n",
      "Epoch 6, Batch 69, loss_ca: 1.8543, adv_loss: 0.5293\n",
      "Epoch 6, Batch 70, loss_ca: 1.8429, adv_loss: 0.5306\n",
      "Epoch 6, Batch 71, loss_ca: 1.8250, adv_loss: 0.5321\n",
      "Epoch 6, Batch 72, loss_ca: 1.7729, adv_loss: 0.5093\n",
      "Epoch 6, Batch 73, loss_ca: 1.7006, adv_loss: 0.5074\n",
      "Epoch 6, Batch 74, loss_ca: 1.7040, adv_loss: 0.5007\n",
      "Epoch 6, Batch 75, loss_ca: 1.7209, adv_loss: 0.5112\n",
      "Epoch 6, Batch 76, loss_ca: 1.7069, adv_loss: 0.5198\n",
      "Epoch 6, Batch 77, loss_ca: 1.7320, adv_loss: 0.5350\n",
      "Epoch 6, Batch 78, loss_ca: 1.7258, adv_loss: 0.5231\n",
      "Epoch 6, Batch 79, loss_ca: 1.7031, adv_loss: 0.5073\n",
      "Epoch 6, Batch 80, loss_ca: 1.6652, adv_loss: 0.5076\n",
      "Epoch 6, Batch 81, loss_ca: 1.7090, adv_loss: 0.4978\n",
      "Epoch 6, Batch 82, loss_ca: 1.7132, adv_loss: 0.4881\n",
      "Epoch 6, Batch 83, loss_ca: 1.7491, adv_loss: 0.4974\n",
      "Epoch 6, Batch 84, loss_ca: 1.7759, adv_loss: 0.4960\n",
      "Epoch 6, Batch 85, loss_ca: 1.7564, adv_loss: 0.4833\n",
      "Epoch 6, Batch 86, loss_ca: 1.8511, adv_loss: 0.4855\n",
      "Epoch 6, Batch 87, loss_ca: 1.8467, adv_loss: 0.4958\n",
      "Epoch 6, Batch 88, loss_ca: 1.8155, adv_loss: 0.5008\n",
      "Epoch 6, Batch 89, loss_ca: 1.8472, adv_loss: 0.5082\n",
      "Epoch 6, Batch 90, loss_ca: 1.8037, adv_loss: 0.5075\n",
      "Epoch 6, Batch 91, loss_ca: 1.9271, adv_loss: 0.5133\n",
      "Epoch 6, Batch 92, loss_ca: 1.9123, adv_loss: 0.4953\n",
      "Epoch 6, Batch 93, loss_ca: 2.0136, adv_loss: 0.4901\n",
      "Epoch 6, Batch 94, loss_ca: 1.9195, adv_loss: 0.4986\n",
      "Epoch 6, Batch 95, loss_ca: 2.0361, adv_loss: 0.5237\n",
      "Epoch 6, Batch 96, loss_ca: 2.0219, adv_loss: 0.5135\n",
      "Epoch 6, Batch 97, loss_ca: 2.0187, adv_loss: 0.5197\n",
      "Epoch 6, Batch 98, loss_ca: 1.9855, adv_loss: 0.5189\n",
      "Epoch 6, Batch 99, loss_ca: 1.8799, adv_loss: 0.5405\n",
      "Epoch 6, Batch 100, loss_ca: 2.0272, adv_loss: 0.5235\n",
      "Epoch 6, Batch 101, loss_ca: 2.1280, adv_loss: 0.5371\n",
      "Epoch 6, Batch 102, loss_ca: 2.0813, adv_loss: 0.5337\n",
      "Epoch 6, Batch 103, loss_ca: 2.0599, adv_loss: 0.5452\n",
      "Epoch 6, Batch 104, loss_ca: 2.1171, adv_loss: 0.5395\n",
      "Epoch 6, Batch 105, loss_ca: 2.1689, adv_loss: 0.5283\n",
      "Epoch 6, Batch 106, loss_ca: 2.0922, adv_loss: 0.5316\n",
      "Epoch 6, Batch 107, loss_ca: 2.1371, adv_loss: 0.5567\n",
      "Epoch 6, Batch 108, loss_ca: 2.0782, adv_loss: 0.5891\n",
      "Epoch 6, Batch 109, loss_ca: 2.1017, adv_loss: 0.6157\n",
      "Epoch 6, Batch 110, loss_ca: 2.0628, adv_loss: 0.6363\n",
      "Epoch 6, Batch 111, loss_ca: 2.0170, adv_loss: 0.6220\n",
      "Epoch 6, Batch 112, loss_ca: 2.0016, adv_loss: 0.6024\n",
      "Epoch 6, Batch 113, loss_ca: 1.8204, adv_loss: 0.5809\n",
      "Epoch 6, Batch 114, loss_ca: 1.7807, adv_loss: 0.5400\n",
      "Epoch 6, Batch 115, loss_ca: 1.7046, adv_loss: 0.5202\n",
      "Epoch 6, Batch 116, loss_ca: 1.7320, adv_loss: 0.5228\n",
      "Epoch 6, Batch 117, loss_ca: 1.8632, adv_loss: 0.5115\n",
      "Epoch 6, Batch 118, loss_ca: 1.8664, adv_loss: 0.5052\n",
      "Epoch 6, Batch 119, loss_ca: 1.7973, adv_loss: 0.4836\n",
      "Epoch 6, Batch 120, loss_ca: 1.7738, adv_loss: 0.4694\n",
      "Epoch 6, Batch 121, loss_ca: 1.9644, adv_loss: 0.4759\n",
      "Epoch 6, Batch 122, loss_ca: 2.0770, adv_loss: 0.4858\n",
      "Epoch 6, Batch 123, loss_ca: 2.1015, adv_loss: 0.5194\n",
      "Epoch 6, Batch 124, loss_ca: 2.0652, adv_loss: 0.5258\n",
      "Epoch 6, Batch 125, loss_ca: 2.0825, adv_loss: 0.5532\n",
      "Epoch 6, Batch 126, loss_ca: 2.0420, adv_loss: 0.5497\n",
      "Epoch 6, Batch 127, loss_ca: 2.1627, adv_loss: 0.5480\n",
      "Epoch 6, Batch 128, loss_ca: 2.2209, adv_loss: 0.5351\n",
      "Epoch 6, Batch 129, loss_ca: 2.1555, adv_loss: 0.5558\n",
      "Epoch 6, Batch 130, loss_ca: 2.0089, adv_loss: 0.5826\n",
      "Epoch 6, Batch 131, loss_ca: 1.9807, adv_loss: 0.5760\n",
      "Epoch 6, Batch 132, loss_ca: 2.0158, adv_loss: 0.5940\n",
      "Epoch 6, Batch 133, loss_ca: 1.9724, adv_loss: 0.5653\n",
      "Epoch 6, Batch 134, loss_ca: 2.0080, adv_loss: 0.5711\n",
      "Epoch 6, Batch 135, loss_ca: 1.9590, adv_loss: 0.5803\n",
      "Epoch 6, Batch 136, loss_ca: 1.9514, adv_loss: 0.5696\n",
      "Epoch 6, Batch 137, loss_ca: 1.8251, adv_loss: 0.5520\n",
      "Epoch 6, Batch 138, loss_ca: 1.8096, adv_loss: 0.5552\n",
      "Epoch 6, Batch 139, loss_ca: 1.8213, adv_loss: 0.5414\n",
      "Epoch 6, Batch 140, loss_ca: 1.8300, adv_loss: 0.5331\n",
      "Epoch 6, Batch 141, loss_ca: 1.8304, adv_loss: 0.5378\n",
      "Epoch 6, Batch 142, loss_ca: 1.9295, adv_loss: 0.5263\n",
      "Epoch 6, Batch 143, loss_ca: 1.8673, adv_loss: 0.5391\n",
      "Epoch 6, Batch 144, loss_ca: 1.8536, adv_loss: 0.5319\n",
      "Epoch 6, Batch 145, loss_ca: 1.8465, adv_loss: 0.5323\n",
      "Epoch 6, Batch 146, loss_ca: 1.8142, adv_loss: 0.5157\n",
      "Epoch 6, Batch 147, loss_ca: 1.9181, adv_loss: 0.5131\n",
      "Epoch 6, Batch 148, loss_ca: 1.9691, adv_loss: 0.5052\n",
      "Epoch 6, Batch 149, loss_ca: 1.8745, adv_loss: 0.5102\n",
      "Epoch 6, Batch 150, loss_ca: 1.8745, adv_loss: 0.5118\n",
      "Epoch 6, Batch 151, loss_ca: 1.8265, adv_loss: 0.5092\n",
      "Epoch 6, Batch 152, loss_ca: 1.7769, adv_loss: 0.5040\n",
      "Epoch 6, Batch 153, loss_ca: 1.8939, adv_loss: 0.5122\n",
      "Epoch 6, Batch 154, loss_ca: 1.8815, adv_loss: 0.4985\n",
      "Epoch 6, Batch 155, loss_ca: 1.9115, adv_loss: 0.5213\n",
      "Epoch 6, Batch 156, loss_ca: 1.9690, adv_loss: 0.5263\n",
      "Epoch 6, Batch 157, loss_ca: 1.9969, adv_loss: 0.5438\n",
      "Epoch 6, Batch 158, loss_ca: 2.2892, adv_loss: 0.5603\n",
      "Epoch 6, Batch 159, loss_ca: 2.7049, adv_loss: 0.5475\n",
      "Epoch 6, Batch 160, loss_ca: 2.4722, adv_loss: 0.5320\n",
      "Epoch 6, Batch 161, loss_ca: 2.2242, adv_loss: 0.5007\n",
      "Epoch 6, Batch 162, loss_ca: 2.5957, adv_loss: 0.4878\n",
      "Epoch 6, Batch 163, loss_ca: 2.4988, adv_loss: 0.4723\n",
      "Epoch 6, Batch 164, loss_ca: 2.6197, adv_loss: 0.4836\n",
      "Epoch 6, Batch 165, loss_ca: 2.7396, adv_loss: 0.5074\n",
      "Epoch 6, Batch 166, loss_ca: 2.4012, adv_loss: 0.5329\n",
      "Epoch 6, Batch 167, loss_ca: 2.6624, adv_loss: 0.5576\n",
      "Epoch 6, Batch 168, loss_ca: 2.5509, adv_loss: 0.5880\n",
      "Epoch 6, Batch 169, loss_ca: 2.5416, adv_loss: 0.5924\n",
      "Epoch 6, Batch 170, loss_ca: 2.5280, adv_loss: 0.5879\n",
      "Epoch 6, Batch 171, loss_ca: 2.4391, adv_loss: 0.5495\n",
      "Epoch 6, Batch 172, loss_ca: 2.2620, adv_loss: 0.4938\n",
      "Epoch 6, Batch 173, loss_ca: 2.2066, adv_loss: 0.4935\n",
      "Epoch 6, Batch 174, loss_ca: 2.5059, adv_loss: 0.4986\n",
      "Epoch 6, Batch 175, loss_ca: 2.3669, adv_loss: 0.4763\n",
      "Epoch 6, Batch 176, loss_ca: 2.0229, adv_loss: 0.4368\n",
      "Epoch 6, Batch 177, loss_ca: 1.8156, adv_loss: 0.3825\n",
      "Epoch 6, Batch 178, loss_ca: 1.8591, adv_loss: 0.3876\n",
      "Epoch 6, Batch 179, loss_ca: 1.8003, adv_loss: 0.4156\n",
      "Epoch 6, Batch 180, loss_ca: 1.7922, adv_loss: 0.4400\n",
      "Epoch 6, Batch 181, loss_ca: 1.8108, adv_loss: 0.5042\n",
      "Epoch 6, Batch 182, loss_ca: 1.8419, adv_loss: 0.4931\n",
      "Epoch 6, Batch 183, loss_ca: 1.9390, adv_loss: 0.4998\n",
      "Epoch 6, Batch 184, loss_ca: 1.9065, adv_loss: 0.5064\n",
      "Epoch 6, Batch 185, loss_ca: 1.7895, adv_loss: 0.5052\n",
      "Epoch 6, Batch 186, loss_ca: 1.7164, adv_loss: 0.4921\n",
      "Epoch 6, Batch 187, loss_ca: 1.7322, adv_loss: 0.5289\n",
      "Epoch 6, Batch 188, loss_ca: 1.7058, adv_loss: 0.5149\n",
      "Epoch 6, Batch 189, loss_ca: 1.8105, adv_loss: 0.5386\n",
      "Epoch 6, Batch 190, loss_ca: 2.1048, adv_loss: 0.5397\n",
      "Epoch 6, Batch 191, loss_ca: 2.1560, adv_loss: 0.5613\n",
      "Epoch 6, Batch 192, loss_ca: 2.3593, adv_loss: 0.5488\n",
      "Epoch 6, Batch 193, loss_ca: 2.4530, adv_loss: 0.5734\n",
      "Epoch 6, Batch 194, loss_ca: 2.4259, adv_loss: 0.5996\n",
      "Epoch 6, Batch 195, loss_ca: 2.3230, adv_loss: 0.5977\n",
      "Epoch 6, Batch 196, loss_ca: 2.5466, adv_loss: 0.6006\n",
      "Epoch 6, Batch 197, loss_ca: 2.3851, adv_loss: 0.5985\n",
      "Epoch 6, Batch 198, loss_ca: 2.6826, adv_loss: 0.5741\n",
      "Epoch 6, Batch 199, loss_ca: 2.4836, adv_loss: 0.5008\n",
      "Epoch 6, Batch 200, loss_ca: 2.2135, adv_loss: 0.4308\n",
      "Epoch 6, Batch 201, loss_ca: 2.0277, adv_loss: 0.3751\n",
      "Epoch 6, Batch 202, loss_ca: 2.1702, adv_loss: 0.3208\n",
      "Epoch 6, Batch 203, loss_ca: 2.3959, adv_loss: 0.4018\n",
      "Epoch 6, Batch 204, loss_ca: 1.9333, adv_loss: 0.4321\n",
      "Epoch 6, Batch 205, loss_ca: 2.2898, adv_loss: 0.5260\n",
      "Epoch 6, Batch 206, loss_ca: 2.4033, adv_loss: 0.4795\n",
      "Epoch 6, Batch 207, loss_ca: 2.4998, adv_loss: 0.5118\n",
      "Epoch 6, Batch 208, loss_ca: 2.2609, adv_loss: 0.4540\n",
      "Epoch 6, Batch 209, loss_ca: 1.9939, adv_loss: 0.4653\n",
      "Epoch 6, Batch 210, loss_ca: 2.0615, adv_loss: 0.5031\n",
      "Epoch 6, Batch 211, loss_ca: 1.9715, adv_loss: 0.4747\n",
      "Epoch 6, Batch 212, loss_ca: 1.7037, adv_loss: 0.4879\n",
      "Epoch 6, Batch 213, loss_ca: 2.1890, adv_loss: 0.5789\n",
      "Epoch 6, Batch 214, loss_ca: 2.0150, adv_loss: 0.5929\n",
      "Epoch 6, Batch 215, loss_ca: 2.0098, adv_loss: 0.6397\n",
      "Epoch 6, Batch 216, loss_ca: 2.1103, adv_loss: 0.6347\n",
      "Epoch 6, Batch 217, loss_ca: 2.0232, adv_loss: 0.6437\n",
      "Epoch 6, Batch 218, loss_ca: 2.0251, adv_loss: 0.6153\n",
      "Epoch 6, Batch 219, loss_ca: 2.4390, adv_loss: 0.5498\n",
      "Epoch 6, Batch 220, loss_ca: 2.4809, adv_loss: 0.5968\n",
      "Epoch 6, Batch 221, loss_ca: 2.5095, adv_loss: 0.5676\n",
      "Epoch 6, Batch 222, loss_ca: 2.3778, adv_loss: 0.6065\n",
      "Epoch 6, Batch 223, loss_ca: 2.2915, adv_loss: 0.6516\n",
      "Epoch 6, Batch 224, loss_ca: 2.1124, adv_loss: 0.6708\n",
      "Epoch 6, Batch 225, loss_ca: 2.0083, adv_loss: 0.6458\n",
      "Epoch 6, Batch 226, loss_ca: 1.9123, adv_loss: 0.6054\n",
      "Epoch 6, Batch 227, loss_ca: 1.9201, adv_loss: 0.5762\n",
      "Epoch 6, Batch 228, loss_ca: 1.8723, adv_loss: 0.5494\n",
      "Epoch 7, Batch 7, loss_ca: 1.9537, adv_loss: 0.4937\n",
      "Epoch 7, Batch 8, loss_ca: 2.1286, adv_loss: 0.5545\n",
      "Epoch 7, Batch 9, loss_ca: 1.9854, adv_loss: 0.5735\n",
      "Epoch 7, Batch 10, loss_ca: 1.8176, adv_loss: 0.4674\n",
      "Epoch 7, Batch 11, loss_ca: 1.8134, adv_loss: 0.4510\n",
      "Epoch 7, Batch 12, loss_ca: 1.7595, adv_loss: 0.4498\n",
      "Epoch 7, Batch 13, loss_ca: 1.7161, adv_loss: 0.4816\n",
      "Epoch 7, Batch 14, loss_ca: 1.8443, adv_loss: 0.5304\n",
      "Epoch 7, Batch 15, loss_ca: 2.0011, adv_loss: 0.5428\n",
      "Epoch 7, Batch 16, loss_ca: 2.0395, adv_loss: 0.5314\n",
      "Epoch 7, Batch 17, loss_ca: 1.9380, adv_loss: 0.5009\n",
      "Epoch 7, Batch 18, loss_ca: 1.8197, adv_loss: 0.5206\n",
      "Epoch 7, Batch 19, loss_ca: 1.8299, adv_loss: 0.5574\n",
      "Epoch 7, Batch 20, loss_ca: 1.9331, adv_loss: 0.5509\n",
      "Epoch 7, Batch 21, loss_ca: 1.8416, adv_loss: 0.5488\n",
      "Epoch 7, Batch 22, loss_ca: 1.7935, adv_loss: 0.5325\n",
      "Epoch 7, Batch 23, loss_ca: 1.9093, adv_loss: 0.4823\n",
      "Epoch 7, Batch 24, loss_ca: 2.1683, adv_loss: 0.4674\n",
      "Epoch 7, Batch 25, loss_ca: 2.0948, adv_loss: 0.4799\n",
      "Epoch 7, Batch 26, loss_ca: 2.1740, adv_loss: 0.4613\n",
      "Epoch 7, Batch 27, loss_ca: 2.0294, adv_loss: 0.4519\n",
      "Epoch 7, Batch 28, loss_ca: 1.8739, adv_loss: 0.4573\n",
      "Epoch 7, Batch 29, loss_ca: 1.8466, adv_loss: 0.4563\n",
      "Epoch 7, Batch 30, loss_ca: 1.9861, adv_loss: 0.4633\n",
      "Epoch 7, Batch 31, loss_ca: 1.9551, adv_loss: 0.4903\n",
      "Epoch 7, Batch 32, loss_ca: 1.9326, adv_loss: 0.5304\n",
      "Epoch 7, Batch 33, loss_ca: 2.1910, adv_loss: 0.5754\n",
      "Epoch 7, Batch 34, loss_ca: 2.1951, adv_loss: 0.5863\n",
      "Epoch 7, Batch 35, loss_ca: 2.2483, adv_loss: 0.6033\n",
      "Epoch 7, Batch 36, loss_ca: 2.2497, adv_loss: 0.6257\n",
      "Epoch 7, Batch 37, loss_ca: 2.1420, adv_loss: 0.6293\n",
      "Epoch 7, Batch 38, loss_ca: 2.1533, adv_loss: 0.6257\n",
      "Epoch 7, Batch 39, loss_ca: 2.2052, adv_loss: 0.6209\n",
      "Epoch 7, Batch 40, loss_ca: 2.1618, adv_loss: 0.5957\n",
      "Epoch 7, Batch 41, loss_ca: 2.3146, adv_loss: 0.5873\n",
      "Epoch 7, Batch 42, loss_ca: 2.1862, adv_loss: 0.5581\n",
      "Epoch 7, Batch 43, loss_ca: 1.9985, adv_loss: 0.5152\n",
      "Epoch 7, Batch 44, loss_ca: 1.8710, adv_loss: 0.5015\n",
      "Epoch 7, Batch 45, loss_ca: 1.8595, adv_loss: 0.4787\n",
      "Epoch 7, Batch 46, loss_ca: 1.8478, adv_loss: 0.4510\n",
      "Epoch 7, Batch 47, loss_ca: 1.8598, adv_loss: 0.4486\n",
      "Epoch 7, Batch 48, loss_ca: 1.9802, adv_loss: 0.4395\n",
      "Epoch 7, Batch 49, loss_ca: 1.9879, adv_loss: 0.4494\n",
      "Epoch 7, Batch 50, loss_ca: 1.9412, adv_loss: 0.4781\n",
      "Epoch 7, Batch 51, loss_ca: 1.8750, adv_loss: 0.5174\n",
      "Epoch 7, Batch 52, loss_ca: 1.8920, adv_loss: 0.5232\n",
      "Epoch 7, Batch 53, loss_ca: 1.8136, adv_loss: 0.5392\n",
      "Epoch 7, Batch 54, loss_ca: 1.7983, adv_loss: 0.5508\n",
      "Epoch 7, Batch 55, loss_ca: 1.7983, adv_loss: 0.5406\n",
      "Epoch 7, Batch 56, loss_ca: 1.8305, adv_loss: 0.5259\n",
      "Epoch 7, Batch 57, loss_ca: 1.8192, adv_loss: 0.5298\n",
      "Epoch 7, Batch 58, loss_ca: 1.9209, adv_loss: 0.5397\n",
      "Epoch 7, Batch 59, loss_ca: 1.8734, adv_loss: 0.5511\n",
      "Epoch 7, Batch 60, loss_ca: 1.8810, adv_loss: 0.5670\n",
      "Epoch 7, Batch 61, loss_ca: 1.9220, adv_loss: 0.5237\n",
      "Epoch 7, Batch 62, loss_ca: 1.9277, adv_loss: 0.5586\n",
      "Epoch 7, Batch 63, loss_ca: 2.0058, adv_loss: 0.5511\n",
      "Epoch 7, Batch 64, loss_ca: 2.0478, adv_loss: 0.5463\n",
      "Epoch 7, Batch 65, loss_ca: 2.0717, adv_loss: 0.5540\n",
      "Epoch 7, Batch 66, loss_ca: 1.9140, adv_loss: 0.6211\n",
      "Epoch 7, Batch 67, loss_ca: 1.8926, adv_loss: 0.5946\n",
      "Epoch 7, Batch 68, loss_ca: 1.9371, adv_loss: 0.5887\n",
      "Epoch 7, Batch 69, loss_ca: 1.8590, adv_loss: 0.5705\n",
      "Epoch 7, Batch 70, loss_ca: 1.8385, adv_loss: 0.5575\n",
      "Epoch 7, Batch 71, loss_ca: 1.8082, adv_loss: 0.5572\n",
      "Epoch 7, Batch 72, loss_ca: 1.8120, adv_loss: 0.5189\n",
      "Epoch 7, Batch 73, loss_ca: 1.7026, adv_loss: 0.5121\n",
      "Epoch 7, Batch 74, loss_ca: 1.6785, adv_loss: 0.5063\n",
      "Epoch 7, Batch 75, loss_ca: 1.6663, adv_loss: 0.4947\n",
      "Epoch 7, Batch 76, loss_ca: 1.6435, adv_loss: 0.5056\n",
      "Epoch 7, Batch 77, loss_ca: 1.6982, adv_loss: 0.4962\n",
      "Epoch 7, Batch 78, loss_ca: 1.7410, adv_loss: 0.4789\n",
      "Epoch 7, Batch 79, loss_ca: 1.7317, adv_loss: 0.4760\n",
      "Epoch 7, Batch 80, loss_ca: 1.7078, adv_loss: 0.4830\n",
      "Epoch 7, Batch 81, loss_ca: 1.7040, adv_loss: 0.4853\n",
      "Epoch 7, Batch 82, loss_ca: 1.7228, adv_loss: 0.4761\n",
      "Epoch 7, Batch 83, loss_ca: 1.7524, adv_loss: 0.4773\n",
      "Epoch 7, Batch 84, loss_ca: 1.7761, adv_loss: 0.4809\n",
      "Epoch 7, Batch 85, loss_ca: 1.8154, adv_loss: 0.4743\n",
      "Epoch 7, Batch 86, loss_ca: 1.9032, adv_loss: 0.4828\n",
      "Epoch 7, Batch 87, loss_ca: 1.9059, adv_loss: 0.4863\n",
      "Epoch 7, Batch 88, loss_ca: 1.8635, adv_loss: 0.4966\n",
      "Epoch 7, Batch 89, loss_ca: 1.8661, adv_loss: 0.4935\n",
      "Epoch 7, Batch 90, loss_ca: 1.7666, adv_loss: 0.4883\n",
      "Epoch 7, Batch 91, loss_ca: 1.8861, adv_loss: 0.5076\n",
      "Epoch 7, Batch 92, loss_ca: 1.8140, adv_loss: 0.5103\n",
      "Epoch 7, Batch 93, loss_ca: 1.9791, adv_loss: 0.5230\n",
      "Epoch 7, Batch 94, loss_ca: 1.8810, adv_loss: 0.5337\n",
      "Epoch 7, Batch 95, loss_ca: 2.0610, adv_loss: 0.5444\n",
      "Epoch 7, Batch 96, loss_ca: 2.0411, adv_loss: 0.5407\n",
      "Epoch 7, Batch 97, loss_ca: 1.9744, adv_loss: 0.5360\n",
      "Epoch 7, Batch 98, loss_ca: 1.9233, adv_loss: 0.5291\n",
      "Epoch 7, Batch 99, loss_ca: 1.8288, adv_loss: 0.5410\n",
      "Epoch 7, Batch 100, loss_ca: 1.9264, adv_loss: 0.5358\n",
      "Epoch 7, Batch 101, loss_ca: 2.0706, adv_loss: 0.5588\n",
      "Epoch 7, Batch 102, loss_ca: 2.0592, adv_loss: 0.5746\n",
      "Epoch 7, Batch 103, loss_ca: 2.0454, adv_loss: 0.5834\n",
      "Epoch 7, Batch 104, loss_ca: 2.0825, adv_loss: 0.5884\n",
      "Epoch 7, Batch 105, loss_ca: 2.1912, adv_loss: 0.5470\n",
      "Epoch 7, Batch 106, loss_ca: 2.1839, adv_loss: 0.5454\n",
      "Epoch 7, Batch 107, loss_ca: 2.2102, adv_loss: 0.5424\n",
      "Epoch 7, Batch 108, loss_ca: 2.1983, adv_loss: 0.5499\n",
      "Epoch 7, Batch 109, loss_ca: 2.1262, adv_loss: 0.5541\n",
      "Epoch 7, Batch 110, loss_ca: 2.0794, adv_loss: 0.5409\n",
      "Epoch 7, Batch 111, loss_ca: 2.0285, adv_loss: 0.5285\n",
      "Epoch 7, Batch 112, loss_ca: 2.0562, adv_loss: 0.5238\n",
      "Epoch 7, Batch 113, loss_ca: 1.9216, adv_loss: 0.5084\n",
      "Epoch 7, Batch 114, loss_ca: 1.8423, adv_loss: 0.4975\n",
      "Epoch 7, Batch 115, loss_ca: 1.7464, adv_loss: 0.5031\n",
      "Epoch 7, Batch 116, loss_ca: 1.8247, adv_loss: 0.5128\n",
      "Epoch 7, Batch 117, loss_ca: 1.8178, adv_loss: 0.5095\n",
      "Epoch 7, Batch 118, loss_ca: 1.7811, adv_loss: 0.5038\n",
      "Epoch 7, Batch 119, loss_ca: 1.7799, adv_loss: 0.4984\n",
      "Epoch 7, Batch 120, loss_ca: 1.7937, adv_loss: 0.4859\n",
      "Epoch 7, Batch 121, loss_ca: 1.9873, adv_loss: 0.4839\n",
      "Epoch 7, Batch 122, loss_ca: 2.0532, adv_loss: 0.4804\n",
      "Epoch 7, Batch 123, loss_ca: 2.0070, adv_loss: 0.4963\n",
      "Epoch 7, Batch 124, loss_ca: 2.0201, adv_loss: 0.5295\n",
      "Epoch 7, Batch 125, loss_ca: 2.0141, adv_loss: 0.5451\n",
      "Epoch 7, Batch 126, loss_ca: 1.9820, adv_loss: 0.5293\n",
      "Epoch 7, Batch 127, loss_ca: 2.0389, adv_loss: 0.5609\n",
      "Epoch 7, Batch 128, loss_ca: 2.1350, adv_loss: 0.5702\n",
      "Epoch 7, Batch 129, loss_ca: 2.1271, adv_loss: 0.5789\n",
      "Epoch 7, Batch 130, loss_ca: 1.9969, adv_loss: 0.5717\n",
      "Epoch 7, Batch 131, loss_ca: 1.9564, adv_loss: 0.5747\n",
      "Epoch 7, Batch 132, loss_ca: 2.0008, adv_loss: 0.5783\n",
      "Epoch 7, Batch 133, loss_ca: 2.0061, adv_loss: 0.5812\n",
      "Epoch 7, Batch 134, loss_ca: 1.9846, adv_loss: 0.5919\n",
      "Epoch 7, Batch 135, loss_ca: 1.9438, adv_loss: 0.6006\n",
      "Epoch 7, Batch 136, loss_ca: 1.9425, adv_loss: 0.5996\n",
      "Epoch 7, Batch 137, loss_ca: 1.8508, adv_loss: 0.5777\n",
      "Epoch 7, Batch 138, loss_ca: 1.8463, adv_loss: 0.5532\n",
      "Epoch 7, Batch 139, loss_ca: 1.8210, adv_loss: 0.5816\n",
      "Epoch 7, Batch 140, loss_ca: 1.8192, adv_loss: 0.5964\n",
      "Epoch 7, Batch 141, loss_ca: 1.7902, adv_loss: 0.6017\n",
      "Epoch 7, Batch 142, loss_ca: 1.9268, adv_loss: 0.6113\n",
      "Epoch 7, Batch 143, loss_ca: 1.8468, adv_loss: 0.5947\n",
      "Epoch 7, Batch 144, loss_ca: 1.8845, adv_loss: 0.5841\n",
      "Epoch 7, Batch 145, loss_ca: 1.8730, adv_loss: 0.5680\n",
      "Epoch 7, Batch 146, loss_ca: 1.8183, adv_loss: 0.5497\n",
      "Epoch 7, Batch 147, loss_ca: 1.9084, adv_loss: 0.5453\n",
      "Epoch 7, Batch 148, loss_ca: 1.9255, adv_loss: 0.5445\n",
      "Epoch 7, Batch 149, loss_ca: 1.8631, adv_loss: 0.5374\n",
      "Epoch 7, Batch 150, loss_ca: 1.8699, adv_loss: 0.5141\n",
      "Epoch 7, Batch 151, loss_ca: 1.8209, adv_loss: 0.4866\n",
      "Epoch 7, Batch 152, loss_ca: 1.7622, adv_loss: 0.4551\n",
      "Epoch 7, Batch 153, loss_ca: 1.8756, adv_loss: 0.4404\n",
      "Epoch 7, Batch 154, loss_ca: 1.8651, adv_loss: 0.4258\n",
      "Epoch 7, Batch 155, loss_ca: 1.8632, adv_loss: 0.4284\n",
      "Epoch 7, Batch 156, loss_ca: 1.9328, adv_loss: 0.4326\n",
      "Epoch 7, Batch 157, loss_ca: 1.9235, adv_loss: 0.4478\n",
      "Epoch 7, Batch 158, loss_ca: 2.1936, adv_loss: 0.4523\n",
      "Epoch 7, Batch 159, loss_ca: 2.6017, adv_loss: 0.4489\n",
      "Epoch 7, Batch 160, loss_ca: 2.3900, adv_loss: 0.4570\n",
      "Epoch 7, Batch 161, loss_ca: 2.2391, adv_loss: 0.4712\n",
      "Epoch 7, Batch 162, loss_ca: 2.5724, adv_loss: 0.4540\n",
      "Epoch 7, Batch 163, loss_ca: 2.5233, adv_loss: 0.4693\n",
      "Epoch 7, Batch 164, loss_ca: 2.5898, adv_loss: 0.5023\n",
      "Epoch 7, Batch 165, loss_ca: 2.6887, adv_loss: 0.5418\n",
      "Epoch 7, Batch 166, loss_ca: 2.3848, adv_loss: 0.5880\n",
      "Epoch 7, Batch 167, loss_ca: 2.6529, adv_loss: 0.6274\n",
      "Epoch 7, Batch 168, loss_ca: 2.7053, adv_loss: 0.6640\n",
      "Epoch 7, Batch 169, loss_ca: 2.6067, adv_loss: 0.6702\n",
      "Epoch 7, Batch 170, loss_ca: 2.6090, adv_loss: 0.6376\n",
      "Epoch 7, Batch 171, loss_ca: 2.4236, adv_loss: 0.6071\n",
      "Epoch 7, Batch 172, loss_ca: 2.3015, adv_loss: 0.5475\n",
      "Epoch 7, Batch 173, loss_ca: 2.2890, adv_loss: 0.5188\n",
      "Epoch 7, Batch 174, loss_ca: 2.4536, adv_loss: 0.5155\n",
      "Epoch 7, Batch 175, loss_ca: 2.2953, adv_loss: 0.4848\n",
      "Epoch 7, Batch 176, loss_ca: 2.1153, adv_loss: 0.4755\n",
      "Epoch 7, Batch 177, loss_ca: 1.8998, adv_loss: 0.4835\n",
      "Epoch 7, Batch 178, loss_ca: 2.0182, adv_loss: 0.4804\n",
      "Epoch 7, Batch 179, loss_ca: 1.8554, adv_loss: 0.4792\n",
      "Epoch 7, Batch 180, loss_ca: 1.7857, adv_loss: 0.4674\n",
      "Epoch 7, Batch 181, loss_ca: 1.8181, adv_loss: 0.4939\n",
      "Epoch 7, Batch 182, loss_ca: 1.8052, adv_loss: 0.4838\n",
      "Epoch 7, Batch 183, loss_ca: 1.9310, adv_loss: 0.5455\n",
      "Epoch 7, Batch 184, loss_ca: 1.9200, adv_loss: 0.5238\n",
      "Epoch 7, Batch 185, loss_ca: 1.8410, adv_loss: 0.5255\n",
      "Epoch 7, Batch 186, loss_ca: 1.7447, adv_loss: 0.5276\n",
      "Epoch 7, Batch 187, loss_ca: 1.7121, adv_loss: 0.5191\n",
      "Epoch 7, Batch 188, loss_ca: 1.6879, adv_loss: 0.4923\n",
      "Epoch 7, Batch 189, loss_ca: 1.7932, adv_loss: 0.5019\n",
      "Epoch 7, Batch 190, loss_ca: 2.0575, adv_loss: 0.5184\n",
      "Epoch 7, Batch 191, loss_ca: 2.0882, adv_loss: 0.5247\n",
      "Epoch 7, Batch 192, loss_ca: 2.3351, adv_loss: 0.5429\n",
      "Epoch 7, Batch 193, loss_ca: 2.3652, adv_loss: 0.5651\n",
      "Epoch 7, Batch 194, loss_ca: 2.2849, adv_loss: 0.5744\n",
      "Epoch 7, Batch 195, loss_ca: 2.3248, adv_loss: 0.6234\n",
      "Epoch 7, Batch 196, loss_ca: 2.4582, adv_loss: 0.5968\n",
      "Epoch 7, Batch 197, loss_ca: 2.2815, adv_loss: 0.6029\n",
      "Epoch 7, Batch 198, loss_ca: 2.5257, adv_loss: 0.5685\n",
      "Epoch 7, Batch 199, loss_ca: 2.3120, adv_loss: 0.5035\n",
      "Epoch 7, Batch 200, loss_ca: 2.1937, adv_loss: 0.4733\n",
      "Epoch 7, Batch 201, loss_ca: 2.0384, adv_loss: 0.4303\n",
      "Epoch 7, Batch 202, loss_ca: 2.1396, adv_loss: 0.4076\n",
      "Epoch 7, Batch 203, loss_ca: 2.4001, adv_loss: 0.4263\n",
      "Epoch 7, Batch 204, loss_ca: 2.1011, adv_loss: 0.4800\n",
      "Epoch 7, Batch 205, loss_ca: 2.4428, adv_loss: 0.5317\n",
      "Epoch 7, Batch 206, loss_ca: 2.2715, adv_loss: 0.5410\n",
      "Epoch 7, Batch 207, loss_ca: 2.5004, adv_loss: 0.5517\n",
      "Epoch 7, Batch 208, loss_ca: 2.2037, adv_loss: 0.5155\n",
      "Epoch 7, Batch 209, loss_ca: 1.9681, adv_loss: 0.5330\n",
      "Epoch 7, Batch 210, loss_ca: 2.0228, adv_loss: 0.5227\n",
      "Epoch 7, Batch 211, loss_ca: 2.1379, adv_loss: 0.5428\n",
      "Epoch 7, Batch 212, loss_ca: 1.8611, adv_loss: 0.4887\n",
      "Epoch 7, Batch 213, loss_ca: 2.2126, adv_loss: 0.5660\n",
      "Epoch 7, Batch 214, loss_ca: 2.0694, adv_loss: 0.5902\n",
      "Epoch 7, Batch 215, loss_ca: 1.8855, adv_loss: 0.6182\n",
      "Epoch 7, Batch 216, loss_ca: 2.0145, adv_loss: 0.6315\n",
      "Epoch 7, Batch 217, loss_ca: 1.9696, adv_loss: 0.6469\n",
      "Epoch 7, Batch 218, loss_ca: 1.8917, adv_loss: 0.6636\n",
      "Epoch 7, Batch 219, loss_ca: 2.3722, adv_loss: 0.6299\n",
      "Epoch 7, Batch 220, loss_ca: 2.2901, adv_loss: 0.6238\n",
      "Epoch 7, Batch 221, loss_ca: 2.5298, adv_loss: 0.6046\n",
      "Epoch 7, Batch 222, loss_ca: 2.4665, adv_loss: 0.6046\n",
      "Epoch 7, Batch 223, loss_ca: 2.4450, adv_loss: 0.5989\n",
      "Epoch 7, Batch 224, loss_ca: 2.1619, adv_loss: 0.6334\n",
      "Epoch 7, Batch 225, loss_ca: 2.0908, adv_loss: 0.6288\n",
      "Epoch 7, Batch 226, loss_ca: 1.9017, adv_loss: 0.6014\n",
      "Epoch 7, Batch 227, loss_ca: 1.8378, adv_loss: 0.5605\n",
      "Epoch 7, Batch 228, loss_ca: 1.7550, adv_loss: 0.5187\n",
      "Epoch 8, Batch 8, loss_ca: 1.8022, adv_loss: 0.5438\n",
      "Epoch 8, Batch 9, loss_ca: 1.8514, adv_loss: 0.4793\n",
      "Epoch 8, Batch 10, loss_ca: 1.8631, adv_loss: 0.4701\n",
      "Epoch 8, Batch 11, loss_ca: 1.8976, adv_loss: 0.4409\n",
      "Epoch 8, Batch 12, loss_ca: 1.9122, adv_loss: 0.4576\n",
      "Epoch 8, Batch 13, loss_ca: 1.7381, adv_loss: 0.4732\n",
      "Epoch 8, Batch 14, loss_ca: 1.8359, adv_loss: 0.5167\n",
      "Epoch 8, Batch 15, loss_ca: 1.8498, adv_loss: 0.5592\n",
      "Epoch 8, Batch 16, loss_ca: 1.7856, adv_loss: 0.5450\n",
      "Epoch 8, Batch 17, loss_ca: 1.7373, adv_loss: 0.5417\n",
      "Epoch 8, Batch 18, loss_ca: 1.7620, adv_loss: 0.5079\n",
      "Epoch 8, Batch 19, loss_ca: 1.8538, adv_loss: 0.5304\n",
      "Epoch 8, Batch 20, loss_ca: 1.9637, adv_loss: 0.4887\n",
      "Epoch 8, Batch 21, loss_ca: 1.8274, adv_loss: 0.4901\n",
      "Epoch 8, Batch 22, loss_ca: 1.7645, adv_loss: 0.4930\n",
      "Epoch 8, Batch 23, loss_ca: 1.8084, adv_loss: 0.5097\n",
      "Epoch 8, Batch 24, loss_ca: 2.1358, adv_loss: 0.4985\n",
      "Epoch 8, Batch 25, loss_ca: 2.1326, adv_loss: 0.4993\n",
      "Epoch 8, Batch 26, loss_ca: 2.3281, adv_loss: 0.5125\n",
      "Epoch 8, Batch 27, loss_ca: 2.1764, adv_loss: 0.5206\n",
      "Epoch 8, Batch 28, loss_ca: 1.9601, adv_loss: 0.5374\n",
      "Epoch 8, Batch 29, loss_ca: 1.9086, adv_loss: 0.5508\n",
      "Epoch 8, Batch 30, loss_ca: 2.0056, adv_loss: 0.5557\n",
      "Epoch 8, Batch 31, loss_ca: 1.9835, adv_loss: 0.5638\n",
      "Epoch 8, Batch 32, loss_ca: 1.9312, adv_loss: 0.5962\n",
      "Epoch 8, Batch 33, loss_ca: 2.1650, adv_loss: 0.6599\n",
      "Epoch 8, Batch 34, loss_ca: 2.1079, adv_loss: 0.6249\n",
      "Epoch 8, Batch 35, loss_ca: 2.1664, adv_loss: 0.6127\n",
      "Epoch 8, Batch 36, loss_ca: 2.1687, adv_loss: 0.6323\n",
      "Epoch 8, Batch 37, loss_ca: 2.0864, adv_loss: 0.6331\n",
      "Epoch 8, Batch 38, loss_ca: 2.1342, adv_loss: 0.6142\n",
      "Epoch 8, Batch 39, loss_ca: 2.1595, adv_loss: 0.6230\n",
      "Epoch 8, Batch 40, loss_ca: 2.0715, adv_loss: 0.6058\n",
      "Epoch 8, Batch 41, loss_ca: 2.2382, adv_loss: 0.5922\n",
      "Epoch 8, Batch 42, loss_ca: 2.1564, adv_loss: 0.5760\n",
      "Epoch 8, Batch 43, loss_ca: 2.0077, adv_loss: 0.5462\n",
      "Epoch 8, Batch 44, loss_ca: 1.9018, adv_loss: 0.4961\n",
      "Epoch 8, Batch 45, loss_ca: 1.8831, adv_loss: 0.4943\n",
      "Epoch 8, Batch 46, loss_ca: 1.8088, adv_loss: 0.4668\n",
      "Epoch 8, Batch 47, loss_ca: 1.8255, adv_loss: 0.4809\n",
      "Epoch 8, Batch 48, loss_ca: 1.9406, adv_loss: 0.5093\n",
      "Epoch 8, Batch 49, loss_ca: 1.8990, adv_loss: 0.5181\n",
      "Epoch 8, Batch 50, loss_ca: 1.8442, adv_loss: 0.5288\n",
      "Epoch 8, Batch 51, loss_ca: 1.8159, adv_loss: 0.5185\n",
      "Epoch 8, Batch 52, loss_ca: 1.8665, adv_loss: 0.4885\n",
      "Epoch 8, Batch 53, loss_ca: 1.7958, adv_loss: 0.5281\n",
      "Epoch 8, Batch 54, loss_ca: 1.7973, adv_loss: 0.5602\n",
      "Epoch 8, Batch 55, loss_ca: 1.7887, adv_loss: 0.5555\n",
      "Epoch 8, Batch 56, loss_ca: 1.8055, adv_loss: 0.5256\n",
      "Epoch 8, Batch 57, loss_ca: 1.7733, adv_loss: 0.5064\n",
      "Epoch 8, Batch 58, loss_ca: 1.9135, adv_loss: 0.5352\n",
      "Epoch 8, Batch 59, loss_ca: 1.9017, adv_loss: 0.5474\n",
      "Epoch 8, Batch 60, loss_ca: 1.9025, adv_loss: 0.5595\n",
      "Epoch 8, Batch 61, loss_ca: 1.9010, adv_loss: 0.5339\n",
      "Epoch 8, Batch 62, loss_ca: 1.9759, adv_loss: 0.5267\n",
      "Epoch 8, Batch 63, loss_ca: 2.0066, adv_loss: 0.5496\n",
      "Epoch 8, Batch 64, loss_ca: 1.9699, adv_loss: 0.5417\n",
      "Epoch 8, Batch 65, loss_ca: 1.8697, adv_loss: 0.5678\n",
      "Epoch 8, Batch 66, loss_ca: 1.8260, adv_loss: 0.5782\n",
      "Epoch 8, Batch 67, loss_ca: 1.7969, adv_loss: 0.5597\n",
      "Epoch 8, Batch 68, loss_ca: 1.8515, adv_loss: 0.5610\n",
      "Epoch 8, Batch 69, loss_ca: 1.8616, adv_loss: 0.5594\n",
      "Epoch 8, Batch 70, loss_ca: 1.8803, adv_loss: 0.5748\n",
      "Epoch 8, Batch 71, loss_ca: 1.8870, adv_loss: 0.5519\n",
      "Epoch 8, Batch 72, loss_ca: 1.8409, adv_loss: 0.4937\n",
      "Epoch 8, Batch 73, loss_ca: 1.7172, adv_loss: 0.5121\n",
      "Epoch 8, Batch 74, loss_ca: 1.6969, adv_loss: 0.5282\n",
      "Epoch 8, Batch 75, loss_ca: 1.6697, adv_loss: 0.5330\n",
      "Epoch 8, Batch 76, loss_ca: 1.6604, adv_loss: 0.5341\n",
      "Epoch 8, Batch 77, loss_ca: 1.6592, adv_loss: 0.5326\n",
      "Epoch 8, Batch 78, loss_ca: 1.7161, adv_loss: 0.5139\n",
      "Epoch 8, Batch 79, loss_ca: 1.7413, adv_loss: 0.5052\n",
      "Epoch 8, Batch 80, loss_ca: 1.7528, adv_loss: 0.4957\n",
      "Epoch 8, Batch 81, loss_ca: 1.7025, adv_loss: 0.5000\n",
      "Epoch 8, Batch 82, loss_ca: 1.6806, adv_loss: 0.4917\n",
      "Epoch 8, Batch 83, loss_ca: 1.7143, adv_loss: 0.4809\n",
      "Epoch 8, Batch 84, loss_ca: 1.7369, adv_loss: 0.4785\n",
      "Epoch 8, Batch 85, loss_ca: 1.7556, adv_loss: 0.4827\n",
      "Epoch 8, Batch 86, loss_ca: 1.8265, adv_loss: 0.4909\n",
      "Epoch 8, Batch 87, loss_ca: 1.8446, adv_loss: 0.4961\n",
      "Epoch 8, Batch 88, loss_ca: 1.8194, adv_loss: 0.4953\n",
      "Epoch 8, Batch 89, loss_ca: 1.8392, adv_loss: 0.4851\n",
      "Epoch 8, Batch 90, loss_ca: 1.7656, adv_loss: 0.4872\n",
      "Epoch 8, Batch 91, loss_ca: 1.8667, adv_loss: 0.4918\n",
      "Epoch 8, Batch 92, loss_ca: 1.8061, adv_loss: 0.4931\n",
      "Epoch 8, Batch 93, loss_ca: 1.9709, adv_loss: 0.4918\n",
      "Epoch 8, Batch 94, loss_ca: 1.9034, adv_loss: 0.5025\n",
      "Epoch 8, Batch 95, loss_ca: 2.0843, adv_loss: 0.4835\n",
      "Epoch 8, Batch 96, loss_ca: 2.0405, adv_loss: 0.5040\n",
      "Epoch 8, Batch 97, loss_ca: 1.9776, adv_loss: 0.5081\n",
      "Epoch 8, Batch 98, loss_ca: 1.9565, adv_loss: 0.5083\n",
      "Epoch 8, Batch 99, loss_ca: 1.8417, adv_loss: 0.5243\n",
      "Epoch 8, Batch 100, loss_ca: 1.9693, adv_loss: 0.5240\n",
      "Epoch 8, Batch 101, loss_ca: 2.0732, adv_loss: 0.5464\n",
      "Epoch 8, Batch 102, loss_ca: 2.0433, adv_loss: 0.5753\n",
      "Epoch 8, Batch 103, loss_ca: 2.0249, adv_loss: 0.5923\n",
      "Epoch 8, Batch 104, loss_ca: 1.9772, adv_loss: 0.6039\n",
      "Epoch 8, Batch 105, loss_ca: 2.1434, adv_loss: 0.5737\n",
      "Epoch 8, Batch 106, loss_ca: 2.1689, adv_loss: 0.5769\n",
      "Epoch 8, Batch 107, loss_ca: 2.2061, adv_loss: 0.5770\n",
      "Epoch 8, Batch 108, loss_ca: 2.1855, adv_loss: 0.5913\n",
      "Epoch 8, Batch 109, loss_ca: 2.1365, adv_loss: 0.5831\n",
      "Epoch 8, Batch 110, loss_ca: 2.0547, adv_loss: 0.5767\n",
      "Epoch 8, Batch 111, loss_ca: 1.9931, adv_loss: 0.5703\n",
      "Epoch 8, Batch 112, loss_ca: 2.0370, adv_loss: 0.5709\n",
      "Epoch 8, Batch 113, loss_ca: 1.9354, adv_loss: 0.5525\n",
      "Epoch 8, Batch 114, loss_ca: 1.8936, adv_loss: 0.5495\n",
      "Epoch 8, Batch 115, loss_ca: 1.8431, adv_loss: 0.5326\n",
      "Epoch 8, Batch 116, loss_ca: 1.8594, adv_loss: 0.5202\n",
      "Epoch 8, Batch 117, loss_ca: 1.8658, adv_loss: 0.5268\n",
      "Epoch 8, Batch 118, loss_ca: 1.8340, adv_loss: 0.5252\n",
      "Epoch 8, Batch 119, loss_ca: 1.8175, adv_loss: 0.5231\n",
      "Epoch 8, Batch 120, loss_ca: 1.7839, adv_loss: 0.5013\n",
      "Epoch 8, Batch 121, loss_ca: 2.0136, adv_loss: 0.4898\n",
      "Epoch 8, Batch 122, loss_ca: 2.0611, adv_loss: 0.4824\n",
      "Epoch 8, Batch 123, loss_ca: 2.0006, adv_loss: 0.4868\n",
      "Epoch 8, Batch 124, loss_ca: 1.9369, adv_loss: 0.5068\n",
      "Epoch 8, Batch 125, loss_ca: 1.9369, adv_loss: 0.5301\n",
      "Epoch 8, Batch 126, loss_ca: 1.9136, adv_loss: 0.5418\n",
      "Epoch 8, Batch 127, loss_ca: 1.9512, adv_loss: 0.5919\n",
      "Epoch 8, Batch 128, loss_ca: 2.0625, adv_loss: 0.5946\n",
      "Epoch 8, Batch 129, loss_ca: 2.1539, adv_loss: 0.5901\n",
      "Epoch 8, Batch 130, loss_ca: 2.0254, adv_loss: 0.5767\n",
      "Epoch 8, Batch 131, loss_ca: 1.9328, adv_loss: 0.5595\n",
      "Epoch 8, Batch 132, loss_ca: 1.9746, adv_loss: 0.5493\n",
      "Epoch 8, Batch 133, loss_ca: 1.9391, adv_loss: 0.5410\n",
      "Epoch 8, Batch 134, loss_ca: 1.9429, adv_loss: 0.5268\n",
      "Epoch 8, Batch 135, loss_ca: 1.9084, adv_loss: 0.5293\n",
      "Epoch 8, Batch 136, loss_ca: 1.9557, adv_loss: 0.5331\n",
      "Epoch 8, Batch 137, loss_ca: 1.8729, adv_loss: 0.5303\n",
      "Epoch 8, Batch 138, loss_ca: 1.8605, adv_loss: 0.5366\n",
      "Epoch 8, Batch 139, loss_ca: 1.8707, adv_loss: 0.5196\n",
      "Epoch 8, Batch 140, loss_ca: 1.8614, adv_loss: 0.5044\n",
      "Epoch 8, Batch 141, loss_ca: 1.8486, adv_loss: 0.5151\n",
      "Epoch 8, Batch 142, loss_ca: 1.9358, adv_loss: 0.4953\n",
      "Epoch 8, Batch 143, loss_ca: 1.8058, adv_loss: 0.5355\n",
      "Epoch 8, Batch 144, loss_ca: 1.8284, adv_loss: 0.5490\n",
      "Epoch 8, Batch 145, loss_ca: 1.8122, adv_loss: 0.5656\n",
      "Epoch 8, Batch 146, loss_ca: 1.7759, adv_loss: 0.5582\n",
      "Epoch 8, Batch 147, loss_ca: 1.8941, adv_loss: 0.5694\n",
      "Epoch 8, Batch 148, loss_ca: 1.9591, adv_loss: 0.5691\n",
      "Epoch 8, Batch 149, loss_ca: 1.8720, adv_loss: 0.5747\n",
      "Epoch 8, Batch 150, loss_ca: 1.8691, adv_loss: 0.5681\n",
      "Epoch 8, Batch 151, loss_ca: 1.8263, adv_loss: 0.5500\n",
      "Epoch 8, Batch 152, loss_ca: 1.7811, adv_loss: 0.5209\n",
      "Epoch 8, Batch 153, loss_ca: 1.8966, adv_loss: 0.5157\n",
      "Epoch 8, Batch 154, loss_ca: 1.8715, adv_loss: 0.4814\n",
      "Epoch 8, Batch 155, loss_ca: 1.8787, adv_loss: 0.4909\n",
      "Epoch 8, Batch 156, loss_ca: 1.9194, adv_loss: 0.4902\n",
      "Epoch 8, Batch 157, loss_ca: 1.9514, adv_loss: 0.5323\n",
      "Epoch 8, Batch 158, loss_ca: 2.2163, adv_loss: 0.4861\n",
      "Epoch 8, Batch 159, loss_ca: 2.6692, adv_loss: 0.4856\n",
      "Epoch 8, Batch 160, loss_ca: 2.4289, adv_loss: 0.4751\n",
      "Epoch 8, Batch 161, loss_ca: 2.0911, adv_loss: 0.4769\n",
      "Epoch 8, Batch 162, loss_ca: 2.4615, adv_loss: 0.4544\n",
      "Epoch 8, Batch 163, loss_ca: 2.3635, adv_loss: 0.4998\n",
      "Epoch 8, Batch 164, loss_ca: 2.5459, adv_loss: 0.4760\n",
      "Epoch 8, Batch 165, loss_ca: 2.7162, adv_loss: 0.5102\n",
      "Epoch 8, Batch 166, loss_ca: 2.3787, adv_loss: 0.5341\n",
      "Epoch 8, Batch 167, loss_ca: 2.6435, adv_loss: 0.5634\n",
      "Epoch 8, Batch 168, loss_ca: 2.6509, adv_loss: 0.6066\n",
      "Epoch 8, Batch 169, loss_ca: 2.5237, adv_loss: 0.6256\n",
      "Epoch 8, Batch 170, loss_ca: 2.4199, adv_loss: 0.6098\n",
      "Epoch 8, Batch 171, loss_ca: 2.3652, adv_loss: 0.6197\n",
      "Epoch 8, Batch 172, loss_ca: 2.2321, adv_loss: 0.5959\n",
      "Epoch 8, Batch 173, loss_ca: 2.1465, adv_loss: 0.5767\n",
      "Epoch 8, Batch 174, loss_ca: 2.4187, adv_loss: 0.5773\n",
      "Epoch 8, Batch 175, loss_ca: 2.3521, adv_loss: 0.5651\n",
      "Epoch 8, Batch 176, loss_ca: 2.1104, adv_loss: 0.5249\n",
      "Epoch 8, Batch 177, loss_ca: 1.9003, adv_loss: 0.5236\n",
      "Epoch 8, Batch 178, loss_ca: 1.9297, adv_loss: 0.5545\n",
      "Epoch 8, Batch 179, loss_ca: 1.8964, adv_loss: 0.6079\n",
      "Epoch 8, Batch 180, loss_ca: 1.8503, adv_loss: 0.5957\n",
      "Epoch 8, Batch 181, loss_ca: 1.9204, adv_loss: 0.6070\n",
      "Epoch 8, Batch 182, loss_ca: 1.8632, adv_loss: 0.5698\n",
      "Epoch 8, Batch 183, loss_ca: 1.9616, adv_loss: 0.5717\n",
      "Epoch 8, Batch 184, loss_ca: 1.8946, adv_loss: 0.5155\n",
      "Epoch 8, Batch 185, loss_ca: 1.8243, adv_loss: 0.5123\n",
      "Epoch 8, Batch 186, loss_ca: 1.7024, adv_loss: 0.5146\n",
      "Epoch 8, Batch 187, loss_ca: 1.7045, adv_loss: 0.5094\n",
      "Epoch 8, Batch 188, loss_ca: 1.6883, adv_loss: 0.4977\n",
      "Epoch 8, Batch 189, loss_ca: 1.8410, adv_loss: 0.5171\n",
      "Epoch 8, Batch 190, loss_ca: 2.1112, adv_loss: 0.5354\n",
      "Epoch 8, Batch 191, loss_ca: 2.1022, adv_loss: 0.5002\n",
      "Epoch 8, Batch 192, loss_ca: 2.3240, adv_loss: 0.4813\n",
      "Epoch 8, Batch 193, loss_ca: 2.3216, adv_loss: 0.4908\n",
      "Epoch 8, Batch 194, loss_ca: 2.3125, adv_loss: 0.5066\n",
      "Epoch 8, Batch 195, loss_ca: 2.2205, adv_loss: 0.5085\n",
      "Epoch 8, Batch 196, loss_ca: 2.4112, adv_loss: 0.5032\n",
      "Epoch 8, Batch 197, loss_ca: 2.2982, adv_loss: 0.5200\n",
      "Epoch 8, Batch 198, loss_ca: 2.7176, adv_loss: 0.5247\n",
      "Epoch 8, Batch 199, loss_ca: 2.5360, adv_loss: 0.4619\n",
      "Epoch 8, Batch 200, loss_ca: 2.1881, adv_loss: 0.4107\n",
      "Epoch 8, Batch 201, loss_ca: 1.9760, adv_loss: 0.3929\n",
      "Epoch 8, Batch 202, loss_ca: 2.1322, adv_loss: 0.3606\n",
      "Epoch 8, Batch 203, loss_ca: 2.3654, adv_loss: 0.3895\n",
      "Epoch 8, Batch 204, loss_ca: 1.9423, adv_loss: 0.4626\n",
      "Epoch 8, Batch 205, loss_ca: 2.2399, adv_loss: 0.5433\n",
      "Epoch 8, Batch 206, loss_ca: 2.3797, adv_loss: 0.5887\n",
      "Epoch 8, Batch 207, loss_ca: 2.4526, adv_loss: 0.5483\n",
      "Epoch 8, Batch 208, loss_ca: 2.4499, adv_loss: 0.5428\n",
      "Epoch 8, Batch 209, loss_ca: 2.2064, adv_loss: 0.5405\n",
      "Epoch 8, Batch 210, loss_ca: 2.1158, adv_loss: 0.5441\n",
      "Epoch 8, Batch 211, loss_ca: 2.1019, adv_loss: 0.5783\n",
      "Epoch 8, Batch 212, loss_ca: 1.8598, adv_loss: 0.6288\n",
      "Epoch 8, Batch 213, loss_ca: 2.2105, adv_loss: 0.5885\n",
      "Epoch 8, Batch 214, loss_ca: 1.9275, adv_loss: 0.6325\n",
      "Epoch 8, Batch 215, loss_ca: 1.9114, adv_loss: 0.6302\n",
      "Epoch 8, Batch 216, loss_ca: 1.9959, adv_loss: 0.6467\n",
      "Epoch 8, Batch 217, loss_ca: 2.0473, adv_loss: 0.6356\n",
      "Epoch 8, Batch 218, loss_ca: 1.8956, adv_loss: 0.6132\n",
      "Epoch 8, Batch 219, loss_ca: 2.3033, adv_loss: 0.5943\n",
      "Epoch 8, Batch 220, loss_ca: 2.1971, adv_loss: 0.6005\n",
      "Epoch 8, Batch 221, loss_ca: 2.3833, adv_loss: 0.5526\n",
      "Epoch 8, Batch 222, loss_ca: 2.3096, adv_loss: 0.5580\n",
      "Epoch 8, Batch 223, loss_ca: 2.2497, adv_loss: 0.5509\n",
      "Epoch 8, Batch 224, loss_ca: 2.0968, adv_loss: 0.5787\n",
      "Epoch 8, Batch 225, loss_ca: 2.0144, adv_loss: 0.5478\n",
      "Epoch 8, Batch 226, loss_ca: 1.9331, adv_loss: 0.4943\n",
      "Epoch 8, Batch 227, loss_ca: 1.8108, adv_loss: 0.5045\n",
      "Epoch 8, Batch 228, loss_ca: 1.7565, adv_loss: 0.4987\n",
      "Epoch 9, Batch 9, loss_ca: 1.8496, adv_loss: 0.6116\n",
      "Epoch 9, Batch 10, loss_ca: 1.8337, adv_loss: 0.5830\n",
      "Epoch 9, Batch 11, loss_ca: 1.9128, adv_loss: 0.5112\n",
      "Epoch 9, Batch 12, loss_ca: 1.9601, adv_loss: 0.4949\n",
      "Epoch 9, Batch 13, loss_ca: 1.9744, adv_loss: 0.4908\n",
      "Epoch 9, Batch 14, loss_ca: 2.0361, adv_loss: 0.5376\n",
      "Epoch 9, Batch 15, loss_ca: 1.9729, adv_loss: 0.5418\n",
      "Epoch 9, Batch 16, loss_ca: 1.7925, adv_loss: 0.5573\n",
      "Epoch 9, Batch 17, loss_ca: 1.7369, adv_loss: 0.5497\n",
      "Epoch 9, Batch 18, loss_ca: 1.7543, adv_loss: 0.5170\n",
      "Epoch 9, Batch 19, loss_ca: 1.9120, adv_loss: 0.5984\n",
      "Epoch 9, Batch 20, loss_ca: 1.9669, adv_loss: 0.5195\n",
      "Epoch 9, Batch 21, loss_ca: 1.8633, adv_loss: 0.4901\n",
      "Epoch 9, Batch 22, loss_ca: 1.7740, adv_loss: 0.4906\n",
      "Epoch 9, Batch 23, loss_ca: 1.8013, adv_loss: 0.5002\n",
      "Epoch 9, Batch 24, loss_ca: 2.0282, adv_loss: 0.4969\n",
      "Epoch 9, Batch 25, loss_ca: 2.0871, adv_loss: 0.5013\n",
      "Epoch 9, Batch 26, loss_ca: 2.1593, adv_loss: 0.4984\n",
      "Epoch 9, Batch 27, loss_ca: 2.0675, adv_loss: 0.5034\n",
      "Epoch 9, Batch 28, loss_ca: 1.9541, adv_loss: 0.5034\n",
      "Epoch 9, Batch 29, loss_ca: 1.9439, adv_loss: 0.4823\n",
      "Epoch 9, Batch 30, loss_ca: 2.1228, adv_loss: 0.4915\n",
      "Epoch 9, Batch 31, loss_ca: 2.1167, adv_loss: 0.4930\n",
      "Epoch 9, Batch 32, loss_ca: 2.0297, adv_loss: 0.4976\n",
      "Epoch 9, Batch 33, loss_ca: 2.2202, adv_loss: 0.5082\n",
      "Epoch 9, Batch 34, loss_ca: 2.1922, adv_loss: 0.5437\n",
      "Epoch 9, Batch 35, loss_ca: 2.1970, adv_loss: 0.5694\n",
      "Epoch 9, Batch 36, loss_ca: 2.1691, adv_loss: 0.6111\n",
      "Epoch 9, Batch 37, loss_ca: 2.0551, adv_loss: 0.6159\n",
      "Epoch 9, Batch 38, loss_ca: 2.0525, adv_loss: 0.5992\n",
      "Epoch 9, Batch 39, loss_ca: 2.0732, adv_loss: 0.5964\n",
      "Epoch 9, Batch 40, loss_ca: 2.0470, adv_loss: 0.5934\n",
      "Epoch 9, Batch 41, loss_ca: 2.1929, adv_loss: 0.5804\n",
      "Epoch 9, Batch 42, loss_ca: 2.1573, adv_loss: 0.5777\n",
      "Epoch 9, Batch 43, loss_ca: 2.1287, adv_loss: 0.5674\n",
      "Epoch 9, Batch 44, loss_ca: 2.0658, adv_loss: 0.5565\n",
      "Epoch 9, Batch 45, loss_ca: 1.9659, adv_loss: 0.5699\n",
      "Epoch 9, Batch 46, loss_ca: 1.8537, adv_loss: 0.5572\n",
      "Epoch 9, Batch 47, loss_ca: 1.8540, adv_loss: 0.5729\n",
      "Epoch 9, Batch 48, loss_ca: 1.9040, adv_loss: 0.5774\n",
      "Epoch 9, Batch 49, loss_ca: 1.8883, adv_loss: 0.5692\n",
      "Epoch 9, Batch 50, loss_ca: 1.8976, adv_loss: 0.5641\n",
      "Epoch 9, Batch 51, loss_ca: 1.9327, adv_loss: 0.5556\n",
      "Epoch 9, Batch 52, loss_ca: 2.0093, adv_loss: 0.5539\n",
      "Epoch 9, Batch 53, loss_ca: 1.8043, adv_loss: 0.5662\n",
      "Epoch 9, Batch 54, loss_ca: 1.7584, adv_loss: 0.5511\n",
      "Epoch 9, Batch 55, loss_ca: 1.7420, adv_loss: 0.5358\n",
      "Epoch 9, Batch 56, loss_ca: 1.7504, adv_loss: 0.5358\n",
      "Epoch 9, Batch 57, loss_ca: 1.7682, adv_loss: 0.5268\n",
      "Epoch 9, Batch 58, loss_ca: 1.9400, adv_loss: 0.5357\n",
      "Epoch 9, Batch 59, loss_ca: 1.9068, adv_loss: 0.5402\n",
      "Epoch 9, Batch 60, loss_ca: 1.8927, adv_loss: 0.5446\n",
      "Epoch 9, Batch 61, loss_ca: 1.8993, adv_loss: 0.5380\n",
      "Epoch 9, Batch 62, loss_ca: 1.9691, adv_loss: 0.5397\n",
      "Epoch 9, Batch 63, loss_ca: 1.9981, adv_loss: 0.5235\n",
      "Epoch 9, Batch 64, loss_ca: 1.9945, adv_loss: 0.5402\n",
      "Epoch 9, Batch 65, loss_ca: 1.9086, adv_loss: 0.5467\n",
      "Epoch 9, Batch 66, loss_ca: 1.8572, adv_loss: 0.5724\n",
      "Epoch 9, Batch 67, loss_ca: 1.8317, adv_loss: 0.5615\n",
      "Epoch 9, Batch 68, loss_ca: 1.8860, adv_loss: 0.5487\n",
      "Epoch 9, Batch 69, loss_ca: 1.8815, adv_loss: 0.5525\n",
      "Epoch 9, Batch 70, loss_ca: 1.8622, adv_loss: 0.5520\n",
      "Epoch 9, Batch 71, loss_ca: 1.8550, adv_loss: 0.5481\n",
      "Epoch 9, Batch 72, loss_ca: 1.8143, adv_loss: 0.4822\n",
      "Epoch 9, Batch 73, loss_ca: 1.7057, adv_loss: 0.4794\n",
      "Epoch 9, Batch 74, loss_ca: 1.6565, adv_loss: 0.4704\n",
      "Epoch 9, Batch 75, loss_ca: 1.6313, adv_loss: 0.4708\n",
      "Epoch 9, Batch 76, loss_ca: 1.6223, adv_loss: 0.4700\n",
      "Epoch 9, Batch 77, loss_ca: 1.6639, adv_loss: 0.4697\n",
      "Epoch 9, Batch 78, loss_ca: 1.6846, adv_loss: 0.4782\n",
      "Epoch 9, Batch 79, loss_ca: 1.6874, adv_loss: 0.4769\n",
      "Epoch 9, Batch 80, loss_ca: 1.6751, adv_loss: 0.4662\n",
      "Epoch 9, Batch 81, loss_ca: 1.6464, adv_loss: 0.4764\n",
      "Epoch 9, Batch 82, loss_ca: 1.6900, adv_loss: 0.4720\n",
      "Epoch 9, Batch 83, loss_ca: 1.7382, adv_loss: 0.4609\n",
      "Epoch 9, Batch 84, loss_ca: 1.7811, adv_loss: 0.4746\n",
      "Epoch 9, Batch 85, loss_ca: 1.8312, adv_loss: 0.5150\n",
      "Epoch 9, Batch 86, loss_ca: 1.8644, adv_loss: 0.5092\n",
      "Epoch 9, Batch 87, loss_ca: 1.8308, adv_loss: 0.5088\n",
      "Epoch 9, Batch 88, loss_ca: 1.8222, adv_loss: 0.5022\n",
      "Epoch 9, Batch 89, loss_ca: 1.8520, adv_loss: 0.5046\n",
      "Epoch 9, Batch 90, loss_ca: 1.8025, adv_loss: 0.5149\n",
      "Epoch 9, Batch 91, loss_ca: 1.9252, adv_loss: 0.5154\n",
      "Epoch 9, Batch 92, loss_ca: 1.8315, adv_loss: 0.5034\n",
      "Epoch 9, Batch 93, loss_ca: 1.9404, adv_loss: 0.4923\n",
      "Epoch 9, Batch 94, loss_ca: 1.8548, adv_loss: 0.5118\n",
      "Epoch 9, Batch 95, loss_ca: 1.9791, adv_loss: 0.5084\n",
      "Epoch 9, Batch 96, loss_ca: 1.9785, adv_loss: 0.5092\n",
      "Epoch 9, Batch 97, loss_ca: 1.9339, adv_loss: 0.5102\n",
      "Epoch 9, Batch 98, loss_ca: 1.9210, adv_loss: 0.5051\n",
      "Epoch 9, Batch 99, loss_ca: 1.8633, adv_loss: 0.5251\n",
      "Epoch 9, Batch 100, loss_ca: 1.9130, adv_loss: 0.5019\n",
      "Epoch 9, Batch 101, loss_ca: 2.0191, adv_loss: 0.5245\n",
      "Epoch 9, Batch 102, loss_ca: 1.9937, adv_loss: 0.5404\n",
      "Epoch 9, Batch 103, loss_ca: 1.9763, adv_loss: 0.5420\n",
      "Epoch 9, Batch 104, loss_ca: 2.0204, adv_loss: 0.5534\n",
      "Epoch 9, Batch 105, loss_ca: 2.1195, adv_loss: 0.5632\n",
      "Epoch 9, Batch 106, loss_ca: 2.0939, adv_loss: 0.5765\n",
      "Epoch 9, Batch 107, loss_ca: 2.2016, adv_loss: 0.5876\n",
      "Epoch 9, Batch 108, loss_ca: 2.1002, adv_loss: 0.6018\n",
      "Epoch 9, Batch 109, loss_ca: 2.1499, adv_loss: 0.6058\n",
      "Epoch 9, Batch 110, loss_ca: 2.0396, adv_loss: 0.5857\n",
      "Epoch 9, Batch 111, loss_ca: 1.9457, adv_loss: 0.5626\n",
      "Epoch 9, Batch 112, loss_ca: 1.9195, adv_loss: 0.5341\n",
      "Epoch 9, Batch 113, loss_ca: 1.8278, adv_loss: 0.5238\n",
      "Epoch 9, Batch 114, loss_ca: 1.7980, adv_loss: 0.5250\n",
      "Epoch 9, Batch 115, loss_ca: 1.7952, adv_loss: 0.5358\n",
      "Epoch 9, Batch 116, loss_ca: 1.9459, adv_loss: 0.5376\n",
      "Epoch 9, Batch 117, loss_ca: 1.8926, adv_loss: 0.5035\n",
      "Epoch 9, Batch 118, loss_ca: 1.7680, adv_loss: 0.4880\n",
      "Epoch 9, Batch 119, loss_ca: 1.7710, adv_loss: 0.4682\n",
      "Epoch 9, Batch 120, loss_ca: 1.7813, adv_loss: 0.4867\n",
      "Epoch 9, Batch 121, loss_ca: 1.9584, adv_loss: 0.5019\n",
      "Epoch 9, Batch 122, loss_ca: 2.1240, adv_loss: 0.5122\n",
      "Epoch 9, Batch 123, loss_ca: 2.1399, adv_loss: 0.5144\n",
      "Epoch 9, Batch 124, loss_ca: 2.0899, adv_loss: 0.5176\n",
      "Epoch 9, Batch 125, loss_ca: 2.0006, adv_loss: 0.5236\n",
      "Epoch 9, Batch 126, loss_ca: 2.0283, adv_loss: 0.5104\n",
      "Epoch 9, Batch 127, loss_ca: 1.9920, adv_loss: 0.5387\n",
      "Epoch 9, Batch 128, loss_ca: 2.0412, adv_loss: 0.5467\n",
      "Epoch 9, Batch 129, loss_ca: 2.1019, adv_loss: 0.5668\n",
      "Epoch 9, Batch 130, loss_ca: 2.0337, adv_loss: 0.5759\n",
      "Epoch 9, Batch 131, loss_ca: 1.9592, adv_loss: 0.5672\n",
      "Epoch 9, Batch 132, loss_ca: 1.9709, adv_loss: 0.5698\n",
      "Epoch 9, Batch 133, loss_ca: 1.9653, adv_loss: 0.5669\n",
      "Epoch 9, Batch 134, loss_ca: 1.9637, adv_loss: 0.5678\n",
      "Epoch 9, Batch 135, loss_ca: 1.9595, adv_loss: 0.5877\n",
      "Epoch 9, Batch 136, loss_ca: 1.9663, adv_loss: 0.5951\n",
      "Epoch 9, Batch 137, loss_ca: 1.8416, adv_loss: 0.5765\n",
      "Epoch 9, Batch 138, loss_ca: 1.8343, adv_loss: 0.5555\n",
      "Epoch 9, Batch 139, loss_ca: 1.8390, adv_loss: 0.5469\n",
      "Epoch 9, Batch 140, loss_ca: 1.8031, adv_loss: 0.5288\n",
      "Epoch 9, Batch 141, loss_ca: 1.8321, adv_loss: 0.5520\n",
      "Epoch 9, Batch 142, loss_ca: 1.9159, adv_loss: 0.5261\n",
      "Epoch 9, Batch 143, loss_ca: 1.7719, adv_loss: 0.5133\n",
      "Epoch 9, Batch 144, loss_ca: 1.7809, adv_loss: 0.5087\n",
      "Epoch 9, Batch 145, loss_ca: 1.7810, adv_loss: 0.5262\n",
      "Epoch 9, Batch 146, loss_ca: 1.7689, adv_loss: 0.5458\n",
      "Epoch 9, Batch 147, loss_ca: 1.9666, adv_loss: 0.5615\n",
      "Epoch 9, Batch 148, loss_ca: 2.0072, adv_loss: 0.5523\n",
      "Epoch 9, Batch 149, loss_ca: 1.9259, adv_loss: 0.5539\n",
      "Epoch 9, Batch 150, loss_ca: 1.8758, adv_loss: 0.5571\n",
      "Epoch 9, Batch 151, loss_ca: 1.8488, adv_loss: 0.5455\n",
      "Epoch 9, Batch 152, loss_ca: 1.7733, adv_loss: 0.5284\n",
      "Epoch 9, Batch 153, loss_ca: 1.8520, adv_loss: 0.5177\n",
      "Epoch 9, Batch 154, loss_ca: 1.8440, adv_loss: 0.4954\n",
      "Epoch 9, Batch 155, loss_ca: 1.8646, adv_loss: 0.4886\n",
      "Epoch 9, Batch 156, loss_ca: 1.8959, adv_loss: 0.4913\n",
      "Epoch 9, Batch 157, loss_ca: 1.8942, adv_loss: 0.5343\n",
      "Epoch 9, Batch 158, loss_ca: 2.1780, adv_loss: 0.5210\n",
      "Epoch 9, Batch 159, loss_ca: 2.6090, adv_loss: 0.5242\n",
      "Epoch 9, Batch 160, loss_ca: 2.3195, adv_loss: 0.5059\n",
      "Epoch 9, Batch 161, loss_ca: 2.0991, adv_loss: 0.4814\n",
      "Epoch 9, Batch 162, loss_ca: 2.4222, adv_loss: 0.4643\n",
      "Epoch 9, Batch 163, loss_ca: 2.3528, adv_loss: 0.4852\n",
      "Epoch 9, Batch 164, loss_ca: 2.5217, adv_loss: 0.4622\n",
      "Epoch 9, Batch 165, loss_ca: 2.6491, adv_loss: 0.4877\n",
      "Epoch 9, Batch 166, loss_ca: 2.3421, adv_loss: 0.5270\n",
      "Epoch 9, Batch 167, loss_ca: 2.5627, adv_loss: 0.5392\n",
      "Epoch 9, Batch 168, loss_ca: 2.4577, adv_loss: 0.5746\n",
      "Epoch 9, Batch 169, loss_ca: 2.4432, adv_loss: 0.5963\n",
      "Epoch 9, Batch 170, loss_ca: 2.4774, adv_loss: 0.5901\n",
      "Epoch 9, Batch 171, loss_ca: 2.4514, adv_loss: 0.5973\n",
      "Epoch 9, Batch 172, loss_ca: 2.2605, adv_loss: 0.5866\n",
      "Epoch 9, Batch 173, loss_ca: 2.2594, adv_loss: 0.5559\n",
      "Epoch 9, Batch 174, loss_ca: 2.3536, adv_loss: 0.5562\n",
      "Epoch 9, Batch 175, loss_ca: 2.2781, adv_loss: 0.5516\n",
      "Epoch 9, Batch 176, loss_ca: 2.2215, adv_loss: 0.5353\n",
      "Epoch 9, Batch 177, loss_ca: 2.0256, adv_loss: 0.5052\n",
      "Epoch 9, Batch 178, loss_ca: 2.0447, adv_loss: 0.4962\n",
      "Epoch 9, Batch 179, loss_ca: 1.8184, adv_loss: 0.4885\n",
      "Epoch 9, Batch 180, loss_ca: 1.6956, adv_loss: 0.4513\n",
      "Epoch 9, Batch 181, loss_ca: 1.6925, adv_loss: 0.4808\n",
      "Epoch 9, Batch 182, loss_ca: 1.7072, adv_loss: 0.4653\n",
      "Epoch 9, Batch 183, loss_ca: 1.9559, adv_loss: 0.5476\n",
      "Epoch 9, Batch 184, loss_ca: 1.9288, adv_loss: 0.4974\n",
      "Epoch 9, Batch 185, loss_ca: 1.8078, adv_loss: 0.4870\n",
      "Epoch 9, Batch 186, loss_ca: 1.6784, adv_loss: 0.4529\n",
      "Epoch 9, Batch 187, loss_ca: 1.7050, adv_loss: 0.4511\n",
      "Epoch 9, Batch 188, loss_ca: 1.7524, adv_loss: 0.4520\n",
      "Epoch 9, Batch 189, loss_ca: 1.8363, adv_loss: 0.5138\n",
      "Epoch 9, Batch 190, loss_ca: 2.1264, adv_loss: 0.5193\n",
      "Epoch 9, Batch 191, loss_ca: 2.1683, adv_loss: 0.5307\n",
      "Epoch 9, Batch 192, loss_ca: 2.4084, adv_loss: 0.4952\n",
      "Epoch 9, Batch 193, loss_ca: 2.3757, adv_loss: 0.5131\n",
      "Epoch 9, Batch 194, loss_ca: 2.2925, adv_loss: 0.5009\n",
      "Epoch 9, Batch 195, loss_ca: 2.2958, adv_loss: 0.5045\n",
      "Epoch 9, Batch 196, loss_ca: 2.5643, adv_loss: 0.4862\n",
      "Epoch 9, Batch 197, loss_ca: 2.2714, adv_loss: 0.5483\n",
      "Epoch 9, Batch 198, loss_ca: 2.5897, adv_loss: 0.5752\n",
      "Epoch 9, Batch 199, loss_ca: 2.2927, adv_loss: 0.5515\n",
      "Epoch 9, Batch 200, loss_ca: 2.0462, adv_loss: 0.5415\n",
      "Epoch 9, Batch 201, loss_ca: 2.0202, adv_loss: 0.5259\n",
      "Epoch 9, Batch 202, loss_ca: 2.1449, adv_loss: 0.5187\n",
      "Epoch 9, Batch 203, loss_ca: 2.3940, adv_loss: 0.5348\n",
      "Epoch 9, Batch 204, loss_ca: 2.0215, adv_loss: 0.5167\n",
      "Epoch 9, Batch 205, loss_ca: 2.2729, adv_loss: 0.5754\n",
      "Epoch 9, Batch 206, loss_ca: 1.9919, adv_loss: 0.5778\n",
      "Epoch 9, Batch 207, loss_ca: 2.4937, adv_loss: 0.6253\n",
      "Epoch 9, Batch 208, loss_ca: 2.3563, adv_loss: 0.6199\n",
      "Epoch 9, Batch 209, loss_ca: 2.2642, adv_loss: 0.6242\n",
      "Epoch 9, Batch 210, loss_ca: 2.1747, adv_loss: 0.5866\n",
      "Epoch 9, Batch 211, loss_ca: 2.3413, adv_loss: 0.5445\n",
      "Epoch 9, Batch 212, loss_ca: 1.7735, adv_loss: 0.4482\n",
      "Epoch 9, Batch 213, loss_ca: 2.0819, adv_loss: 0.4983\n",
      "Epoch 9, Batch 214, loss_ca: 1.8209, adv_loss: 0.4576\n",
      "Epoch 9, Batch 215, loss_ca: 1.7354, adv_loss: 0.4913\n",
      "Epoch 9, Batch 216, loss_ca: 1.8513, adv_loss: 0.5176\n",
      "Epoch 9, Batch 217, loss_ca: 1.8331, adv_loss: 0.6050\n",
      "Epoch 9, Batch 218, loss_ca: 1.9350, adv_loss: 0.6136\n",
      "Epoch 9, Batch 219, loss_ca: 2.5239, adv_loss: 0.6406\n",
      "Epoch 9, Batch 220, loss_ca: 2.3949, adv_loss: 0.6359\n",
      "Epoch 9, Batch 221, loss_ca: 2.4641, adv_loss: 0.6046\n",
      "Epoch 9, Batch 222, loss_ca: 2.2938, adv_loss: 0.5986\n",
      "Epoch 9, Batch 223, loss_ca: 2.1415, adv_loss: 0.5948\n",
      "Epoch 9, Batch 224, loss_ca: 2.0176, adv_loss: 0.5920\n",
      "Epoch 9, Batch 225, loss_ca: 1.9104, adv_loss: 0.5934\n",
      "Epoch 9, Batch 226, loss_ca: 1.8817, adv_loss: 0.5998\n",
      "Epoch 9, Batch 227, loss_ca: 2.0960, adv_loss: 0.5131\n",
      "Epoch 9, Batch 228, loss_ca: 1.8794, adv_loss: 0.5149\n",
      "Epoch 10, Batch 10, loss_ca: 1.8700, adv_loss: 0.4998\n",
      "Epoch 10, Batch 11, loss_ca: 1.8535, adv_loss: 0.4298\n",
      "Epoch 10, Batch 12, loss_ca: 1.7271, adv_loss: 0.4470\n",
      "Epoch 10, Batch 13, loss_ca: 1.7647, adv_loss: 0.4129\n",
      "Epoch 10, Batch 14, loss_ca: 1.8410, adv_loss: 0.4873\n",
      "Epoch 10, Batch 15, loss_ca: 1.9431, adv_loss: 0.5117\n",
      "Epoch 10, Batch 16, loss_ca: 1.9769, adv_loss: 0.5333\n",
      "Epoch 10, Batch 17, loss_ca: 1.8394, adv_loss: 0.5015\n",
      "Epoch 10, Batch 18, loss_ca: 1.9390, adv_loss: 0.5723\n",
      "Epoch 10, Batch 19, loss_ca: 1.8810, adv_loss: 0.5258\n",
      "Epoch 10, Batch 20, loss_ca: 2.0106, adv_loss: 0.5984\n",
      "Epoch 10, Batch 21, loss_ca: 1.8982, adv_loss: 0.5808\n",
      "Epoch 10, Batch 22, loss_ca: 1.8033, adv_loss: 0.5953\n",
      "Epoch 10, Batch 23, loss_ca: 1.8434, adv_loss: 0.5661\n",
      "Epoch 10, Batch 24, loss_ca: 2.1491, adv_loss: 0.5426\n",
      "Epoch 10, Batch 25, loss_ca: 2.1821, adv_loss: 0.5280\n",
      "Epoch 10, Batch 26, loss_ca: 2.3072, adv_loss: 0.5224\n",
      "Epoch 10, Batch 27, loss_ca: 2.1516, adv_loss: 0.5278\n",
      "Epoch 10, Batch 28, loss_ca: 1.9543, adv_loss: 0.5394\n",
      "Epoch 10, Batch 29, loss_ca: 1.9617, adv_loss: 0.5378\n",
      "Epoch 10, Batch 30, loss_ca: 2.0227, adv_loss: 0.5407\n",
      "Epoch 10, Batch 31, loss_ca: 1.9948, adv_loss: 0.5191\n",
      "Epoch 10, Batch 32, loss_ca: 1.9026, adv_loss: 0.5088\n",
      "Epoch 10, Batch 33, loss_ca: 2.1352, adv_loss: 0.5008\n",
      "Epoch 10, Batch 34, loss_ca: 2.0740, adv_loss: 0.5022\n",
      "Epoch 10, Batch 35, loss_ca: 2.0513, adv_loss: 0.5134\n",
      "Epoch 10, Batch 36, loss_ca: 2.0722, adv_loss: 0.5292\n",
      "Epoch 10, Batch 37, loss_ca: 2.0431, adv_loss: 0.5517\n",
      "Epoch 10, Batch 38, loss_ca: 2.1097, adv_loss: 0.5644\n",
      "Epoch 10, Batch 39, loss_ca: 2.1116, adv_loss: 0.5771\n",
      "Epoch 10, Batch 40, loss_ca: 2.0577, adv_loss: 0.5787\n",
      "Epoch 10, Batch 41, loss_ca: 2.1566, adv_loss: 0.5770\n",
      "Epoch 10, Batch 42, loss_ca: 2.1025, adv_loss: 0.5610\n",
      "Epoch 10, Batch 43, loss_ca: 2.0249, adv_loss: 0.5508\n",
      "Epoch 10, Batch 44, loss_ca: 1.9368, adv_loss: 0.5583\n",
      "Epoch 10, Batch 45, loss_ca: 1.9085, adv_loss: 0.5750\n",
      "Epoch 10, Batch 46, loss_ca: 1.9159, adv_loss: 0.5776\n",
      "Epoch 10, Batch 47, loss_ca: 1.9472, adv_loss: 0.5825\n",
      "Epoch 10, Batch 48, loss_ca: 2.0049, adv_loss: 0.5601\n",
      "Epoch 10, Batch 49, loss_ca: 1.9837, adv_loss: 0.5446\n",
      "Epoch 10, Batch 50, loss_ca: 1.9018, adv_loss: 0.5233\n",
      "Epoch 10, Batch 51, loss_ca: 1.8539, adv_loss: 0.5298\n",
      "Epoch 10, Batch 52, loss_ca: 1.9406, adv_loss: 0.5255\n",
      "Epoch 10, Batch 53, loss_ca: 1.8192, adv_loss: 0.5311\n",
      "Epoch 10, Batch 54, loss_ca: 1.8116, adv_loss: 0.5253\n",
      "Epoch 10, Batch 55, loss_ca: 1.7896, adv_loss: 0.5161\n",
      "Epoch 10, Batch 56, loss_ca: 1.7850, adv_loss: 0.5130\n",
      "Epoch 10, Batch 57, loss_ca: 1.7622, adv_loss: 0.5277\n",
      "Epoch 10, Batch 58, loss_ca: 1.9123, adv_loss: 0.5500\n",
      "Epoch 10, Batch 59, loss_ca: 1.8680, adv_loss: 0.5518\n",
      "Epoch 10, Batch 60, loss_ca: 1.8769, adv_loss: 0.5745\n",
      "Epoch 10, Batch 61, loss_ca: 1.8811, adv_loss: 0.5765\n",
      "Epoch 10, Batch 62, loss_ca: 1.9647, adv_loss: 0.5926\n",
      "Epoch 10, Batch 63, loss_ca: 2.0071, adv_loss: 0.5570\n",
      "Epoch 10, Batch 64, loss_ca: 1.9854, adv_loss: 0.5349\n",
      "Epoch 10, Batch 65, loss_ca: 1.9399, adv_loss: 0.5494\n",
      "Epoch 10, Batch 66, loss_ca: 1.8512, adv_loss: 0.5751\n",
      "Epoch 10, Batch 67, loss_ca: 1.8419, adv_loss: 0.6264\n",
      "Epoch 10, Batch 68, loss_ca: 1.9103, adv_loss: 0.6218\n",
      "Epoch 10, Batch 69, loss_ca: 1.8849, adv_loss: 0.6161\n",
      "Epoch 10, Batch 70, loss_ca: 1.9308, adv_loss: 0.6170\n",
      "Epoch 10, Batch 71, loss_ca: 1.7822, adv_loss: 0.5594\n",
      "Epoch 10, Batch 72, loss_ca: 1.9096, adv_loss: 0.5452\n",
      "Epoch 10, Batch 73, loss_ca: 1.8442, adv_loss: 0.5336\n",
      "Epoch 10, Batch 74, loss_ca: 1.8004, adv_loss: 0.5162\n",
      "Epoch 10, Batch 75, loss_ca: 1.7418, adv_loss: 0.5110\n",
      "Epoch 10, Batch 76, loss_ca: 1.6374, adv_loss: 0.4973\n",
      "Epoch 10, Batch 77, loss_ca: 1.6328, adv_loss: 0.4906\n",
      "Epoch 10, Batch 78, loss_ca: 1.6642, adv_loss: 0.4822\n",
      "Epoch 10, Batch 79, loss_ca: 1.6469, adv_loss: 0.4680\n",
      "Epoch 10, Batch 80, loss_ca: 1.6523, adv_loss: 0.4699\n",
      "Epoch 10, Batch 81, loss_ca: 1.6663, adv_loss: 0.4793\n",
      "Epoch 10, Batch 82, loss_ca: 1.7482, adv_loss: 0.4956\n",
      "Epoch 10, Batch 83, loss_ca: 1.7337, adv_loss: 0.4830\n",
      "Epoch 10, Batch 84, loss_ca: 1.7561, adv_loss: 0.4900\n",
      "Epoch 10, Batch 85, loss_ca: 1.7763, adv_loss: 0.4817\n",
      "Epoch 10, Batch 86, loss_ca: 1.8505, adv_loss: 0.4814\n",
      "Epoch 10, Batch 87, loss_ca: 1.8307, adv_loss: 0.4868\n",
      "Epoch 10, Batch 88, loss_ca: 1.8033, adv_loss: 0.4797\n",
      "Epoch 10, Batch 89, loss_ca: 1.8676, adv_loss: 0.4783\n",
      "Epoch 10, Batch 90, loss_ca: 1.8231, adv_loss: 0.4838\n",
      "Epoch 10, Batch 91, loss_ca: 1.9296, adv_loss: 0.4779\n",
      "Epoch 10, Batch 92, loss_ca: 1.8564, adv_loss: 0.4844\n",
      "Epoch 10, Batch 93, loss_ca: 1.9602, adv_loss: 0.4896\n",
      "Epoch 10, Batch 94, loss_ca: 1.8488, adv_loss: 0.5138\n",
      "Epoch 10, Batch 95, loss_ca: 1.9546, adv_loss: 0.5262\n",
      "Epoch 10, Batch 96, loss_ca: 1.9980, adv_loss: 0.5389\n",
      "Epoch 10, Batch 97, loss_ca: 1.9663, adv_loss: 0.5422\n",
      "Epoch 10, Batch 98, loss_ca: 1.9634, adv_loss: 0.5453\n",
      "Epoch 10, Batch 99, loss_ca: 1.9045, adv_loss: 0.5543\n",
      "Epoch 10, Batch 100, loss_ca: 1.9519, adv_loss: 0.5486\n",
      "Epoch 10, Batch 101, loss_ca: 2.0385, adv_loss: 0.5460\n",
      "Epoch 10, Batch 102, loss_ca: 2.0230, adv_loss: 0.5598\n",
      "Epoch 10, Batch 103, loss_ca: 2.0395, adv_loss: 0.5683\n",
      "Epoch 10, Batch 104, loss_ca: 2.0313, adv_loss: 0.5697\n",
      "Epoch 10, Batch 105, loss_ca: 2.0952, adv_loss: 0.5491\n",
      "Epoch 10, Batch 106, loss_ca: 2.0646, adv_loss: 0.5478\n",
      "Epoch 10, Batch 107, loss_ca: 2.1523, adv_loss: 0.5485\n",
      "Epoch 10, Batch 108, loss_ca: 2.0929, adv_loss: 0.5708\n",
      "Epoch 10, Batch 109, loss_ca: 2.0774, adv_loss: 0.5732\n",
      "Epoch 10, Batch 110, loss_ca: 2.0215, adv_loss: 0.5660\n",
      "Epoch 10, Batch 111, loss_ca: 1.9719, adv_loss: 0.5586\n",
      "Epoch 10, Batch 112, loss_ca: 1.9840, adv_loss: 0.5428\n",
      "Epoch 10, Batch 113, loss_ca: 1.8547, adv_loss: 0.5353\n",
      "Epoch 10, Batch 114, loss_ca: 1.8347, adv_loss: 0.5342\n",
      "Epoch 10, Batch 115, loss_ca: 1.7478, adv_loss: 0.5369\n",
      "Epoch 10, Batch 116, loss_ca: 1.7905, adv_loss: 0.5329\n",
      "Epoch 10, Batch 117, loss_ca: 1.9499, adv_loss: 0.5587\n",
      "Epoch 10, Batch 118, loss_ca: 1.9174, adv_loss: 0.5522\n",
      "Epoch 10, Batch 119, loss_ca: 1.8515, adv_loss: 0.5468\n",
      "Epoch 10, Batch 120, loss_ca: 1.7815, adv_loss: 0.5452\n",
      "Epoch 10, Batch 121, loss_ca: 1.9229, adv_loss: 0.5411\n",
      "Epoch 10, Batch 122, loss_ca: 2.0543, adv_loss: 0.5491\n",
      "Epoch 10, Batch 123, loss_ca: 2.0695, adv_loss: 0.5531\n",
      "Epoch 10, Batch 124, loss_ca: 2.0568, adv_loss: 0.5634\n",
      "Epoch 10, Batch 125, loss_ca: 1.9739, adv_loss: 0.5790\n",
      "Epoch 10, Batch 126, loss_ca: 1.9628, adv_loss: 0.5833\n",
      "Epoch 10, Batch 127, loss_ca: 1.9847, adv_loss: 0.6093\n",
      "Epoch 10, Batch 128, loss_ca: 2.0868, adv_loss: 0.6061\n",
      "Epoch 10, Batch 129, loss_ca: 2.1255, adv_loss: 0.6106\n",
      "Epoch 10, Batch 130, loss_ca: 2.0082, adv_loss: 0.5972\n",
      "Epoch 10, Batch 131, loss_ca: 1.9449, adv_loss: 0.5783\n",
      "Epoch 10, Batch 132, loss_ca: 1.9887, adv_loss: 0.5766\n",
      "Epoch 10, Batch 133, loss_ca: 1.9720, adv_loss: 0.5947\n",
      "Epoch 10, Batch 134, loss_ca: 1.9913, adv_loss: 0.6152\n",
      "Epoch 10, Batch 135, loss_ca: 1.9710, adv_loss: 0.6121\n",
      "Epoch 10, Batch 136, loss_ca: 2.0284, adv_loss: 0.5976\n",
      "Epoch 10, Batch 137, loss_ca: 1.8862, adv_loss: 0.5828\n",
      "Epoch 10, Batch 138, loss_ca: 1.8664, adv_loss: 0.5857\n",
      "Epoch 10, Batch 139, loss_ca: 1.8514, adv_loss: 0.5566\n",
      "Epoch 10, Batch 140, loss_ca: 1.8142, adv_loss: 0.5389\n",
      "Epoch 10, Batch 141, loss_ca: 1.8152, adv_loss: 0.5476\n",
      "Epoch 10, Batch 142, loss_ca: 1.8925, adv_loss: 0.5264\n",
      "Epoch 10, Batch 143, loss_ca: 1.7847, adv_loss: 0.5230\n",
      "Epoch 10, Batch 144, loss_ca: 1.8116, adv_loss: 0.5304\n",
      "Epoch 10, Batch 145, loss_ca: 1.7906, adv_loss: 0.5249\n",
      "Epoch 10, Batch 146, loss_ca: 1.7617, adv_loss: 0.5051\n",
      "Epoch 10, Batch 147, loss_ca: 1.8999, adv_loss: 0.5182\n",
      "Epoch 10, Batch 148, loss_ca: 1.9449, adv_loss: 0.5227\n",
      "Epoch 10, Batch 149, loss_ca: 1.8683, adv_loss: 0.5400\n",
      "Epoch 10, Batch 150, loss_ca: 1.8434, adv_loss: 0.5406\n",
      "Epoch 10, Batch 151, loss_ca: 1.8274, adv_loss: 0.5271\n",
      "Epoch 10, Batch 152, loss_ca: 1.7674, adv_loss: 0.5183\n",
      "Epoch 10, Batch 153, loss_ca: 1.8583, adv_loss: 0.5038\n",
      "Epoch 10, Batch 154, loss_ca: 1.8478, adv_loss: 0.4850\n",
      "Epoch 10, Batch 155, loss_ca: 1.8441, adv_loss: 0.4982\n",
      "Epoch 10, Batch 156, loss_ca: 1.8565, adv_loss: 0.5126\n",
      "Epoch 10, Batch 157, loss_ca: 1.9336, adv_loss: 0.5365\n",
      "Epoch 10, Batch 158, loss_ca: 2.1905, adv_loss: 0.5106\n",
      "Epoch 10, Batch 159, loss_ca: 2.6190, adv_loss: 0.5282\n",
      "Epoch 10, Batch 160, loss_ca: 2.4028, adv_loss: 0.4943\n",
      "Epoch 10, Batch 161, loss_ca: 2.1446, adv_loss: 0.5005\n",
      "Epoch 10, Batch 162, loss_ca: 2.4541, adv_loss: 0.4706\n",
      "Epoch 10, Batch 163, loss_ca: 2.3966, adv_loss: 0.4827\n",
      "Epoch 10, Batch 164, loss_ca: 2.5724, adv_loss: 0.4826\n",
      "Epoch 10, Batch 165, loss_ca: 2.7180, adv_loss: 0.5003\n",
      "Epoch 10, Batch 166, loss_ca: 2.4072, adv_loss: 0.5127\n",
      "Epoch 10, Batch 167, loss_ca: 2.6380, adv_loss: 0.5212\n",
      "Epoch 10, Batch 168, loss_ca: 2.5685, adv_loss: 0.5709\n",
      "Epoch 10, Batch 169, loss_ca: 2.5326, adv_loss: 0.5709\n",
      "Epoch 10, Batch 170, loss_ca: 2.5529, adv_loss: 0.5636\n",
      "Epoch 10, Batch 171, loss_ca: 2.3958, adv_loss: 0.5429\n",
      "Epoch 10, Batch 172, loss_ca: 2.2944, adv_loss: 0.5176\n",
      "Epoch 10, Batch 173, loss_ca: 2.2548, adv_loss: 0.5286\n",
      "Epoch 10, Batch 174, loss_ca: 2.4466, adv_loss: 0.5528\n",
      "Epoch 10, Batch 175, loss_ca: 2.3320, adv_loss: 0.5320\n",
      "Epoch 10, Batch 176, loss_ca: 2.1501, adv_loss: 0.5160\n",
      "Epoch 10, Batch 177, loss_ca: 1.9819, adv_loss: 0.5090\n",
      "Epoch 10, Batch 178, loss_ca: 2.0027, adv_loss: 0.5310\n",
      "Epoch 10, Batch 179, loss_ca: 1.9331, adv_loss: 0.5569\n",
      "Epoch 10, Batch 180, loss_ca: 1.8411, adv_loss: 0.5340\n",
      "Epoch 10, Batch 181, loss_ca: 1.7928, adv_loss: 0.5266\n",
      "Epoch 10, Batch 182, loss_ca: 1.7684, adv_loss: 0.4978\n",
      "Epoch 10, Batch 183, loss_ca: 1.8795, adv_loss: 0.4873\n",
      "Epoch 10, Batch 184, loss_ca: 1.8823, adv_loss: 0.4903\n",
      "Epoch 10, Batch 185, loss_ca: 1.8848, adv_loss: 0.4779\n",
      "Epoch 10, Batch 186, loss_ca: 1.7854, adv_loss: 0.4736\n",
      "Epoch 10, Batch 187, loss_ca: 1.7426, adv_loss: 0.4596\n",
      "Epoch 10, Batch 188, loss_ca: 1.6512, adv_loss: 0.5021\n",
      "Epoch 10, Batch 189, loss_ca: 1.7796, adv_loss: 0.5053\n",
      "Epoch 10, Batch 190, loss_ca: 2.1107, adv_loss: 0.5268\n",
      "Epoch 10, Batch 191, loss_ca: 2.0927, adv_loss: 0.5476\n",
      "Epoch 10, Batch 192, loss_ca: 2.2901, adv_loss: 0.5443\n",
      "Epoch 10, Batch 193, loss_ca: 2.2505, adv_loss: 0.5621\n",
      "Epoch 10, Batch 194, loss_ca: 2.1498, adv_loss: 0.5747\n",
      "Epoch 10, Batch 195, loss_ca: 2.1966, adv_loss: 0.5755\n",
      "Epoch 10, Batch 196, loss_ca: 2.4921, adv_loss: 0.5785\n",
      "Epoch 10, Batch 197, loss_ca: 2.3706, adv_loss: 0.5656\n",
      "Epoch 10, Batch 198, loss_ca: 2.7760, adv_loss: 0.6103\n",
      "Epoch 10, Batch 199, loss_ca: 2.5803, adv_loss: 0.5877\n",
      "Epoch 10, Batch 200, loss_ca: 2.4441, adv_loss: 0.5028\n",
      "Epoch 10, Batch 201, loss_ca: 2.0655, adv_loss: 0.4680\n",
      "Epoch 10, Batch 202, loss_ca: 2.1172, adv_loss: 0.4287\n",
      "Epoch 10, Batch 203, loss_ca: 2.3495, adv_loss: 0.5061\n",
      "Epoch 10, Batch 204, loss_ca: 1.9807, adv_loss: 0.4888\n",
      "Epoch 10, Batch 205, loss_ca: 2.1743, adv_loss: 0.5208\n",
      "Epoch 10, Batch 206, loss_ca: 1.9850, adv_loss: 0.5886\n",
      "Epoch 10, Batch 207, loss_ca: 2.2502, adv_loss: 0.5677\n",
      "Epoch 10, Batch 208, loss_ca: 2.1675, adv_loss: 0.5686\n",
      "Epoch 10, Batch 209, loss_ca: 2.0409, adv_loss: 0.6099\n",
      "Epoch 10, Batch 210, loss_ca: 2.0253, adv_loss: 0.5526\n",
      "Epoch 10, Batch 211, loss_ca: 2.1084, adv_loss: 0.5825\n",
      "Epoch 10, Batch 212, loss_ca: 1.8140, adv_loss: 0.5428\n",
      "Epoch 10, Batch 213, loss_ca: 2.5446, adv_loss: 0.6289\n",
      "Epoch 10, Batch 214, loss_ca: 1.8972, adv_loss: 0.5164\n",
      "Epoch 10, Batch 215, loss_ca: 1.7468, adv_loss: 0.5140\n",
      "Epoch 10, Batch 216, loss_ca: 1.8395, adv_loss: 0.5591\n",
      "Epoch 10, Batch 217, loss_ca: 1.8673, adv_loss: 0.5861\n",
      "Epoch 10, Batch 218, loss_ca: 1.8663, adv_loss: 0.6133\n",
      "Epoch 10, Batch 219, loss_ca: 2.3276, adv_loss: 0.6426\n",
      "Epoch 10, Batch 220, loss_ca: 2.0134, adv_loss: 0.6100\n",
      "Epoch 10, Batch 221, loss_ca: 2.1732, adv_loss: 0.6146\n",
      "Epoch 10, Batch 222, loss_ca: 2.2009, adv_loss: 0.5914\n",
      "Epoch 10, Batch 223, loss_ca: 2.2301, adv_loss: 0.5624\n",
      "Epoch 10, Batch 224, loss_ca: 2.1694, adv_loss: 0.5649\n",
      "Epoch 10, Batch 225, loss_ca: 2.0678, adv_loss: 0.5330\n",
      "Epoch 10, Batch 226, loss_ca: 1.9643, adv_loss: 0.5101\n",
      "Epoch 10, Batch 227, loss_ca: 1.8729, adv_loss: 0.5508\n",
      "Epoch 10, Batch 228, loss_ca: 1.8391, adv_loss: 0.5322\n",
      "Epoch 11, Batch 11, loss_ca: 1.8555, adv_loss: 0.5519\n",
      "Epoch 11, Batch 12, loss_ca: 1.7604, adv_loss: 0.5034\n",
      "Epoch 11, Batch 13, loss_ca: 1.8723, adv_loss: 0.5070\n",
      "Epoch 11, Batch 14, loss_ca: 2.0011, adv_loss: 0.4954\n",
      "Epoch 11, Batch 15, loss_ca: 2.0434, adv_loss: 0.4648\n",
      "Epoch 11, Batch 16, loss_ca: 1.7738, adv_loss: 0.4613\n",
      "Epoch 11, Batch 17, loss_ca: 1.7045, adv_loss: 0.4655\n",
      "Epoch 11, Batch 18, loss_ca: 1.6961, adv_loss: 0.4798\n",
      "Epoch 11, Batch 19, loss_ca: 1.7897, adv_loss: 0.5244\n",
      "Epoch 11, Batch 20, loss_ca: 1.9091, adv_loss: 0.5586\n",
      "Epoch 11, Batch 21, loss_ca: 1.8727, adv_loss: 0.5549\n",
      "Epoch 11, Batch 22, loss_ca: 1.7527, adv_loss: 0.5464\n",
      "Epoch 11, Batch 23, loss_ca: 1.8467, adv_loss: 0.5491\n",
      "Epoch 11, Batch 24, loss_ca: 2.0708, adv_loss: 0.5421\n",
      "Epoch 11, Batch 25, loss_ca: 2.0898, adv_loss: 0.5546\n",
      "Epoch 11, Batch 26, loss_ca: 2.2452, adv_loss: 0.5475\n",
      "Epoch 11, Batch 27, loss_ca: 2.1116, adv_loss: 0.5398\n",
      "Epoch 11, Batch 28, loss_ca: 1.9585, adv_loss: 0.5304\n",
      "Epoch 11, Batch 29, loss_ca: 1.9637, adv_loss: 0.5374\n",
      "Epoch 11, Batch 30, loss_ca: 2.0617, adv_loss: 0.5573\n",
      "Epoch 11, Batch 31, loss_ca: 2.0306, adv_loss: 0.5558\n",
      "Epoch 11, Batch 32, loss_ca: 1.9485, adv_loss: 0.5651\n",
      "Epoch 11, Batch 33, loss_ca: 2.2364, adv_loss: 0.5800\n",
      "Epoch 11, Batch 34, loss_ca: 2.1643, adv_loss: 0.5993\n",
      "Epoch 11, Batch 35, loss_ca: 2.1523, adv_loss: 0.6006\n",
      "Epoch 11, Batch 36, loss_ca: 2.1391, adv_loss: 0.6071\n",
      "Epoch 11, Batch 37, loss_ca: 2.0573, adv_loss: 0.6297\n",
      "Epoch 11, Batch 38, loss_ca: 2.0568, adv_loss: 0.6308\n",
      "Epoch 11, Batch 39, loss_ca: 2.0881, adv_loss: 0.6266\n",
      "Epoch 11, Batch 40, loss_ca: 2.0578, adv_loss: 0.6248\n",
      "Epoch 11, Batch 41, loss_ca: 2.2232, adv_loss: 0.6132\n",
      "Epoch 11, Batch 42, loss_ca: 2.1758, adv_loss: 0.5967\n",
      "Epoch 11, Batch 43, loss_ca: 2.0498, adv_loss: 0.5639\n",
      "Epoch 11, Batch 44, loss_ca: 1.9317, adv_loss: 0.5490\n",
      "Epoch 11, Batch 45, loss_ca: 1.8961, adv_loss: 0.5287\n",
      "Epoch 11, Batch 46, loss_ca: 1.9029, adv_loss: 0.5381\n",
      "Epoch 11, Batch 47, loss_ca: 1.9628, adv_loss: 0.5669\n",
      "Epoch 11, Batch 48, loss_ca: 1.9480, adv_loss: 0.5646\n",
      "Epoch 11, Batch 49, loss_ca: 1.9105, adv_loss: 0.5578\n",
      "Epoch 11, Batch 50, loss_ca: 1.8627, adv_loss: 0.5408\n",
      "Epoch 11, Batch 51, loss_ca: 1.8137, adv_loss: 0.5235\n",
      "Epoch 11, Batch 52, loss_ca: 1.8885, adv_loss: 0.4978\n",
      "Epoch 11, Batch 53, loss_ca: 1.7805, adv_loss: 0.5239\n",
      "Epoch 11, Batch 54, loss_ca: 1.7579, adv_loss: 0.5308\n",
      "Epoch 11, Batch 55, loss_ca: 1.7597, adv_loss: 0.5368\n",
      "Epoch 11, Batch 56, loss_ca: 1.7640, adv_loss: 0.5506\n",
      "Epoch 11, Batch 57, loss_ca: 1.7808, adv_loss: 0.5534\n",
      "Epoch 11, Batch 58, loss_ca: 1.9056, adv_loss: 0.5717\n",
      "Epoch 11, Batch 59, loss_ca: 1.8844, adv_loss: 0.5615\n",
      "Epoch 11, Batch 60, loss_ca: 1.8549, adv_loss: 0.5553\n",
      "Epoch 11, Batch 61, loss_ca: 1.8937, adv_loss: 0.5617\n",
      "Epoch 11, Batch 62, loss_ca: 1.9472, adv_loss: 0.5482\n",
      "Epoch 11, Batch 63, loss_ca: 1.9521, adv_loss: 0.5258\n",
      "Epoch 11, Batch 64, loss_ca: 1.8999, adv_loss: 0.5247\n",
      "Epoch 11, Batch 65, loss_ca: 1.8360, adv_loss: 0.5328\n",
      "Epoch 11, Batch 66, loss_ca: 1.8425, adv_loss: 0.5528\n",
      "Epoch 11, Batch 67, loss_ca: 1.8292, adv_loss: 0.5729\n",
      "Epoch 11, Batch 68, loss_ca: 1.8311, adv_loss: 0.5625\n",
      "Epoch 11, Batch 69, loss_ca: 1.7879, adv_loss: 0.5462\n",
      "Epoch 11, Batch 70, loss_ca: 1.8014, adv_loss: 0.5658\n",
      "Epoch 11, Batch 71, loss_ca: 1.8223, adv_loss: 0.5370\n",
      "Epoch 11, Batch 72, loss_ca: 1.8745, adv_loss: 0.5027\n",
      "Epoch 11, Batch 73, loss_ca: 1.7649, adv_loss: 0.5112\n",
      "Epoch 11, Batch 74, loss_ca: 1.7060, adv_loss: 0.5030\n",
      "Epoch 11, Batch 75, loss_ca: 1.6865, adv_loss: 0.5014\n",
      "Epoch 11, Batch 76, loss_ca: 1.6580, adv_loss: 0.4919\n",
      "Epoch 11, Batch 77, loss_ca: 1.6853, adv_loss: 0.4971\n",
      "Epoch 11, Batch 78, loss_ca: 1.6905, adv_loss: 0.4865\n",
      "Epoch 11, Batch 79, loss_ca: 1.6882, adv_loss: 0.4968\n",
      "Epoch 11, Batch 80, loss_ca: 1.6834, adv_loss: 0.5232\n",
      "Epoch 11, Batch 81, loss_ca: 1.6995, adv_loss: 0.5379\n",
      "Epoch 11, Batch 82, loss_ca: 1.7334, adv_loss: 0.5425\n",
      "Epoch 11, Batch 83, loss_ca: 1.7509, adv_loss: 0.5486\n",
      "Epoch 11, Batch 84, loss_ca: 1.7640, adv_loss: 0.5438\n",
      "Epoch 11, Batch 85, loss_ca: 1.7797, adv_loss: 0.5413\n",
      "Epoch 11, Batch 86, loss_ca: 1.8531, adv_loss: 0.5350\n",
      "Epoch 11, Batch 87, loss_ca: 1.8429, adv_loss: 0.5195\n",
      "Epoch 11, Batch 88, loss_ca: 1.8159, adv_loss: 0.5046\n",
      "Epoch 11, Batch 89, loss_ca: 1.8511, adv_loss: 0.4909\n",
      "Epoch 11, Batch 90, loss_ca: 1.7425, adv_loss: 0.4769\n",
      "Epoch 11, Batch 91, loss_ca: 1.8607, adv_loss: 0.4896\n",
      "Epoch 11, Batch 92, loss_ca: 1.7976, adv_loss: 0.5007\n",
      "Epoch 11, Batch 93, loss_ca: 1.9284, adv_loss: 0.5092\n",
      "Epoch 11, Batch 94, loss_ca: 1.8485, adv_loss: 0.5201\n",
      "Epoch 11, Batch 95, loss_ca: 2.0034, adv_loss: 0.4998\n",
      "Epoch 11, Batch 96, loss_ca: 2.0389, adv_loss: 0.5248\n",
      "Epoch 11, Batch 97, loss_ca: 2.0123, adv_loss: 0.5298\n",
      "Epoch 11, Batch 98, loss_ca: 1.9945, adv_loss: 0.5331\n",
      "Epoch 11, Batch 99, loss_ca: 1.8966, adv_loss: 0.5452\n",
      "Epoch 11, Batch 100, loss_ca: 1.9621, adv_loss: 0.5250\n",
      "Epoch 11, Batch 101, loss_ca: 2.0405, adv_loss: 0.5369\n",
      "Epoch 11, Batch 102, loss_ca: 2.0111, adv_loss: 0.5469\n",
      "Epoch 11, Batch 103, loss_ca: 1.9916, adv_loss: 0.5531\n",
      "Epoch 11, Batch 104, loss_ca: 1.9761, adv_loss: 0.5476\n",
      "Epoch 11, Batch 105, loss_ca: 2.0931, adv_loss: 0.5314\n",
      "Epoch 11, Batch 106, loss_ca: 2.0826, adv_loss: 0.5565\n",
      "Epoch 11, Batch 107, loss_ca: 2.2221, adv_loss: 0.5579\n",
      "Epoch 11, Batch 108, loss_ca: 2.1751, adv_loss: 0.5914\n",
      "Epoch 11, Batch 109, loss_ca: 2.1655, adv_loss: 0.5917\n",
      "Epoch 11, Batch 110, loss_ca: 2.0150, adv_loss: 0.5795\n",
      "Epoch 11, Batch 111, loss_ca: 1.9044, adv_loss: 0.5665\n",
      "Epoch 11, Batch 112, loss_ca: 1.8786, adv_loss: 0.5494\n",
      "Epoch 11, Batch 113, loss_ca: 1.8085, adv_loss: 0.5427\n",
      "Epoch 11, Batch 114, loss_ca: 1.8291, adv_loss: 0.5330\n",
      "Epoch 11, Batch 115, loss_ca: 1.7934, adv_loss: 0.5461\n",
      "Epoch 11, Batch 116, loss_ca: 1.8873, adv_loss: 0.5407\n",
      "Epoch 11, Batch 117, loss_ca: 1.8755, adv_loss: 0.5294\n",
      "Epoch 11, Batch 118, loss_ca: 1.7842, adv_loss: 0.5135\n",
      "Epoch 11, Batch 119, loss_ca: 1.7505, adv_loss: 0.5099\n",
      "Epoch 11, Batch 120, loss_ca: 1.7463, adv_loss: 0.5090\n",
      "Epoch 11, Batch 121, loss_ca: 1.9220, adv_loss: 0.5071\n",
      "Epoch 11, Batch 122, loss_ca: 2.1421, adv_loss: 0.5373\n",
      "Epoch 11, Batch 123, loss_ca: 2.2016, adv_loss: 0.5504\n",
      "Epoch 11, Batch 124, loss_ca: 2.1529, adv_loss: 0.5657\n",
      "Epoch 11, Batch 125, loss_ca: 2.0229, adv_loss: 0.5435\n",
      "Epoch 11, Batch 126, loss_ca: 1.9610, adv_loss: 0.5027\n",
      "Epoch 11, Batch 127, loss_ca: 1.9969, adv_loss: 0.5040\n",
      "Epoch 11, Batch 128, loss_ca: 1.9737, adv_loss: 0.4793\n",
      "Epoch 11, Batch 129, loss_ca: 1.9770, adv_loss: 0.4770\n",
      "Epoch 11, Batch 130, loss_ca: 1.9375, adv_loss: 0.4824\n",
      "Epoch 11, Batch 131, loss_ca: 1.8781, adv_loss: 0.4818\n",
      "Epoch 11, Batch 132, loss_ca: 2.0255, adv_loss: 0.5260\n",
      "Epoch 11, Batch 133, loss_ca: 2.0910, adv_loss: 0.5635\n",
      "Epoch 11, Batch 134, loss_ca: 2.0145, adv_loss: 0.5861\n",
      "Epoch 11, Batch 135, loss_ca: 1.9179, adv_loss: 0.6040\n",
      "Epoch 11, Batch 136, loss_ca: 1.8884, adv_loss: 0.5888\n",
      "Epoch 11, Batch 137, loss_ca: 1.8201, adv_loss: 0.5826\n",
      "Epoch 11, Batch 138, loss_ca: 1.8398, adv_loss: 0.5750\n",
      "Epoch 11, Batch 139, loss_ca: 1.8248, adv_loss: 0.5908\n",
      "Epoch 11, Batch 140, loss_ca: 1.8391, adv_loss: 0.5933\n",
      "Epoch 11, Batch 141, loss_ca: 1.9005, adv_loss: 0.6064\n",
      "Epoch 11, Batch 142, loss_ca: 2.0225, adv_loss: 0.6232\n",
      "Epoch 11, Batch 143, loss_ca: 1.9721, adv_loss: 0.5962\n",
      "Epoch 11, Batch 144, loss_ca: 1.9505, adv_loss: 0.5749\n",
      "Epoch 11, Batch 145, loss_ca: 1.8911, adv_loss: 0.5780\n",
      "Epoch 11, Batch 146, loss_ca: 1.7612, adv_loss: 0.5684\n",
      "Epoch 11, Batch 147, loss_ca: 1.8405, adv_loss: 0.5548\n",
      "Epoch 11, Batch 148, loss_ca: 1.8214, adv_loss: 0.5421\n",
      "Epoch 11, Batch 149, loss_ca: 1.8018, adv_loss: 0.5291\n",
      "Epoch 11, Batch 150, loss_ca: 1.7995, adv_loss: 0.5170\n",
      "Epoch 11, Batch 151, loss_ca: 1.7787, adv_loss: 0.5198\n",
      "Epoch 11, Batch 152, loss_ca: 1.7350, adv_loss: 0.5231\n",
      "Epoch 11, Batch 153, loss_ca: 1.8887, adv_loss: 0.5180\n",
      "Epoch 11, Batch 154, loss_ca: 1.8130, adv_loss: 0.4821\n",
      "Epoch 11, Batch 155, loss_ca: 1.8813, adv_loss: 0.4858\n",
      "Epoch 11, Batch 156, loss_ca: 1.9084, adv_loss: 0.4830\n",
      "Epoch 11, Batch 157, loss_ca: 1.9064, adv_loss: 0.4750\n",
      "Epoch 11, Batch 158, loss_ca: 2.1468, adv_loss: 0.4844\n",
      "Epoch 11, Batch 159, loss_ca: 2.5152, adv_loss: 0.4806\n",
      "Epoch 11, Batch 160, loss_ca: 2.3351, adv_loss: 0.4982\n",
      "Epoch 11, Batch 161, loss_ca: 2.0318, adv_loss: 0.4987\n",
      "Epoch 11, Batch 162, loss_ca: 2.4154, adv_loss: 0.4849\n",
      "Epoch 11, Batch 163, loss_ca: 2.2342, adv_loss: 0.5282\n",
      "Epoch 11, Batch 164, loss_ca: 2.4753, adv_loss: 0.5117\n",
      "Epoch 11, Batch 165, loss_ca: 2.6576, adv_loss: 0.5385\n",
      "Epoch 11, Batch 166, loss_ca: 2.3727, adv_loss: 0.5817\n",
      "Epoch 11, Batch 167, loss_ca: 2.8387, adv_loss: 0.5492\n",
      "Epoch 11, Batch 168, loss_ca: 2.9103, adv_loss: 0.5856\n",
      "Epoch 11, Batch 169, loss_ca: 2.8270, adv_loss: 0.6357\n",
      "Epoch 11, Batch 170, loss_ca: 2.7301, adv_loss: 0.6672\n",
      "Epoch 11, Batch 171, loss_ca: 2.4703, adv_loss: 0.6250\n",
      "Epoch 11, Batch 172, loss_ca: 2.3128, adv_loss: 0.5869\n",
      "Epoch 11, Batch 173, loss_ca: 2.1587, adv_loss: 0.5388\n",
      "Epoch 11, Batch 174, loss_ca: 2.1902, adv_loss: 0.5105\n",
      "Epoch 11, Batch 175, loss_ca: 2.1055, adv_loss: 0.5036\n",
      "Epoch 11, Batch 176, loss_ca: 1.9885, adv_loss: 0.4925\n",
      "Epoch 11, Batch 177, loss_ca: 1.8777, adv_loss: 0.5326\n",
      "Epoch 11, Batch 178, loss_ca: 1.9610, adv_loss: 0.5517\n",
      "Epoch 11, Batch 179, loss_ca: 1.8746, adv_loss: 0.5462\n",
      "Epoch 11, Batch 180, loss_ca: 1.7167, adv_loss: 0.5127\n",
      "Epoch 11, Batch 181, loss_ca: 1.7263, adv_loss: 0.5026\n",
      "Epoch 11, Batch 182, loss_ca: 1.7612, adv_loss: 0.5097\n",
      "Epoch 11, Batch 183, loss_ca: 1.9093, adv_loss: 0.4760\n",
      "Epoch 11, Batch 184, loss_ca: 1.8944, adv_loss: 0.4548\n",
      "Epoch 11, Batch 185, loss_ca: 1.8204, adv_loss: 0.4711\n",
      "Epoch 11, Batch 186, loss_ca: 1.7185, adv_loss: 0.4614\n",
      "Epoch 11, Batch 187, loss_ca: 1.7309, adv_loss: 0.4655\n",
      "Epoch 11, Batch 188, loss_ca: 1.6726, adv_loss: 0.4502\n",
      "Epoch 11, Batch 189, loss_ca: 1.7747, adv_loss: 0.4830\n",
      "Epoch 11, Batch 190, loss_ca: 2.0393, adv_loss: 0.5084\n",
      "Epoch 11, Batch 191, loss_ca: 2.1169, adv_loss: 0.5269\n",
      "Epoch 11, Batch 192, loss_ca: 2.2696, adv_loss: 0.5412\n",
      "Epoch 11, Batch 193, loss_ca: 2.2317, adv_loss: 0.5583\n",
      "Epoch 11, Batch 194, loss_ca: 2.2459, adv_loss: 0.5772\n",
      "Epoch 11, Batch 195, loss_ca: 2.3828, adv_loss: 0.5745\n",
      "Epoch 11, Batch 196, loss_ca: 2.7190, adv_loss: 0.5707\n",
      "Epoch 11, Batch 197, loss_ca: 2.3960, adv_loss: 0.5823\n",
      "Epoch 11, Batch 198, loss_ca: 2.7558, adv_loss: 0.6059\n",
      "Epoch 11, Batch 199, loss_ca: 2.4642, adv_loss: 0.5823\n",
      "Epoch 11, Batch 200, loss_ca: 2.1920, adv_loss: 0.4865\n",
      "Epoch 11, Batch 201, loss_ca: 1.8765, adv_loss: 0.4449\n",
      "Epoch 11, Batch 202, loss_ca: 2.0689, adv_loss: 0.4663\n",
      "Epoch 11, Batch 203, loss_ca: 2.2476, adv_loss: 0.5001\n",
      "Epoch 11, Batch 204, loss_ca: 1.8919, adv_loss: 0.5160\n",
      "Epoch 11, Batch 205, loss_ca: 2.2389, adv_loss: 0.5809\n",
      "Epoch 11, Batch 206, loss_ca: 1.9105, adv_loss: 0.5566\n",
      "Epoch 11, Batch 207, loss_ca: 2.1281, adv_loss: 0.6088\n",
      "Epoch 11, Batch 208, loss_ca: 2.2636, adv_loss: 0.6327\n",
      "Epoch 11, Batch 209, loss_ca: 2.0208, adv_loss: 0.6779\n",
      "Epoch 11, Batch 210, loss_ca: 2.1256, adv_loss: 0.6469\n",
      "Epoch 11, Batch 211, loss_ca: 2.2877, adv_loss: 0.5735\n",
      "Epoch 11, Batch 212, loss_ca: 1.9577, adv_loss: 0.6195\n",
      "Epoch 11, Batch 213, loss_ca: 2.1774, adv_loss: 0.5663\n",
      "Epoch 11, Batch 214, loss_ca: 2.0093, adv_loss: 0.5470\n",
      "Epoch 11, Batch 215, loss_ca: 1.8231, adv_loss: 0.5633\n",
      "Epoch 11, Batch 216, loss_ca: 2.0295, adv_loss: 0.6156\n",
      "Epoch 11, Batch 217, loss_ca: 1.9120, adv_loss: 0.6011\n",
      "Epoch 11, Batch 218, loss_ca: 1.7637, adv_loss: 0.5619\n",
      "Epoch 11, Batch 219, loss_ca: 2.1436, adv_loss: 0.5563\n",
      "Epoch 11, Batch 220, loss_ca: 1.9788, adv_loss: 0.5158\n",
      "Epoch 11, Batch 221, loss_ca: 1.9981, adv_loss: 0.5368\n",
      "Epoch 11, Batch 222, loss_ca: 2.0839, adv_loss: 0.6266\n",
      "Epoch 11, Batch 223, loss_ca: 2.2504, adv_loss: 0.6378\n",
      "Epoch 11, Batch 224, loss_ca: 2.3428, adv_loss: 0.6596\n",
      "Epoch 11, Batch 225, loss_ca: 2.3217, adv_loss: 0.6323\n",
      "Epoch 11, Batch 226, loss_ca: 2.2246, adv_loss: 0.5710\n",
      "Epoch 11, Batch 227, loss_ca: 2.0896, adv_loss: 0.5654\n",
      "Epoch 11, Batch 228, loss_ca: 1.9581, adv_loss: 0.4892\n",
      "Epoch 12, Batch 12, loss_ca: 1.8332, adv_loss: 0.5009\n",
      "Epoch 12, Batch 13, loss_ca: 1.7544, adv_loss: 0.5451\n",
      "Epoch 12, Batch 14, loss_ca: 1.9019, adv_loss: 0.5301\n",
      "Epoch 12, Batch 15, loss_ca: 1.8494, adv_loss: 0.5240\n",
      "Epoch 12, Batch 16, loss_ca: 1.8346, adv_loss: 0.4774\n",
      "Epoch 12, Batch 17, loss_ca: 1.7842, adv_loss: 0.4514\n",
      "Epoch 12, Batch 18, loss_ca: 1.8570, adv_loss: 0.4613\n",
      "Epoch 12, Batch 19, loss_ca: 1.8211, adv_loss: 0.4694\n",
      "Epoch 12, Batch 20, loss_ca: 1.9094, adv_loss: 0.4723\n",
      "Epoch 12, Batch 21, loss_ca: 1.8233, adv_loss: 0.5129\n",
      "Epoch 12, Batch 22, loss_ca: 1.7729, adv_loss: 0.5405\n",
      "Epoch 12, Batch 23, loss_ca: 1.8649, adv_loss: 0.5531\n",
      "Epoch 12, Batch 24, loss_ca: 2.1246, adv_loss: 0.5277\n",
      "Epoch 12, Batch 25, loss_ca: 2.1397, adv_loss: 0.5313\n",
      "Epoch 12, Batch 26, loss_ca: 2.2649, adv_loss: 0.5231\n",
      "Epoch 12, Batch 27, loss_ca: 2.1060, adv_loss: 0.5120\n",
      "Epoch 12, Batch 28, loss_ca: 1.9924, adv_loss: 0.5131\n",
      "Epoch 12, Batch 29, loss_ca: 1.9304, adv_loss: 0.5207\n",
      "Epoch 12, Batch 30, loss_ca: 2.0677, adv_loss: 0.5397\n",
      "Epoch 12, Batch 31, loss_ca: 2.0008, adv_loss: 0.5465\n",
      "Epoch 12, Batch 32, loss_ca: 1.9354, adv_loss: 0.5628\n",
      "Epoch 12, Batch 33, loss_ca: 2.1850, adv_loss: 0.5745\n",
      "Epoch 12, Batch 34, loss_ca: 2.1087, adv_loss: 0.5845\n",
      "Epoch 12, Batch 35, loss_ca: 2.0838, adv_loss: 0.5865\n",
      "Epoch 12, Batch 36, loss_ca: 2.0774, adv_loss: 0.5754\n",
      "Epoch 12, Batch 37, loss_ca: 2.0268, adv_loss: 0.5928\n",
      "Epoch 12, Batch 38, loss_ca: 2.0382, adv_loss: 0.5834\n",
      "Epoch 12, Batch 39, loss_ca: 1.9978, adv_loss: 0.5803\n",
      "Epoch 12, Batch 40, loss_ca: 1.9792, adv_loss: 0.5854\n",
      "Epoch 12, Batch 41, loss_ca: 2.1417, adv_loss: 0.5836\n",
      "Epoch 12, Batch 42, loss_ca: 2.1679, adv_loss: 0.5866\n",
      "Epoch 12, Batch 43, loss_ca: 2.0885, adv_loss: 0.5732\n",
      "Epoch 12, Batch 44, loss_ca: 1.9569, adv_loss: 0.5592\n",
      "Epoch 12, Batch 45, loss_ca: 1.8704, adv_loss: 0.5538\n",
      "Epoch 12, Batch 46, loss_ca: 1.8596, adv_loss: 0.5464\n",
      "Epoch 12, Batch 47, loss_ca: 1.8776, adv_loss: 0.5587\n",
      "Epoch 12, Batch 48, loss_ca: 1.8623, adv_loss: 0.5563\n",
      "Epoch 12, Batch 49, loss_ca: 1.8610, adv_loss: 0.5580\n",
      "Epoch 12, Batch 50, loss_ca: 1.9043, adv_loss: 0.5464\n",
      "Epoch 12, Batch 51, loss_ca: 1.9031, adv_loss: 0.5354\n",
      "Epoch 12, Batch 52, loss_ca: 1.9086, adv_loss: 0.4951\n",
      "Epoch 12, Batch 53, loss_ca: 1.8018, adv_loss: 0.4970\n",
      "Epoch 12, Batch 54, loss_ca: 1.7467, adv_loss: 0.4767\n",
      "Epoch 12, Batch 55, loss_ca: 1.7246, adv_loss: 0.4809\n",
      "Epoch 12, Batch 56, loss_ca: 1.7198, adv_loss: 0.4913\n",
      "Epoch 12, Batch 57, loss_ca: 1.7498, adv_loss: 0.5071\n",
      "Epoch 12, Batch 58, loss_ca: 1.9479, adv_loss: 0.5215\n",
      "Epoch 12, Batch 59, loss_ca: 1.9247, adv_loss: 0.5009\n",
      "Epoch 12, Batch 60, loss_ca: 1.8992, adv_loss: 0.5233\n",
      "Epoch 12, Batch 61, loss_ca: 1.9184, adv_loss: 0.5776\n",
      "Epoch 12, Batch 62, loss_ca: 1.9173, adv_loss: 0.5907\n",
      "Epoch 12, Batch 63, loss_ca: 1.9680, adv_loss: 0.5846\n",
      "Epoch 12, Batch 64, loss_ca: 1.9418, adv_loss: 0.5820\n",
      "Epoch 12, Batch 65, loss_ca: 1.8719, adv_loss: 0.5960\n",
      "Epoch 12, Batch 66, loss_ca: 1.8752, adv_loss: 0.6071\n",
      "Epoch 12, Batch 67, loss_ca: 1.7969, adv_loss: 0.5868\n",
      "Epoch 12, Batch 68, loss_ca: 1.8171, adv_loss: 0.5575\n",
      "Epoch 12, Batch 69, loss_ca: 1.9050, adv_loss: 0.5478\n",
      "Epoch 12, Batch 70, loss_ca: 1.8112, adv_loss: 0.5478\n",
      "Epoch 12, Batch 71, loss_ca: 1.7785, adv_loss: 0.5457\n",
      "Epoch 12, Batch 72, loss_ca: 1.8511, adv_loss: 0.5198\n",
      "Epoch 12, Batch 73, loss_ca: 1.8082, adv_loss: 0.5118\n",
      "Epoch 12, Batch 74, loss_ca: 1.8055, adv_loss: 0.5069\n",
      "Epoch 12, Batch 75, loss_ca: 1.7362, adv_loss: 0.4987\n",
      "Epoch 12, Batch 76, loss_ca: 1.6528, adv_loss: 0.4874\n",
      "Epoch 12, Batch 77, loss_ca: 1.6439, adv_loss: 0.4792\n",
      "Epoch 12, Batch 78, loss_ca: 1.6728, adv_loss: 0.4534\n",
      "Epoch 12, Batch 79, loss_ca: 1.6660, adv_loss: 0.4674\n",
      "Epoch 12, Batch 80, loss_ca: 1.6530, adv_loss: 0.4911\n",
      "Epoch 12, Batch 81, loss_ca: 1.7033, adv_loss: 0.5046\n",
      "Epoch 12, Batch 82, loss_ca: 1.7442, adv_loss: 0.4999\n",
      "Epoch 12, Batch 83, loss_ca: 1.7606, adv_loss: 0.5061\n",
      "Epoch 12, Batch 84, loss_ca: 1.7418, adv_loss: 0.5005\n",
      "Epoch 12, Batch 85, loss_ca: 1.7189, adv_loss: 0.4748\n",
      "Epoch 12, Batch 86, loss_ca: 1.8068, adv_loss: 0.4847\n",
      "Epoch 12, Batch 87, loss_ca: 1.8055, adv_loss: 0.4966\n",
      "Epoch 12, Batch 88, loss_ca: 1.8043, adv_loss: 0.5095\n",
      "Epoch 12, Batch 89, loss_ca: 1.8736, adv_loss: 0.5121\n",
      "Epoch 12, Batch 90, loss_ca: 1.8014, adv_loss: 0.5133\n",
      "Epoch 12, Batch 91, loss_ca: 1.9379, adv_loss: 0.5179\n",
      "Epoch 12, Batch 92, loss_ca: 1.8774, adv_loss: 0.5097\n",
      "Epoch 12, Batch 93, loss_ca: 1.9459, adv_loss: 0.5143\n",
      "Epoch 12, Batch 94, loss_ca: 1.8525, adv_loss: 0.5043\n",
      "Epoch 12, Batch 95, loss_ca: 1.8893, adv_loss: 0.5108\n",
      "Epoch 12, Batch 96, loss_ca: 1.9485, adv_loss: 0.5298\n",
      "Epoch 12, Batch 97, loss_ca: 1.9424, adv_loss: 0.5208\n",
      "Epoch 12, Batch 98, loss_ca: 1.9502, adv_loss: 0.5293\n",
      "Epoch 12, Batch 99, loss_ca: 1.8651, adv_loss: 0.5108\n",
      "Epoch 12, Batch 100, loss_ca: 1.9258, adv_loss: 0.5286\n",
      "Epoch 12, Batch 101, loss_ca: 2.0148, adv_loss: 0.5205\n",
      "Epoch 12, Batch 102, loss_ca: 2.0224, adv_loss: 0.5626\n",
      "Epoch 12, Batch 103, loss_ca: 1.9940, adv_loss: 0.5476\n",
      "Epoch 12, Batch 104, loss_ca: 2.0390, adv_loss: 0.5439\n",
      "Epoch 12, Batch 105, loss_ca: 2.1639, adv_loss: 0.5003\n",
      "Epoch 12, Batch 106, loss_ca: 2.0948, adv_loss: 0.5220\n",
      "Epoch 12, Batch 107, loss_ca: 2.1638, adv_loss: 0.5308\n",
      "Epoch 12, Batch 108, loss_ca: 2.0353, adv_loss: 0.5737\n",
      "Epoch 12, Batch 109, loss_ca: 2.0851, adv_loss: 0.5934\n",
      "Epoch 12, Batch 110, loss_ca: 2.0308, adv_loss: 0.5951\n",
      "Epoch 12, Batch 111, loss_ca: 1.9748, adv_loss: 0.5934\n",
      "Epoch 12, Batch 112, loss_ca: 1.9966, adv_loss: 0.5834\n",
      "Epoch 12, Batch 113, loss_ca: 1.8848, adv_loss: 0.5682\n",
      "Epoch 12, Batch 114, loss_ca: 1.8528, adv_loss: 0.5566\n",
      "Epoch 12, Batch 115, loss_ca: 1.7884, adv_loss: 0.5430\n",
      "Epoch 12, Batch 116, loss_ca: 1.8455, adv_loss: 0.5356\n",
      "Epoch 12, Batch 117, loss_ca: 1.8425, adv_loss: 0.5068\n",
      "Epoch 12, Batch 118, loss_ca: 1.8503, adv_loss: 0.4975\n",
      "Epoch 12, Batch 119, loss_ca: 1.7906, adv_loss: 0.4836\n",
      "Epoch 12, Batch 120, loss_ca: 1.7349, adv_loss: 0.4679\n",
      "Epoch 12, Batch 121, loss_ca: 1.8898, adv_loss: 0.4637\n",
      "Epoch 12, Batch 122, loss_ca: 2.0339, adv_loss: 0.4876\n",
      "Epoch 12, Batch 123, loss_ca: 2.0242, adv_loss: 0.5079\n",
      "Epoch 12, Batch 124, loss_ca: 2.0346, adv_loss: 0.5386\n",
      "Epoch 12, Batch 125, loss_ca: 1.9907, adv_loss: 0.5492\n",
      "Epoch 12, Batch 126, loss_ca: 1.9972, adv_loss: 0.5504\n",
      "Epoch 12, Batch 127, loss_ca: 2.0311, adv_loss: 0.5479\n",
      "Epoch 12, Batch 128, loss_ca: 2.1095, adv_loss: 0.5528\n",
      "Epoch 12, Batch 129, loss_ca: 2.1050, adv_loss: 0.5694\n",
      "Epoch 12, Batch 130, loss_ca: 1.9769, adv_loss: 0.5858\n",
      "Epoch 12, Batch 131, loss_ca: 1.9460, adv_loss: 0.5988\n",
      "Epoch 12, Batch 132, loss_ca: 2.0165, adv_loss: 0.6164\n",
      "Epoch 12, Batch 133, loss_ca: 1.9454, adv_loss: 0.5947\n",
      "Epoch 12, Batch 134, loss_ca: 1.9316, adv_loss: 0.5837\n",
      "Epoch 12, Batch 135, loss_ca: 1.9066, adv_loss: 0.5874\n",
      "Epoch 12, Batch 136, loss_ca: 1.9226, adv_loss: 0.5983\n",
      "Epoch 12, Batch 137, loss_ca: 1.8113, adv_loss: 0.5876\n",
      "Epoch 12, Batch 138, loss_ca: 1.7841, adv_loss: 0.5624\n",
      "Epoch 12, Batch 139, loss_ca: 1.7858, adv_loss: 0.5532\n",
      "Epoch 12, Batch 140, loss_ca: 1.8095, adv_loss: 0.5625\n",
      "Epoch 12, Batch 141, loss_ca: 1.8210, adv_loss: 0.5618\n",
      "Epoch 12, Batch 142, loss_ca: 1.9693, adv_loss: 0.5539\n",
      "Epoch 12, Batch 143, loss_ca: 1.8424, adv_loss: 0.5379\n",
      "Epoch 12, Batch 144, loss_ca: 1.8647, adv_loss: 0.5162\n",
      "Epoch 12, Batch 145, loss_ca: 1.8163, adv_loss: 0.5073\n",
      "Epoch 12, Batch 146, loss_ca: 1.7644, adv_loss: 0.5036\n",
      "Epoch 12, Batch 147, loss_ca: 1.8731, adv_loss: 0.5004\n",
      "Epoch 12, Batch 148, loss_ca: 1.9481, adv_loss: 0.5051\n",
      "Epoch 12, Batch 149, loss_ca: 1.8714, adv_loss: 0.5180\n",
      "Epoch 12, Batch 150, loss_ca: 1.7963, adv_loss: 0.5169\n",
      "Epoch 12, Batch 151, loss_ca: 1.7720, adv_loss: 0.5201\n",
      "Epoch 12, Batch 152, loss_ca: 1.7074, adv_loss: 0.5129\n",
      "Epoch 12, Batch 153, loss_ca: 1.8161, adv_loss: 0.5055\n",
      "Epoch 12, Batch 154, loss_ca: 1.8588, adv_loss: 0.5060\n",
      "Epoch 12, Batch 155, loss_ca: 1.8429, adv_loss: 0.5452\n",
      "Epoch 12, Batch 156, loss_ca: 1.8259, adv_loss: 0.5347\n",
      "Epoch 12, Batch 157, loss_ca: 1.8446, adv_loss: 0.5333\n",
      "Epoch 12, Batch 158, loss_ca: 2.1357, adv_loss: 0.5452\n",
      "Epoch 12, Batch 159, loss_ca: 2.5672, adv_loss: 0.5247\n",
      "Epoch 12, Batch 160, loss_ca: 2.3311, adv_loss: 0.5254\n",
      "Epoch 12, Batch 161, loss_ca: 2.1120, adv_loss: 0.5036\n",
      "Epoch 12, Batch 162, loss_ca: 2.4254, adv_loss: 0.4973\n",
      "Epoch 12, Batch 163, loss_ca: 2.3664, adv_loss: 0.4644\n",
      "Epoch 12, Batch 164, loss_ca: 2.5060, adv_loss: 0.4802\n",
      "Epoch 12, Batch 165, loss_ca: 2.7264, adv_loss: 0.5128\n",
      "Epoch 12, Batch 166, loss_ca: 2.3246, adv_loss: 0.5129\n",
      "Epoch 12, Batch 167, loss_ca: 2.3273, adv_loss: 0.4952\n",
      "Epoch 12, Batch 168, loss_ca: 2.3927, adv_loss: 0.5456\n",
      "Epoch 12, Batch 169, loss_ca: 2.4125, adv_loss: 0.5702\n",
      "Epoch 12, Batch 170, loss_ca: 2.3374, adv_loss: 0.5228\n",
      "Epoch 12, Batch 171, loss_ca: 2.4132, adv_loss: 0.5617\n",
      "Epoch 12, Batch 172, loss_ca: 3.0179, adv_loss: 0.6410\n",
      "Epoch 12, Batch 173, loss_ca: 2.1379, adv_loss: 0.4733\n",
      "Epoch 12, Batch 174, loss_ca: 2.3064, adv_loss: 0.4545\n",
      "Epoch 12, Batch 175, loss_ca: 2.1372, adv_loss: 0.4152\n",
      "Epoch 12, Batch 176, loss_ca: 1.9972, adv_loss: 0.4481\n",
      "Epoch 12, Batch 177, loss_ca: 1.9365, adv_loss: 0.4710\n",
      "Epoch 12, Batch 178, loss_ca: 2.0154, adv_loss: 0.5441\n",
      "Epoch 12, Batch 179, loss_ca: 1.9206, adv_loss: 0.5500\n",
      "Epoch 12, Batch 180, loss_ca: 1.8728, adv_loss: 0.5602\n",
      "Epoch 12, Batch 181, loss_ca: 1.8179, adv_loss: 0.5369\n",
      "Epoch 12, Batch 182, loss_ca: 1.8979, adv_loss: 0.5701\n",
      "Epoch 12, Batch 183, loss_ca: 1.9167, adv_loss: 0.5128\n",
      "Epoch 12, Batch 184, loss_ca: 1.8682, adv_loss: 0.5572\n",
      "Epoch 12, Batch 185, loss_ca: 1.8520, adv_loss: 0.5550\n",
      "Epoch 12, Batch 186, loss_ca: 1.8414, adv_loss: 0.5360\n",
      "Epoch 12, Batch 187, loss_ca: 1.7740, adv_loss: 0.5290\n",
      "Epoch 12, Batch 188, loss_ca: 1.7351, adv_loss: 0.5286\n",
      "Epoch 12, Batch 189, loss_ca: 1.7779, adv_loss: 0.4889\n",
      "Epoch 12, Batch 190, loss_ca: 1.9938, adv_loss: 0.5011\n",
      "Epoch 12, Batch 191, loss_ca: 1.9851, adv_loss: 0.5148\n",
      "Epoch 12, Batch 192, loss_ca: 2.2086, adv_loss: 0.5167\n",
      "Epoch 12, Batch 193, loss_ca: 2.1525, adv_loss: 0.5601\n",
      "Epoch 12, Batch 194, loss_ca: 2.1885, adv_loss: 0.5793\n",
      "Epoch 12, Batch 195, loss_ca: 2.1715, adv_loss: 0.5962\n",
      "Epoch 12, Batch 196, loss_ca: 2.3853, adv_loss: 0.6112\n",
      "Epoch 12, Batch 197, loss_ca: 2.3463, adv_loss: 0.6122\n",
      "Epoch 12, Batch 198, loss_ca: 2.7426, adv_loss: 0.6170\n",
      "Epoch 12, Batch 199, loss_ca: 2.5091, adv_loss: 0.5920\n",
      "Epoch 12, Batch 200, loss_ca: 2.3863, adv_loss: 0.5845\n",
      "Epoch 12, Batch 201, loss_ca: 2.1108, adv_loss: 0.5835\n",
      "Epoch 12, Batch 202, loss_ca: 2.1110, adv_loss: 0.5485\n",
      "Epoch 12, Batch 203, loss_ca: 2.3018, adv_loss: 0.5413\n",
      "Epoch 12, Batch 204, loss_ca: 2.0294, adv_loss: 0.5533\n",
      "Epoch 12, Batch 205, loss_ca: 2.1962, adv_loss: 0.5408\n",
      "Epoch 12, Batch 206, loss_ca: 2.0740, adv_loss: 0.5815\n",
      "Epoch 12, Batch 207, loss_ca: 2.3567, adv_loss: 0.5515\n",
      "Epoch 12, Batch 208, loss_ca: 2.2033, adv_loss: 0.5336\n",
      "Epoch 12, Batch 209, loss_ca: 1.9982, adv_loss: 0.5439\n",
      "Epoch 12, Batch 210, loss_ca: 2.0792, adv_loss: 0.5444\n",
      "Epoch 12, Batch 211, loss_ca: 2.1350, adv_loss: 0.5342\n",
      "Epoch 12, Batch 212, loss_ca: 1.8707, adv_loss: 0.5405\n",
      "Epoch 12, Batch 213, loss_ca: 2.1737, adv_loss: 0.5589\n",
      "Epoch 12, Batch 214, loss_ca: 2.0674, adv_loss: 0.5470\n",
      "Epoch 12, Batch 215, loss_ca: 1.8441, adv_loss: 0.5309\n",
      "Epoch 12, Batch 216, loss_ca: 1.8435, adv_loss: 0.5276\n",
      "Epoch 12, Batch 217, loss_ca: 1.8533, adv_loss: 0.5504\n",
      "Epoch 12, Batch 218, loss_ca: 1.8089, adv_loss: 0.5645\n",
      "Epoch 12, Batch 219, loss_ca: 2.1336, adv_loss: 0.5077\n",
      "Epoch 12, Batch 220, loss_ca: 2.0140, adv_loss: 0.4819\n",
      "Epoch 12, Batch 221, loss_ca: 1.9497, adv_loss: 0.4993\n",
      "Epoch 12, Batch 222, loss_ca: 1.9272, adv_loss: 0.4744\n",
      "Epoch 12, Batch 223, loss_ca: 1.9616, adv_loss: 0.5521\n",
      "Epoch 12, Batch 224, loss_ca: 1.9548, adv_loss: 0.6265\n",
      "Epoch 12, Batch 225, loss_ca: 2.0881, adv_loss: 0.6240\n",
      "Epoch 12, Batch 226, loss_ca: 2.1856, adv_loss: 0.5937\n",
      "Epoch 12, Batch 227, loss_ca: 2.1214, adv_loss: 0.6175\n",
      "Epoch 12, Batch 228, loss_ca: 1.9680, adv_loss: 0.5916\n",
      "Epoch 13, Batch 13, loss_ca: 2.0370, adv_loss: 0.6112\n",
      "Epoch 13, Batch 14, loss_ca: 2.0225, adv_loss: 0.5642\n",
      "Epoch 13, Batch 15, loss_ca: 1.9586, adv_loss: 0.5464\n",
      "Epoch 13, Batch 16, loss_ca: 1.7317, adv_loss: 0.5378\n",
      "Epoch 13, Batch 17, loss_ca: 1.7242, adv_loss: 0.5399\n",
      "Epoch 13, Batch 18, loss_ca: 1.8787, adv_loss: 0.5096\n",
      "Epoch 13, Batch 19, loss_ca: 1.8901, adv_loss: 0.5189\n",
      "Epoch 13, Batch 20, loss_ca: 1.9467, adv_loss: 0.4798\n",
      "Epoch 13, Batch 21, loss_ca: 1.7521, adv_loss: 0.4897\n",
      "Epoch 13, Batch 22, loss_ca: 1.6945, adv_loss: 0.4638\n",
      "Epoch 13, Batch 23, loss_ca: 1.7604, adv_loss: 0.4496\n",
      "Epoch 13, Batch 24, loss_ca: 2.0778, adv_loss: 0.4695\n",
      "Epoch 13, Batch 25, loss_ca: 2.0581, adv_loss: 0.4870\n",
      "Epoch 13, Batch 26, loss_ca: 2.2729, adv_loss: 0.5038\n",
      "Epoch 13, Batch 27, loss_ca: 2.1249, adv_loss: 0.5034\n",
      "Epoch 13, Batch 28, loss_ca: 2.0283, adv_loss: 0.5426\n",
      "Epoch 13, Batch 29, loss_ca: 2.0198, adv_loss: 0.5443\n",
      "Epoch 13, Batch 30, loss_ca: 2.0811, adv_loss: 0.5595\n",
      "Epoch 13, Batch 31, loss_ca: 2.0343, adv_loss: 0.5460\n",
      "Epoch 13, Batch 32, loss_ca: 1.9605, adv_loss: 0.5548\n",
      "Epoch 13, Batch 33, loss_ca: 2.1029, adv_loss: 0.5641\n",
      "Epoch 13, Batch 34, loss_ca: 2.1017, adv_loss: 0.5882\n",
      "Epoch 13, Batch 35, loss_ca: 2.1091, adv_loss: 0.5900\n",
      "Epoch 13, Batch 36, loss_ca: 2.1084, adv_loss: 0.5989\n",
      "Epoch 13, Batch 37, loss_ca: 2.0701, adv_loss: 0.6201\n",
      "Epoch 13, Batch 38, loss_ca: 2.1136, adv_loss: 0.6315\n",
      "Epoch 13, Batch 39, loss_ca: 2.0969, adv_loss: 0.6273\n",
      "Epoch 13, Batch 40, loss_ca: 2.0250, adv_loss: 0.6123\n",
      "Epoch 13, Batch 41, loss_ca: 2.1270, adv_loss: 0.6060\n",
      "Epoch 13, Batch 42, loss_ca: 2.0190, adv_loss: 0.6020\n",
      "Epoch 13, Batch 43, loss_ca: 1.9157, adv_loss: 0.5696\n",
      "Epoch 13, Batch 44, loss_ca: 1.8571, adv_loss: 0.5607\n",
      "Epoch 13, Batch 45, loss_ca: 1.8236, adv_loss: 0.5473\n",
      "Epoch 13, Batch 46, loss_ca: 1.8013, adv_loss: 0.5460\n",
      "Epoch 13, Batch 47, loss_ca: 1.8387, adv_loss: 0.5561\n",
      "Epoch 13, Batch 48, loss_ca: 1.8943, adv_loss: 0.5589\n",
      "Epoch 13, Batch 49, loss_ca: 1.8915, adv_loss: 0.5547\n",
      "Epoch 13, Batch 50, loss_ca: 1.8318, adv_loss: 0.5544\n",
      "Epoch 13, Batch 51, loss_ca: 1.8288, adv_loss: 0.5565\n",
      "Epoch 13, Batch 52, loss_ca: 1.8527, adv_loss: 0.5275\n",
      "Epoch 13, Batch 53, loss_ca: 1.8899, adv_loss: 0.5383\n",
      "Epoch 13, Batch 54, loss_ca: 1.8523, adv_loss: 0.5500\n",
      "Epoch 13, Batch 55, loss_ca: 1.8057, adv_loss: 0.5625\n",
      "Epoch 13, Batch 56, loss_ca: 1.7552, adv_loss: 0.5559\n",
      "Epoch 13, Batch 57, loss_ca: 1.7470, adv_loss: 0.5404\n",
      "Epoch 13, Batch 58, loss_ca: 1.8849, adv_loss: 0.5142\n",
      "Epoch 13, Batch 59, loss_ca: 1.8619, adv_loss: 0.5181\n",
      "Epoch 13, Batch 60, loss_ca: 1.8401, adv_loss: 0.5188\n",
      "Epoch 13, Batch 61, loss_ca: 1.8676, adv_loss: 0.5323\n",
      "Epoch 13, Batch 62, loss_ca: 1.9187, adv_loss: 0.5258\n",
      "Epoch 13, Batch 63, loss_ca: 1.9534, adv_loss: 0.5356\n",
      "Epoch 13, Batch 64, loss_ca: 1.9890, adv_loss: 0.5556\n",
      "Epoch 13, Batch 65, loss_ca: 1.8786, adv_loss: 0.5547\n",
      "Epoch 13, Batch 66, loss_ca: 1.8802, adv_loss: 0.5796\n",
      "Epoch 13, Batch 67, loss_ca: 1.7607, adv_loss: 0.5502\n",
      "Epoch 13, Batch 68, loss_ca: 1.7911, adv_loss: 0.5802\n",
      "Epoch 13, Batch 69, loss_ca: 1.7853, adv_loss: 0.5643\n",
      "Epoch 13, Batch 70, loss_ca: 1.8130, adv_loss: 0.5909\n",
      "Epoch 13, Batch 71, loss_ca: 1.8063, adv_loss: 0.5717\n",
      "Epoch 13, Batch 72, loss_ca: 1.8612, adv_loss: 0.5651\n",
      "Epoch 13, Batch 73, loss_ca: 1.8200, adv_loss: 0.5616\n",
      "Epoch 13, Batch 74, loss_ca: 1.7427, adv_loss: 0.5526\n",
      "Epoch 13, Batch 75, loss_ca: 1.6877, adv_loss: 0.5123\n",
      "Epoch 13, Batch 76, loss_ca: 1.6330, adv_loss: 0.4868\n",
      "Epoch 13, Batch 77, loss_ca: 1.6350, adv_loss: 0.4718\n",
      "Epoch 13, Batch 78, loss_ca: 1.6690, adv_loss: 0.4642\n",
      "Epoch 13, Batch 79, loss_ca: 1.6998, adv_loss: 0.4684\n",
      "Epoch 13, Batch 80, loss_ca: 1.6784, adv_loss: 0.4780\n",
      "Epoch 13, Batch 81, loss_ca: 1.6779, adv_loss: 0.4827\n",
      "Epoch 13, Batch 82, loss_ca: 1.7080, adv_loss: 0.5000\n",
      "Epoch 13, Batch 83, loss_ca: 1.7539, adv_loss: 0.5052\n",
      "Epoch 13, Batch 84, loss_ca: 1.7660, adv_loss: 0.5103\n",
      "Epoch 13, Batch 85, loss_ca: 1.7660, adv_loss: 0.5061\n",
      "Epoch 13, Batch 86, loss_ca: 1.8128, adv_loss: 0.5009\n",
      "Epoch 13, Batch 87, loss_ca: 1.8339, adv_loss: 0.5200\n",
      "Epoch 13, Batch 88, loss_ca: 1.8109, adv_loss: 0.5228\n",
      "Epoch 13, Batch 89, loss_ca: 1.8738, adv_loss: 0.5250\n",
      "Epoch 13, Batch 90, loss_ca: 1.7804, adv_loss: 0.5288\n",
      "Epoch 13, Batch 91, loss_ca: 1.9026, adv_loss: 0.5240\n",
      "Epoch 13, Batch 92, loss_ca: 1.8597, adv_loss: 0.5257\n",
      "Epoch 13, Batch 93, loss_ca: 1.9419, adv_loss: 0.5316\n",
      "Epoch 13, Batch 94, loss_ca: 1.8217, adv_loss: 0.5294\n",
      "Epoch 13, Batch 95, loss_ca: 1.9436, adv_loss: 0.5152\n",
      "Epoch 13, Batch 96, loss_ca: 1.9808, adv_loss: 0.5337\n",
      "Epoch 13, Batch 97, loss_ca: 1.9424, adv_loss: 0.5288\n",
      "Epoch 13, Batch 98, loss_ca: 1.9058, adv_loss: 0.5220\n",
      "Epoch 13, Batch 99, loss_ca: 1.8328, adv_loss: 0.5157\n",
      "Epoch 13, Batch 100, loss_ca: 1.8634, adv_loss: 0.4873\n",
      "Epoch 13, Batch 101, loss_ca: 1.9560, adv_loss: 0.5019\n",
      "Epoch 13, Batch 102, loss_ca: 1.9518, adv_loss: 0.5106\n",
      "Epoch 13, Batch 103, loss_ca: 1.9340, adv_loss: 0.5140\n",
      "Epoch 13, Batch 104, loss_ca: 1.9349, adv_loss: 0.5307\n",
      "Epoch 13, Batch 105, loss_ca: 2.0389, adv_loss: 0.5309\n",
      "Epoch 13, Batch 106, loss_ca: 2.0300, adv_loss: 0.5361\n",
      "Epoch 13, Batch 107, loss_ca: 2.1534, adv_loss: 0.5452\n",
      "Epoch 13, Batch 108, loss_ca: 2.0980, adv_loss: 0.5648\n",
      "Epoch 13, Batch 109, loss_ca: 2.1081, adv_loss: 0.5869\n",
      "Epoch 13, Batch 110, loss_ca: 2.0121, adv_loss: 0.5804\n",
      "Epoch 13, Batch 111, loss_ca: 1.9534, adv_loss: 0.5813\n",
      "Epoch 13, Batch 112, loss_ca: 1.9362, adv_loss: 0.5753\n",
      "Epoch 13, Batch 113, loss_ca: 1.8535, adv_loss: 0.5592\n",
      "Epoch 13, Batch 114, loss_ca: 1.8442, adv_loss: 0.5554\n",
      "Epoch 13, Batch 115, loss_ca: 1.8104, adv_loss: 0.5532\n",
      "Epoch 13, Batch 116, loss_ca: 1.8206, adv_loss: 0.5402\n",
      "Epoch 13, Batch 117, loss_ca: 1.9985, adv_loss: 0.5711\n",
      "Epoch 13, Batch 118, loss_ca: 1.8824, adv_loss: 0.5646\n",
      "Epoch 13, Batch 119, loss_ca: 1.8027, adv_loss: 0.5549\n",
      "Epoch 13, Batch 120, loss_ca: 1.7887, adv_loss: 0.5338\n",
      "Epoch 13, Batch 121, loss_ca: 1.9368, adv_loss: 0.4890\n",
      "Epoch 13, Batch 122, loss_ca: 2.0872, adv_loss: 0.4817\n",
      "Epoch 13, Batch 123, loss_ca: 2.0282, adv_loss: 0.4768\n",
      "Epoch 13, Batch 124, loss_ca: 2.0120, adv_loss: 0.5091\n",
      "Epoch 13, Batch 125, loss_ca: 1.9752, adv_loss: 0.5199\n",
      "Epoch 13, Batch 126, loss_ca: 2.0330, adv_loss: 0.5332\n",
      "Epoch 13, Batch 127, loss_ca: 1.9984, adv_loss: 0.5372\n",
      "Epoch 13, Batch 128, loss_ca: 2.0176, adv_loss: 0.5505\n",
      "Epoch 13, Batch 129, loss_ca: 2.0910, adv_loss: 0.5489\n",
      "Epoch 13, Batch 130, loss_ca: 2.0477, adv_loss: 0.5557\n",
      "Epoch 13, Batch 131, loss_ca: 1.9329, adv_loss: 0.5514\n",
      "Epoch 13, Batch 132, loss_ca: 1.9954, adv_loss: 0.5814\n",
      "Epoch 13, Batch 133, loss_ca: 1.9496, adv_loss: 0.5660\n",
      "Epoch 13, Batch 134, loss_ca: 1.9362, adv_loss: 0.5705\n",
      "Epoch 13, Batch 135, loss_ca: 1.9060, adv_loss: 0.5738\n",
      "Epoch 13, Batch 136, loss_ca: 1.9281, adv_loss: 0.5526\n",
      "Epoch 13, Batch 137, loss_ca: 1.8186, adv_loss: 0.5366\n",
      "Epoch 13, Batch 138, loss_ca: 1.7216, adv_loss: 0.5154\n",
      "Epoch 13, Batch 139, loss_ca: 1.6927, adv_loss: 0.5127\n",
      "Epoch 13, Batch 140, loss_ca: 1.7296, adv_loss: 0.4924\n",
      "Epoch 13, Batch 141, loss_ca: 1.8127, adv_loss: 0.4949\n",
      "Epoch 13, Batch 142, loss_ca: 1.9704, adv_loss: 0.4929\n",
      "Epoch 13, Batch 143, loss_ca: 1.9702, adv_loss: 0.5001\n",
      "Epoch 13, Batch 144, loss_ca: 1.9272, adv_loss: 0.5103\n",
      "Epoch 13, Batch 145, loss_ca: 1.7778, adv_loss: 0.5111\n",
      "Epoch 13, Batch 146, loss_ca: 1.7301, adv_loss: 0.5243\n",
      "Epoch 13, Batch 147, loss_ca: 1.8414, adv_loss: 0.5202\n",
      "Epoch 13, Batch 148, loss_ca: 1.8471, adv_loss: 0.5255\n",
      "Epoch 13, Batch 149, loss_ca: 1.7848, adv_loss: 0.5313\n",
      "Epoch 13, Batch 150, loss_ca: 1.7475, adv_loss: 0.5630\n",
      "Epoch 13, Batch 151, loss_ca: 1.7165, adv_loss: 0.5710\n",
      "Epoch 13, Batch 152, loss_ca: 1.6674, adv_loss: 0.5658\n",
      "Epoch 13, Batch 153, loss_ca: 1.8412, adv_loss: 0.5295\n",
      "Epoch 13, Batch 154, loss_ca: 1.8464, adv_loss: 0.5290\n",
      "Epoch 13, Batch 155, loss_ca: 1.8875, adv_loss: 0.5493\n",
      "Epoch 13, Batch 156, loss_ca: 1.8916, adv_loss: 0.5367\n",
      "Epoch 13, Batch 157, loss_ca: 1.9663, adv_loss: 0.5324\n",
      "Epoch 13, Batch 158, loss_ca: 2.1911, adv_loss: 0.5022\n",
      "Epoch 13, Batch 159, loss_ca: 2.5798, adv_loss: 0.4993\n",
      "Epoch 13, Batch 160, loss_ca: 2.3955, adv_loss: 0.5096\n",
      "Epoch 13, Batch 161, loss_ca: 1.9384, adv_loss: 0.4621\n",
      "Epoch 13, Batch 162, loss_ca: 2.4506, adv_loss: 0.4981\n",
      "Epoch 13, Batch 163, loss_ca: 2.2841, adv_loss: 0.4734\n",
      "Epoch 13, Batch 164, loss_ca: 2.5083, adv_loss: 0.5163\n",
      "Epoch 13, Batch 165, loss_ca: 2.7036, adv_loss: 0.5488\n",
      "Epoch 13, Batch 166, loss_ca: 2.4027, adv_loss: 0.5686\n",
      "Epoch 13, Batch 167, loss_ca: 2.3646, adv_loss: 0.5667\n",
      "Epoch 13, Batch 168, loss_ca: 2.4796, adv_loss: 0.6303\n",
      "Epoch 13, Batch 169, loss_ca: 2.5253, adv_loss: 0.6406\n",
      "Epoch 13, Batch 170, loss_ca: 2.3084, adv_loss: 0.5953\n",
      "Epoch 13, Batch 171, loss_ca: 2.2761, adv_loss: 0.5845\n",
      "Epoch 13, Batch 172, loss_ca: 2.3266, adv_loss: 0.6008\n",
      "Epoch 13, Batch 173, loss_ca: 2.2548, adv_loss: 0.5942\n",
      "Epoch 13, Batch 174, loss_ca: 2.1815, adv_loss: 0.5258\n",
      "Epoch 13, Batch 175, loss_ca: 2.3096, adv_loss: 0.5380\n",
      "Epoch 13, Batch 176, loss_ca: 2.1140, adv_loss: 0.5316\n",
      "Epoch 13, Batch 177, loss_ca: 1.9586, adv_loss: 0.5299\n",
      "Epoch 13, Batch 178, loss_ca: 1.9875, adv_loss: 0.5416\n",
      "Epoch 13, Batch 179, loss_ca: 2.0156, adv_loss: 0.5596\n",
      "Epoch 13, Batch 180, loss_ca: 1.9174, adv_loss: 0.5586\n",
      "Epoch 13, Batch 181, loss_ca: 1.8307, adv_loss: 0.5719\n",
      "Epoch 13, Batch 182, loss_ca: 1.9749, adv_loss: 0.5842\n",
      "Epoch 13, Batch 183, loss_ca: 2.1130, adv_loss: 0.5995\n",
      "Epoch 13, Batch 184, loss_ca: 1.8375, adv_loss: 0.4921\n",
      "Epoch 13, Batch 185, loss_ca: 1.8326, adv_loss: 0.5020\n",
      "Epoch 13, Batch 186, loss_ca: 1.7873, adv_loss: 0.4988\n",
      "Epoch 13, Batch 187, loss_ca: 1.7785, adv_loss: 0.5168\n",
      "Epoch 13, Batch 188, loss_ca: 1.7473, adv_loss: 0.5039\n",
      "Epoch 13, Batch 189, loss_ca: 1.7951, adv_loss: 0.4968\n",
      "Epoch 13, Batch 190, loss_ca: 2.0713, adv_loss: 0.4978\n",
      "Epoch 13, Batch 191, loss_ca: 2.0306, adv_loss: 0.5005\n",
      "Epoch 13, Batch 192, loss_ca: 2.1679, adv_loss: 0.5374\n",
      "Epoch 13, Batch 193, loss_ca: 2.1420, adv_loss: 0.5812\n",
      "Epoch 13, Batch 194, loss_ca: 2.1331, adv_loss: 0.5885\n",
      "Epoch 13, Batch 195, loss_ca: 2.2533, adv_loss: 0.6460\n",
      "Epoch 13, Batch 196, loss_ca: 2.5703, adv_loss: 0.6573\n",
      "Epoch 13, Batch 197, loss_ca: 2.1637, adv_loss: 0.5699\n",
      "Epoch 13, Batch 198, loss_ca: 2.3704, adv_loss: 0.5437\n",
      "Epoch 13, Batch 199, loss_ca: 2.1684, adv_loss: 0.4931\n",
      "Epoch 13, Batch 200, loss_ca: 1.9966, adv_loss: 0.4681\n",
      "Epoch 13, Batch 201, loss_ca: 1.9765, adv_loss: 0.5101\n",
      "Epoch 13, Batch 202, loss_ca: 2.0476, adv_loss: 0.5367\n",
      "Epoch 13, Batch 203, loss_ca: 2.2776, adv_loss: 0.5453\n",
      "Epoch 13, Batch 204, loss_ca: 2.1269, adv_loss: 0.5336\n",
      "Epoch 13, Batch 205, loss_ca: 2.2428, adv_loss: 0.5494\n",
      "Epoch 13, Batch 206, loss_ca: 2.0713, adv_loss: 0.4892\n",
      "Epoch 13, Batch 207, loss_ca: 2.2236, adv_loss: 0.5483\n",
      "Epoch 13, Batch 208, loss_ca: 2.1228, adv_loss: 0.5319\n",
      "Epoch 13, Batch 209, loss_ca: 1.8788, adv_loss: 0.4855\n",
      "Epoch 13, Batch 210, loss_ca: 1.9993, adv_loss: 0.5546\n",
      "Epoch 13, Batch 211, loss_ca: 2.1854, adv_loss: 0.6042\n",
      "Epoch 13, Batch 212, loss_ca: 1.9233, adv_loss: 0.6482\n",
      "Epoch 13, Batch 213, loss_ca: 2.3520, adv_loss: 0.6210\n",
      "Epoch 13, Batch 214, loss_ca: 2.0472, adv_loss: 0.5896\n",
      "Epoch 13, Batch 215, loss_ca: 1.9762, adv_loss: 0.6070\n",
      "Epoch 13, Batch 216, loss_ca: 2.0616, adv_loss: 0.6120\n",
      "Epoch 13, Batch 217, loss_ca: 1.9802, adv_loss: 0.5860\n",
      "Epoch 13, Batch 218, loss_ca: 1.8020, adv_loss: 0.5896\n",
      "Epoch 13, Batch 219, loss_ca: 2.1503, adv_loss: 0.5455\n",
      "Epoch 13, Batch 220, loss_ca: 2.0009, adv_loss: 0.5613\n",
      "Epoch 13, Batch 221, loss_ca: 1.9445, adv_loss: 0.5213\n",
      "Epoch 13, Batch 222, loss_ca: 1.8989, adv_loss: 0.5453\n",
      "Epoch 13, Batch 223, loss_ca: 1.8673, adv_loss: 0.5129\n",
      "Epoch 13, Batch 224, loss_ca: 1.8659, adv_loss: 0.5360\n",
      "Epoch 13, Batch 225, loss_ca: 1.8712, adv_loss: 0.5312\n",
      "Epoch 13, Batch 226, loss_ca: 1.8837, adv_loss: 0.5008\n",
      "Epoch 13, Batch 227, loss_ca: 2.0176, adv_loss: 0.5180\n",
      "Epoch 13, Batch 228, loss_ca: 1.9806, adv_loss: 0.4898\n",
      "Epoch 14, Batch 14, loss_ca: 2.0393, adv_loss: 0.5312\n",
      "Epoch 14, Batch 15, loss_ca: 2.0299, adv_loss: 0.5162\n",
      "Epoch 14, Batch 16, loss_ca: 1.8110, adv_loss: 0.5072\n",
      "Epoch 14, Batch 17, loss_ca: 1.8018, adv_loss: 0.5137\n",
      "Epoch 14, Batch 18, loss_ca: 1.8360, adv_loss: 0.5413\n",
      "Epoch 14, Batch 19, loss_ca: 1.8828, adv_loss: 0.5571\n",
      "Epoch 14, Batch 20, loss_ca: 1.9222, adv_loss: 0.5425\n",
      "Epoch 14, Batch 21, loss_ca: 1.8950, adv_loss: 0.5479\n",
      "Epoch 14, Batch 22, loss_ca: 1.8004, adv_loss: 0.5300\n",
      "Epoch 14, Batch 23, loss_ca: 1.8121, adv_loss: 0.5183\n",
      "Epoch 14, Batch 24, loss_ca: 2.0676, adv_loss: 0.5108\n",
      "Epoch 14, Batch 25, loss_ca: 2.0836, adv_loss: 0.5224\n",
      "Epoch 14, Batch 26, loss_ca: 2.1960, adv_loss: 0.5066\n",
      "Epoch 14, Batch 27, loss_ca: 2.0687, adv_loss: 0.5021\n",
      "Epoch 14, Batch 28, loss_ca: 1.9681, adv_loss: 0.5159\n",
      "Epoch 14, Batch 29, loss_ca: 1.9649, adv_loss: 0.5192\n",
      "Epoch 14, Batch 30, loss_ca: 2.0535, adv_loss: 0.5169\n",
      "Epoch 14, Batch 31, loss_ca: 2.0831, adv_loss: 0.5222\n",
      "Epoch 14, Batch 32, loss_ca: 1.9581, adv_loss: 0.5277\n",
      "Epoch 14, Batch 33, loss_ca: 2.1127, adv_loss: 0.5438\n",
      "Epoch 14, Batch 34, loss_ca: 2.0861, adv_loss: 0.5614\n",
      "Epoch 14, Batch 35, loss_ca: 2.0746, adv_loss: 0.5704\n",
      "Epoch 14, Batch 36, loss_ca: 2.0836, adv_loss: 0.5983\n",
      "Epoch 14, Batch 37, loss_ca: 2.0369, adv_loss: 0.6040\n",
      "Epoch 14, Batch 38, loss_ca: 2.0441, adv_loss: 0.6064\n",
      "Epoch 14, Batch 39, loss_ca: 2.0561, adv_loss: 0.6084\n",
      "Epoch 14, Batch 40, loss_ca: 2.0518, adv_loss: 0.6111\n",
      "Epoch 14, Batch 41, loss_ca: 2.2383, adv_loss: 0.6239\n",
      "Epoch 14, Batch 42, loss_ca: 2.1984, adv_loss: 0.6131\n",
      "Epoch 14, Batch 43, loss_ca: 2.1078, adv_loss: 0.6088\n",
      "Epoch 14, Batch 44, loss_ca: 2.0187, adv_loss: 0.5863\n",
      "Epoch 14, Batch 45, loss_ca: 1.9261, adv_loss: 0.5800\n",
      "Epoch 14, Batch 46, loss_ca: 1.8573, adv_loss: 0.5692\n",
      "Epoch 14, Batch 47, loss_ca: 1.8717, adv_loss: 0.5716\n",
      "Epoch 14, Batch 48, loss_ca: 1.8452, adv_loss: 0.5865\n",
      "Epoch 14, Batch 49, loss_ca: 1.8536, adv_loss: 0.5754\n",
      "Epoch 14, Batch 50, loss_ca: 1.8968, adv_loss: 0.5658\n",
      "Epoch 14, Batch 51, loss_ca: 1.9407, adv_loss: 0.5691\n",
      "Epoch 14, Batch 52, loss_ca: 2.0860, adv_loss: 0.5607\n",
      "Epoch 14, Batch 53, loss_ca: 1.8905, adv_loss: 0.5689\n",
      "Epoch 14, Batch 54, loss_ca: 1.8119, adv_loss: 0.5517\n",
      "Epoch 14, Batch 55, loss_ca: 1.7631, adv_loss: 0.5457\n",
      "Epoch 14, Batch 56, loss_ca: 1.7329, adv_loss: 0.5343\n",
      "Epoch 14, Batch 57, loss_ca: 1.7193, adv_loss: 0.5307\n",
      "Epoch 14, Batch 58, loss_ca: 1.8797, adv_loss: 0.5426\n",
      "Epoch 14, Batch 59, loss_ca: 1.8757, adv_loss: 0.5159\n",
      "Epoch 14, Batch 60, loss_ca: 1.8900, adv_loss: 0.5188\n",
      "Epoch 14, Batch 61, loss_ca: 1.9025, adv_loss: 0.5439\n",
      "Epoch 14, Batch 62, loss_ca: 1.9522, adv_loss: 0.5452\n",
      "Epoch 14, Batch 63, loss_ca: 1.9924, adv_loss: 0.5527\n",
      "Epoch 14, Batch 64, loss_ca: 1.9090, adv_loss: 0.5252\n",
      "Epoch 14, Batch 65, loss_ca: 1.8736, adv_loss: 0.5342\n",
      "Epoch 14, Batch 66, loss_ca: 1.7638, adv_loss: 0.5767\n",
      "Epoch 14, Batch 67, loss_ca: 1.9122, adv_loss: 0.5280\n",
      "Epoch 14, Batch 68, loss_ca: 1.9060, adv_loss: 0.5571\n",
      "Epoch 14, Batch 69, loss_ca: 1.9281, adv_loss: 0.5679\n",
      "Epoch 14, Batch 70, loss_ca: 1.8272, adv_loss: 0.5839\n",
      "Epoch 14, Batch 71, loss_ca: 1.7721, adv_loss: 0.5620\n",
      "Epoch 14, Batch 72, loss_ca: 1.7419, adv_loss: 0.5023\n",
      "Epoch 14, Batch 73, loss_ca: 1.7102, adv_loss: 0.5183\n",
      "Epoch 14, Batch 74, loss_ca: 1.7301, adv_loss: 0.5283\n",
      "Epoch 14, Batch 75, loss_ca: 1.7072, adv_loss: 0.5262\n",
      "Epoch 14, Batch 76, loss_ca: 1.6729, adv_loss: 0.5153\n",
      "Epoch 14, Batch 77, loss_ca: 1.6706, adv_loss: 0.5098\n",
      "Epoch 14, Batch 78, loss_ca: 1.6849, adv_loss: 0.4902\n",
      "Epoch 14, Batch 79, loss_ca: 1.6629, adv_loss: 0.4899\n",
      "Epoch 14, Batch 80, loss_ca: 1.6341, adv_loss: 0.4786\n",
      "Epoch 14, Batch 81, loss_ca: 1.6356, adv_loss: 0.4810\n",
      "Epoch 14, Batch 82, loss_ca: 1.7083, adv_loss: 0.4801\n",
      "Epoch 14, Batch 83, loss_ca: 1.6997, adv_loss: 0.4820\n",
      "Epoch 14, Batch 84, loss_ca: 1.7239, adv_loss: 0.4869\n",
      "Epoch 14, Batch 85, loss_ca: 1.7379, adv_loss: 0.4776\n",
      "Epoch 14, Batch 86, loss_ca: 1.7991, adv_loss: 0.4828\n",
      "Epoch 14, Batch 87, loss_ca: 1.8143, adv_loss: 0.5025\n",
      "Epoch 14, Batch 88, loss_ca: 1.7983, adv_loss: 0.5087\n",
      "Epoch 14, Batch 89, loss_ca: 1.8514, adv_loss: 0.5119\n",
      "Epoch 14, Batch 90, loss_ca: 1.7550, adv_loss: 0.5137\n",
      "Epoch 14, Batch 91, loss_ca: 1.9028, adv_loss: 0.5093\n",
      "Epoch 14, Batch 92, loss_ca: 1.8717, adv_loss: 0.5057\n",
      "Epoch 14, Batch 93, loss_ca: 1.9535, adv_loss: 0.5060\n",
      "Epoch 14, Batch 94, loss_ca: 1.8493, adv_loss: 0.5123\n",
      "Epoch 14, Batch 95, loss_ca: 1.9456, adv_loss: 0.5109\n",
      "Epoch 14, Batch 96, loss_ca: 1.9796, adv_loss: 0.5118\n",
      "Epoch 14, Batch 97, loss_ca: 1.9358, adv_loss: 0.5136\n",
      "Epoch 14, Batch 98, loss_ca: 1.9102, adv_loss: 0.5085\n",
      "Epoch 14, Batch 99, loss_ca: 1.8595, adv_loss: 0.5236\n",
      "Epoch 14, Batch 100, loss_ca: 1.8848, adv_loss: 0.4954\n",
      "Epoch 14, Batch 101, loss_ca: 1.9664, adv_loss: 0.5274\n",
      "Epoch 14, Batch 102, loss_ca: 1.9427, adv_loss: 0.5273\n",
      "Epoch 14, Batch 103, loss_ca: 1.9211, adv_loss: 0.5463\n",
      "Epoch 14, Batch 104, loss_ca: 1.9736, adv_loss: 0.5588\n",
      "Epoch 14, Batch 105, loss_ca: 2.0656, adv_loss: 0.5512\n",
      "Epoch 14, Batch 106, loss_ca: 2.0216, adv_loss: 0.5612\n",
      "Epoch 14, Batch 107, loss_ca: 2.1271, adv_loss: 0.5762\n",
      "Epoch 14, Batch 108, loss_ca: 2.0225, adv_loss: 0.5725\n",
      "Epoch 14, Batch 109, loss_ca: 2.0550, adv_loss: 0.5931\n",
      "Epoch 14, Batch 110, loss_ca: 2.0139, adv_loss: 0.5939\n",
      "Epoch 14, Batch 111, loss_ca: 1.9727, adv_loss: 0.5920\n",
      "Epoch 14, Batch 112, loss_ca: 1.9612, adv_loss: 0.5853\n",
      "Epoch 14, Batch 113, loss_ca: 1.8654, adv_loss: 0.5728\n",
      "Epoch 14, Batch 114, loss_ca: 1.8922, adv_loss: 0.5743\n",
      "Epoch 14, Batch 115, loss_ca: 1.8244, adv_loss: 0.5869\n",
      "Epoch 14, Batch 116, loss_ca: 1.8664, adv_loss: 0.5876\n",
      "Epoch 14, Batch 117, loss_ca: 1.9591, adv_loss: 0.5961\n",
      "Epoch 14, Batch 118, loss_ca: 1.9673, adv_loss: 0.5953\n",
      "Epoch 14, Batch 119, loss_ca: 1.8705, adv_loss: 0.5751\n",
      "Epoch 14, Batch 120, loss_ca: 1.7724, adv_loss: 0.5530\n",
      "Epoch 14, Batch 121, loss_ca: 1.9199, adv_loss: 0.5144\n",
      "Epoch 14, Batch 122, loss_ca: 1.9669, adv_loss: 0.4997\n",
      "Epoch 14, Batch 123, loss_ca: 1.9316, adv_loss: 0.4913\n",
      "Epoch 14, Batch 124, loss_ca: 1.9013, adv_loss: 0.4876\n",
      "Epoch 14, Batch 125, loss_ca: 1.9310, adv_loss: 0.5164\n",
      "Epoch 14, Batch 126, loss_ca: 1.9581, adv_loss: 0.5281\n",
      "Epoch 14, Batch 127, loss_ca: 1.9378, adv_loss: 0.4849\n",
      "Epoch 14, Batch 128, loss_ca: 1.9516, adv_loss: 0.4895\n",
      "Epoch 14, Batch 129, loss_ca: 1.9480, adv_loss: 0.5085\n",
      "Epoch 14, Batch 130, loss_ca: 1.8787, adv_loss: 0.5211\n",
      "Epoch 14, Batch 131, loss_ca: 1.8559, adv_loss: 0.5368\n",
      "Epoch 14, Batch 132, loss_ca: 1.9295, adv_loss: 0.5873\n",
      "Epoch 14, Batch 133, loss_ca: 1.9939, adv_loss: 0.5963\n",
      "Epoch 14, Batch 134, loss_ca: 1.9818, adv_loss: 0.6236\n",
      "Epoch 14, Batch 135, loss_ca: 1.9469, adv_loss: 0.6346\n",
      "Epoch 14, Batch 136, loss_ca: 1.9650, adv_loss: 0.6348\n",
      "Epoch 14, Batch 137, loss_ca: 1.8768, adv_loss: 0.6249\n",
      "Epoch 14, Batch 138, loss_ca: 1.8277, adv_loss: 0.6093\n",
      "Epoch 14, Batch 139, loss_ca: 1.8240, adv_loss: 0.6049\n",
      "Epoch 14, Batch 140, loss_ca: 1.8119, adv_loss: 0.5773\n",
      "Epoch 14, Batch 141, loss_ca: 1.8350, adv_loss: 0.5690\n",
      "Epoch 14, Batch 142, loss_ca: 1.9303, adv_loss: 0.5737\n",
      "Epoch 14, Batch 143, loss_ca: 1.8394, adv_loss: 0.5523\n",
      "Epoch 14, Batch 144, loss_ca: 1.7920, adv_loss: 0.5229\n",
      "Epoch 14, Batch 145, loss_ca: 1.7497, adv_loss: 0.5046\n",
      "Epoch 14, Batch 146, loss_ca: 1.7035, adv_loss: 0.4751\n",
      "Epoch 14, Batch 147, loss_ca: 1.8052, adv_loss: 0.4702\n",
      "Epoch 14, Batch 148, loss_ca: 1.8496, adv_loss: 0.4712\n",
      "Epoch 14, Batch 149, loss_ca: 1.7720, adv_loss: 0.4854\n",
      "Epoch 14, Batch 150, loss_ca: 1.6755, adv_loss: 0.4714\n",
      "Epoch 14, Batch 151, loss_ca: 1.6901, adv_loss: 0.4833\n",
      "Epoch 14, Batch 152, loss_ca: 1.6487, adv_loss: 0.4759\n",
      "Epoch 14, Batch 153, loss_ca: 1.8620, adv_loss: 0.4758\n",
      "Epoch 14, Batch 154, loss_ca: 1.8593, adv_loss: 0.4651\n",
      "Epoch 14, Batch 155, loss_ca: 1.8514, adv_loss: 0.5108\n",
      "Epoch 14, Batch 156, loss_ca: 1.8534, adv_loss: 0.5087\n",
      "Epoch 14, Batch 157, loss_ca: 1.8683, adv_loss: 0.5121\n",
      "Epoch 14, Batch 158, loss_ca: 2.1211, adv_loss: 0.5531\n",
      "Epoch 14, Batch 159, loss_ca: 2.3986, adv_loss: 0.5492\n",
      "Epoch 14, Batch 160, loss_ca: 2.2805, adv_loss: 0.5592\n",
      "Epoch 14, Batch 161, loss_ca: 2.1938, adv_loss: 0.5045\n",
      "Epoch 14, Batch 162, loss_ca: 2.4553, adv_loss: 0.5228\n",
      "Epoch 14, Batch 163, loss_ca: 2.3580, adv_loss: 0.4918\n",
      "Epoch 14, Batch 164, loss_ca: 2.4789, adv_loss: 0.5295\n",
      "Epoch 14, Batch 165, loss_ca: 2.6185, adv_loss: 0.6039\n",
      "Epoch 14, Batch 166, loss_ca: 2.2681, adv_loss: 0.5772\n",
      "Epoch 14, Batch 167, loss_ca: 2.1737, adv_loss: 0.5116\n",
      "Epoch 14, Batch 168, loss_ca: 2.2498, adv_loss: 0.6330\n",
      "Epoch 14, Batch 169, loss_ca: 2.3545, adv_loss: 0.6764\n",
      "Epoch 14, Batch 170, loss_ca: 2.4644, adv_loss: 0.6292\n",
      "Epoch 14, Batch 171, loss_ca: 2.4015, adv_loss: 0.6523\n",
      "Epoch 14, Batch 172, loss_ca: 2.3423, adv_loss: 0.6705\n",
      "Epoch 14, Batch 173, loss_ca: 2.2222, adv_loss: 0.5910\n",
      "Epoch 14, Batch 174, loss_ca: 2.1593, adv_loss: 0.5307\n",
      "Epoch 14, Batch 175, loss_ca: 2.1473, adv_loss: 0.5326\n",
      "Epoch 14, Batch 176, loss_ca: 1.9813, adv_loss: 0.5004\n",
      "Epoch 14, Batch 177, loss_ca: 1.8327, adv_loss: 0.5146\n",
      "Epoch 14, Batch 178, loss_ca: 1.9725, adv_loss: 0.5066\n",
      "Epoch 14, Batch 179, loss_ca: 1.9044, adv_loss: 0.5218\n",
      "Epoch 14, Batch 180, loss_ca: 1.8419, adv_loss: 0.5198\n",
      "Epoch 14, Batch 181, loss_ca: 1.8235, adv_loss: 0.5646\n",
      "Epoch 14, Batch 182, loss_ca: 1.7607, adv_loss: 0.5409\n",
      "Epoch 14, Batch 183, loss_ca: 1.9146, adv_loss: 0.5825\n",
      "Epoch 14, Batch 184, loss_ca: 1.7872, adv_loss: 0.5503\n",
      "Epoch 14, Batch 185, loss_ca: 1.8449, adv_loss: 0.5592\n",
      "Epoch 14, Batch 186, loss_ca: 1.9180, adv_loss: 0.5147\n",
      "Epoch 14, Batch 187, loss_ca: 1.8947, adv_loss: 0.5304\n",
      "Epoch 14, Batch 188, loss_ca: 1.7938, adv_loss: 0.5179\n",
      "Epoch 14, Batch 189, loss_ca: 1.8106, adv_loss: 0.5050\n",
      "Epoch 14, Batch 190, loss_ca: 2.0465, adv_loss: 0.4924\n",
      "Epoch 14, Batch 191, loss_ca: 2.0907, adv_loss: 0.5036\n",
      "Epoch 14, Batch 192, loss_ca: 2.1581, adv_loss: 0.5336\n",
      "Epoch 14, Batch 193, loss_ca: 2.1230, adv_loss: 0.5124\n",
      "Epoch 14, Batch 194, loss_ca: 1.9506, adv_loss: 0.5196\n",
      "Epoch 14, Batch 195, loss_ca: 1.9834, adv_loss: 0.5787\n",
      "Epoch 14, Batch 196, loss_ca: 2.4645, adv_loss: 0.6226\n",
      "Epoch 14, Batch 197, loss_ca: 2.1962, adv_loss: 0.5675\n",
      "Epoch 14, Batch 198, loss_ca: 2.7377, adv_loss: 0.5844\n",
      "Epoch 14, Batch 199, loss_ca: 2.6773, adv_loss: 0.5707\n",
      "Epoch 14, Batch 200, loss_ca: 2.5699, adv_loss: 0.5774\n",
      "Epoch 14, Batch 201, loss_ca: 2.6292, adv_loss: 0.6089\n",
      "Epoch 14, Batch 202, loss_ca: 2.4690, adv_loss: 0.5988\n",
      "Epoch 14, Batch 203, loss_ca: 2.8024, adv_loss: 0.6680\n",
      "Epoch 14, Batch 204, loss_ca: 1.8892, adv_loss: 0.4417\n",
      "Epoch 14, Batch 205, loss_ca: 2.1934, adv_loss: 0.4895\n",
      "Epoch 14, Batch 206, loss_ca: 1.9073, adv_loss: 0.5598\n",
      "Epoch 14, Batch 207, loss_ca: 2.1429, adv_loss: 0.5975\n",
      "Epoch 14, Batch 208, loss_ca: 2.1914, adv_loss: 0.6088\n",
      "Epoch 14, Batch 209, loss_ca: 1.9385, adv_loss: 0.6282\n",
      "Epoch 14, Batch 210, loss_ca: 2.0808, adv_loss: 0.5932\n",
      "Epoch 14, Batch 211, loss_ca: 2.1220, adv_loss: 0.5829\n",
      "Epoch 14, Batch 212, loss_ca: 1.7938, adv_loss: 0.6137\n",
      "Epoch 14, Batch 213, loss_ca: 2.2980, adv_loss: 0.6136\n",
      "Epoch 14, Batch 214, loss_ca: 1.9363, adv_loss: 0.6179\n",
      "Epoch 14, Batch 215, loss_ca: 1.8472, adv_loss: 0.6180\n",
      "Epoch 14, Batch 216, loss_ca: 1.9454, adv_loss: 0.6209\n",
      "Epoch 14, Batch 217, loss_ca: 1.9720, adv_loss: 0.6392\n",
      "Epoch 14, Batch 218, loss_ca: 1.9775, adv_loss: 0.6290\n",
      "Epoch 14, Batch 219, loss_ca: 2.3561, adv_loss: 0.6075\n",
      "Epoch 14, Batch 220, loss_ca: 2.1201, adv_loss: 0.5662\n",
      "Epoch 14, Batch 221, loss_ca: 1.9797, adv_loss: 0.5231\n",
      "Epoch 14, Batch 222, loss_ca: 1.8766, adv_loss: 0.5015\n",
      "Epoch 14, Batch 223, loss_ca: 1.7906, adv_loss: 0.4352\n",
      "Epoch 14, Batch 224, loss_ca: 1.8418, adv_loss: 0.4811\n",
      "Epoch 14, Batch 225, loss_ca: 1.8459, adv_loss: 0.4710\n",
      "Epoch 14, Batch 226, loss_ca: 1.8531, adv_loss: 0.4787\n",
      "Epoch 14, Batch 227, loss_ca: 1.9794, adv_loss: 0.5440\n",
      "Epoch 14, Batch 228, loss_ca: 1.8514, adv_loss: 0.5270\n",
      "Epoch 15, Batch 15, loss_ca: 2.1650, adv_loss: 0.5742\n",
      "Epoch 15, Batch 16, loss_ca: 1.9942, adv_loss: 0.5457\n",
      "Epoch 15, Batch 17, loss_ca: 1.8435, adv_loss: 0.5212\n",
      "Epoch 15, Batch 18, loss_ca: 1.8842, adv_loss: 0.5394\n",
      "Epoch 15, Batch 19, loss_ca: 1.9119, adv_loss: 0.5661\n",
      "Epoch 15, Batch 20, loss_ca: 1.8811, adv_loss: 0.5205\n",
      "Epoch 15, Batch 21, loss_ca: 1.9161, adv_loss: 0.4754\n",
      "Epoch 15, Batch 22, loss_ca: 1.8008, adv_loss: 0.4369\n",
      "Epoch 15, Batch 23, loss_ca: 1.9113, adv_loss: 0.4407\n",
      "Epoch 15, Batch 24, loss_ca: 2.0555, adv_loss: 0.4386\n",
      "Epoch 15, Batch 25, loss_ca: 2.1294, adv_loss: 0.4848\n",
      "Epoch 15, Batch 26, loss_ca: 2.1761, adv_loss: 0.5076\n",
      "Epoch 15, Batch 27, loss_ca: 2.0218, adv_loss: 0.5240\n",
      "Epoch 15, Batch 28, loss_ca: 1.9354, adv_loss: 0.5409\n",
      "Epoch 15, Batch 29, loss_ca: 1.9526, adv_loss: 0.5379\n",
      "Epoch 15, Batch 30, loss_ca: 2.0882, adv_loss: 0.5467\n",
      "Epoch 15, Batch 31, loss_ca: 2.0193, adv_loss: 0.5522\n",
      "Epoch 15, Batch 32, loss_ca: 1.9287, adv_loss: 0.5676\n",
      "Epoch 15, Batch 33, loss_ca: 2.0579, adv_loss: 0.5813\n",
      "Epoch 15, Batch 34, loss_ca: 2.0324, adv_loss: 0.5881\n",
      "Epoch 15, Batch 35, loss_ca: 2.0509, adv_loss: 0.5917\n",
      "Epoch 15, Batch 36, loss_ca: 2.0509, adv_loss: 0.6003\n",
      "Epoch 15, Batch 37, loss_ca: 2.0190, adv_loss: 0.6028\n",
      "Epoch 15, Batch 38, loss_ca: 2.0625, adv_loss: 0.6084\n",
      "Epoch 15, Batch 39, loss_ca: 2.0359, adv_loss: 0.6063\n",
      "Epoch 15, Batch 40, loss_ca: 2.0244, adv_loss: 0.6070\n",
      "Epoch 15, Batch 41, loss_ca: 2.1133, adv_loss: 0.5902\n",
      "Epoch 15, Batch 42, loss_ca: 2.0741, adv_loss: 0.5922\n",
      "Epoch 15, Batch 43, loss_ca: 1.9818, adv_loss: 0.5784\n",
      "Epoch 15, Batch 44, loss_ca: 1.9311, adv_loss: 0.5738\n",
      "Epoch 15, Batch 45, loss_ca: 1.8858, adv_loss: 0.5612\n",
      "Epoch 15, Batch 46, loss_ca: 1.8594, adv_loss: 0.5629\n",
      "Epoch 15, Batch 47, loss_ca: 1.8287, adv_loss: 0.5625\n",
      "Epoch 15, Batch 48, loss_ca: 1.8663, adv_loss: 0.5487\n",
      "Epoch 15, Batch 49, loss_ca: 1.8708, adv_loss: 0.5463\n",
      "Epoch 15, Batch 50, loss_ca: 1.8723, adv_loss: 0.5428\n",
      "Epoch 15, Batch 51, loss_ca: 1.8584, adv_loss: 0.5555\n",
      "Epoch 15, Batch 52, loss_ca: 1.9439, adv_loss: 0.5415\n",
      "Epoch 15, Batch 53, loss_ca: 1.8479, adv_loss: 0.5469\n",
      "Epoch 15, Batch 54, loss_ca: 1.7741, adv_loss: 0.5355\n",
      "Epoch 15, Batch 55, loss_ca: 1.7540, adv_loss: 0.5321\n",
      "Epoch 15, Batch 56, loss_ca: 1.7322, adv_loss: 0.5189\n",
      "Epoch 15, Batch 57, loss_ca: 1.7504, adv_loss: 0.5044\n",
      "Epoch 15, Batch 58, loss_ca: 1.9345, adv_loss: 0.5260\n",
      "Epoch 15, Batch 59, loss_ca: 1.9830, adv_loss: 0.5272\n",
      "Epoch 15, Batch 60, loss_ca: 1.9892, adv_loss: 0.5367\n",
      "Epoch 15, Batch 61, loss_ca: 1.9317, adv_loss: 0.5263\n",
      "Epoch 15, Batch 62, loss_ca: 1.9626, adv_loss: 0.5380\n",
      "Epoch 15, Batch 63, loss_ca: 1.9479, adv_loss: 0.5593\n",
      "Epoch 15, Batch 64, loss_ca: 1.9501, adv_loss: 0.5550\n",
      "Epoch 15, Batch 65, loss_ca: 1.9134, adv_loss: 0.5523\n",
      "Epoch 15, Batch 66, loss_ca: 1.8572, adv_loss: 0.5823\n",
      "Epoch 15, Batch 67, loss_ca: 1.8336, adv_loss: 0.5470\n",
      "Epoch 15, Batch 68, loss_ca: 1.8137, adv_loss: 0.5418\n",
      "Epoch 15, Batch 69, loss_ca: 1.8195, adv_loss: 0.5367\n",
      "Epoch 15, Batch 70, loss_ca: 1.8016, adv_loss: 0.5351\n",
      "Epoch 15, Batch 71, loss_ca: 1.7601, adv_loss: 0.5079\n",
      "Epoch 15, Batch 72, loss_ca: 1.7591, adv_loss: 0.4739\n",
      "Epoch 15, Batch 73, loss_ca: 1.7111, adv_loss: 0.4782\n",
      "Epoch 15, Batch 74, loss_ca: 1.7164, adv_loss: 0.4899\n",
      "Epoch 15, Batch 75, loss_ca: 1.6822, adv_loss: 0.5026\n",
      "Epoch 15, Batch 76, loss_ca: 1.6373, adv_loss: 0.5115\n",
      "Epoch 15, Batch 77, loss_ca: 1.6716, adv_loss: 0.5255\n",
      "Epoch 15, Batch 78, loss_ca: 1.7048, adv_loss: 0.5205\n",
      "Epoch 15, Batch 79, loss_ca: 1.6898, adv_loss: 0.5207\n",
      "Epoch 15, Batch 80, loss_ca: 1.6584, adv_loss: 0.5165\n",
      "Epoch 15, Batch 81, loss_ca: 1.6582, adv_loss: 0.5053\n",
      "Epoch 15, Batch 82, loss_ca: 1.6845, adv_loss: 0.4905\n",
      "Epoch 15, Batch 83, loss_ca: 1.7269, adv_loss: 0.5011\n",
      "Epoch 15, Batch 84, loss_ca: 1.7402, adv_loss: 0.4927\n",
      "Epoch 15, Batch 85, loss_ca: 1.7153, adv_loss: 0.4694\n",
      "Epoch 15, Batch 86, loss_ca: 1.7831, adv_loss: 0.4710\n",
      "Epoch 15, Batch 87, loss_ca: 1.8031, adv_loss: 0.4659\n",
      "Epoch 15, Batch 88, loss_ca: 1.7628, adv_loss: 0.4761\n",
      "Epoch 15, Batch 89, loss_ca: 1.8267, adv_loss: 0.4838\n",
      "Epoch 15, Batch 90, loss_ca: 1.7563, adv_loss: 0.4857\n",
      "Epoch 15, Batch 91, loss_ca: 1.8907, adv_loss: 0.4899\n",
      "Epoch 15, Batch 92, loss_ca: 1.8448, adv_loss: 0.4820\n",
      "Epoch 15, Batch 93, loss_ca: 1.9637, adv_loss: 0.4952\n",
      "Epoch 15, Batch 94, loss_ca: 1.8731, adv_loss: 0.4973\n",
      "Epoch 15, Batch 95, loss_ca: 1.9261, adv_loss: 0.5007\n",
      "Epoch 15, Batch 96, loss_ca: 1.9932, adv_loss: 0.5235\n",
      "Epoch 15, Batch 97, loss_ca: 1.9441, adv_loss: 0.5274\n",
      "Epoch 15, Batch 98, loss_ca: 1.9099, adv_loss: 0.5338\n",
      "Epoch 15, Batch 99, loss_ca: 1.8421, adv_loss: 0.5083\n",
      "Epoch 15, Batch 100, loss_ca: 1.9222, adv_loss: 0.4977\n",
      "Epoch 15, Batch 101, loss_ca: 1.9616, adv_loss: 0.4967\n",
      "Epoch 15, Batch 102, loss_ca: 1.9301, adv_loss: 0.5305\n",
      "Epoch 15, Batch 103, loss_ca: 1.9367, adv_loss: 0.5356\n",
      "Epoch 15, Batch 104, loss_ca: 1.9250, adv_loss: 0.5357\n",
      "Epoch 15, Batch 105, loss_ca: 2.0054, adv_loss: 0.5370\n",
      "Epoch 15, Batch 106, loss_ca: 1.9774, adv_loss: 0.5629\n",
      "Epoch 15, Batch 107, loss_ca: 2.1131, adv_loss: 0.5699\n",
      "Epoch 15, Batch 108, loss_ca: 2.0937, adv_loss: 0.5930\n",
      "Epoch 15, Batch 109, loss_ca: 2.1948, adv_loss: 0.6184\n",
      "Epoch 15, Batch 110, loss_ca: 2.0917, adv_loss: 0.6156\n",
      "Epoch 15, Batch 111, loss_ca: 2.0047, adv_loss: 0.6166\n",
      "Epoch 15, Batch 112, loss_ca: 1.9710, adv_loss: 0.6063\n",
      "Epoch 15, Batch 113, loss_ca: 1.8607, adv_loss: 0.5952\n",
      "Epoch 15, Batch 114, loss_ca: 1.8391, adv_loss: 0.5774\n",
      "Epoch 15, Batch 115, loss_ca: 1.8004, adv_loss: 0.5750\n",
      "Epoch 15, Batch 116, loss_ca: 1.8723, adv_loss: 0.5487\n",
      "Epoch 15, Batch 117, loss_ca: 1.8779, adv_loss: 0.5512\n",
      "Epoch 15, Batch 118, loss_ca: 1.8723, adv_loss: 0.5476\n",
      "Epoch 15, Batch 119, loss_ca: 1.8143, adv_loss: 0.5401\n",
      "Epoch 15, Batch 120, loss_ca: 1.7728, adv_loss: 0.5303\n",
      "Epoch 15, Batch 121, loss_ca: 1.9420, adv_loss: 0.5488\n",
      "Epoch 15, Batch 122, loss_ca: 2.0542, adv_loss: 0.5548\n",
      "Epoch 15, Batch 123, loss_ca: 2.0243, adv_loss: 0.5409\n",
      "Epoch 15, Batch 124, loss_ca: 1.9909, adv_loss: 0.5583\n",
      "Epoch 15, Batch 125, loss_ca: 1.9905, adv_loss: 0.5419\n",
      "Epoch 15, Batch 126, loss_ca: 2.0102, adv_loss: 0.5287\n",
      "Epoch 15, Batch 127, loss_ca: 1.9853, adv_loss: 0.4885\n",
      "Epoch 15, Batch 128, loss_ca: 1.9777, adv_loss: 0.4690\n",
      "Epoch 15, Batch 129, loss_ca: 1.9831, adv_loss: 0.4687\n",
      "Epoch 15, Batch 130, loss_ca: 1.9367, adv_loss: 0.4908\n",
      "Epoch 15, Batch 131, loss_ca: 1.9297, adv_loss: 0.4945\n",
      "Epoch 15, Batch 132, loss_ca: 1.9915, adv_loss: 0.5497\n",
      "Epoch 15, Batch 133, loss_ca: 1.9355, adv_loss: 0.5502\n",
      "Epoch 15, Batch 134, loss_ca: 1.9241, adv_loss: 0.5671\n",
      "Epoch 15, Batch 135, loss_ca: 1.8787, adv_loss: 0.5572\n",
      "Epoch 15, Batch 136, loss_ca: 1.8823, adv_loss: 0.5492\n",
      "Epoch 15, Batch 137, loss_ca: 1.8312, adv_loss: 0.5770\n",
      "Epoch 15, Batch 138, loss_ca: 1.7620, adv_loss: 0.5839\n",
      "Epoch 15, Batch 139, loss_ca: 1.7603, adv_loss: 0.5773\n",
      "Epoch 15, Batch 140, loss_ca: 1.7894, adv_loss: 0.5699\n",
      "Epoch 15, Batch 141, loss_ca: 1.8715, adv_loss: 0.5710\n",
      "Epoch 15, Batch 142, loss_ca: 1.9675, adv_loss: 0.5729\n",
      "Epoch 15, Batch 143, loss_ca: 1.8918, adv_loss: 0.5432\n",
      "Epoch 15, Batch 144, loss_ca: 1.8613, adv_loss: 0.5478\n",
      "Epoch 15, Batch 145, loss_ca: 1.7807, adv_loss: 0.5600\n",
      "Epoch 15, Batch 146, loss_ca: 1.7405, adv_loss: 0.5398\n",
      "Epoch 15, Batch 147, loss_ca: 1.8532, adv_loss: 0.5562\n",
      "Epoch 15, Batch 148, loss_ca: 1.9149, adv_loss: 0.5586\n",
      "Epoch 15, Batch 149, loss_ca: 1.8146, adv_loss: 0.5436\n",
      "Epoch 15, Batch 150, loss_ca: 1.7435, adv_loss: 0.5453\n",
      "Epoch 15, Batch 151, loss_ca: 1.7214, adv_loss: 0.5232\n",
      "Epoch 15, Batch 152, loss_ca: 1.6720, adv_loss: 0.4983\n",
      "Epoch 15, Batch 153, loss_ca: 1.8052, adv_loss: 0.4745\n",
      "Epoch 15, Batch 154, loss_ca: 1.8023, adv_loss: 0.4640\n",
      "Epoch 15, Batch 155, loss_ca: 1.8231, adv_loss: 0.4875\n",
      "Epoch 15, Batch 156, loss_ca: 1.8427, adv_loss: 0.5052\n",
      "Epoch 15, Batch 157, loss_ca: 1.9258, adv_loss: 0.5416\n",
      "Epoch 15, Batch 158, loss_ca: 2.1782, adv_loss: 0.5381\n",
      "Epoch 15, Batch 159, loss_ca: 2.3223, adv_loss: 0.5300\n",
      "Epoch 15, Batch 160, loss_ca: 2.2623, adv_loss: 0.5281\n",
      "Epoch 15, Batch 161, loss_ca: 1.9359, adv_loss: 0.4643\n",
      "Epoch 15, Batch 162, loss_ca: 2.5192, adv_loss: 0.4530\n",
      "Epoch 15, Batch 163, loss_ca: 2.2713, adv_loss: 0.4560\n",
      "Epoch 15, Batch 164, loss_ca: 2.5291, adv_loss: 0.4776\n",
      "Epoch 15, Batch 165, loss_ca: 2.6614, adv_loss: 0.4605\n",
      "Epoch 15, Batch 166, loss_ca: 2.5428, adv_loss: 0.5124\n",
      "Epoch 15, Batch 167, loss_ca: 2.2926, adv_loss: 0.4512\n",
      "Epoch 15, Batch 168, loss_ca: 2.4977, adv_loss: 0.5057\n",
      "Epoch 15, Batch 169, loss_ca: 2.2790, adv_loss: 0.5537\n",
      "Epoch 15, Batch 170, loss_ca: 2.1809, adv_loss: 0.5467\n",
      "Epoch 15, Batch 171, loss_ca: 2.2361, adv_loss: 0.5881\n",
      "Epoch 15, Batch 172, loss_ca: 2.2874, adv_loss: 0.6082\n",
      "Epoch 15, Batch 173, loss_ca: 2.1517, adv_loss: 0.5989\n",
      "Epoch 15, Batch 174, loss_ca: 2.1338, adv_loss: 0.5866\n",
      "Epoch 15, Batch 175, loss_ca: 2.1977, adv_loss: 0.5795\n",
      "Epoch 15, Batch 176, loss_ca: 2.0196, adv_loss: 0.5507\n",
      "Epoch 15, Batch 177, loss_ca: 2.0252, adv_loss: 0.5589\n",
      "Epoch 15, Batch 178, loss_ca: 1.9234, adv_loss: 0.5229\n",
      "Epoch 15, Batch 179, loss_ca: 1.8999, adv_loss: 0.5211\n",
      "Epoch 15, Batch 180, loss_ca: 1.8004, adv_loss: 0.5220\n",
      "Epoch 15, Batch 181, loss_ca: 1.7856, adv_loss: 0.5629\n",
      "Epoch 15, Batch 182, loss_ca: 1.9235, adv_loss: 0.5719\n",
      "Epoch 15, Batch 183, loss_ca: 1.9604, adv_loss: 0.5960\n",
      "Epoch 15, Batch 184, loss_ca: 1.8675, adv_loss: 0.5697\n",
      "Epoch 15, Batch 185, loss_ca: 1.7674, adv_loss: 0.5536\n",
      "Epoch 15, Batch 186, loss_ca: 1.7730, adv_loss: 0.5115\n",
      "Epoch 15, Batch 187, loss_ca: 1.7888, adv_loss: 0.4881\n",
      "Epoch 15, Batch 188, loss_ca: 1.7642, adv_loss: 0.4837\n",
      "Epoch 15, Batch 189, loss_ca: 1.8462, adv_loss: 0.4690\n",
      "Epoch 15, Batch 190, loss_ca: 2.0730, adv_loss: 0.5097\n",
      "Epoch 15, Batch 191, loss_ca: 2.0743, adv_loss: 0.4576\n",
      "Epoch 15, Batch 192, loss_ca: 2.3946, adv_loss: 0.4710\n",
      "Epoch 15, Batch 193, loss_ca: 2.1776, adv_loss: 0.4947\n",
      "Epoch 15, Batch 194, loss_ca: 2.0923, adv_loss: 0.5496\n",
      "Epoch 15, Batch 195, loss_ca: 2.1571, adv_loss: 0.5274\n",
      "Epoch 15, Batch 196, loss_ca: 2.4945, adv_loss: 0.5663\n",
      "Epoch 15, Batch 197, loss_ca: 2.3937, adv_loss: 0.5802\n",
      "Epoch 15, Batch 198, loss_ca: 2.7392, adv_loss: 0.5401\n",
      "Epoch 15, Batch 199, loss_ca: 2.5157, adv_loss: 0.5340\n",
      "Epoch 15, Batch 200, loss_ca: 2.2198, adv_loss: 0.4396\n",
      "Epoch 15, Batch 201, loss_ca: 2.0919, adv_loss: 0.4573\n",
      "Epoch 15, Batch 202, loss_ca: 2.1091, adv_loss: 0.4345\n",
      "Epoch 15, Batch 203, loss_ca: 2.3128, adv_loss: 0.4877\n",
      "Epoch 15, Batch 204, loss_ca: 2.1107, adv_loss: 0.5096\n",
      "Epoch 15, Batch 205, loss_ca: 2.4593, adv_loss: 0.5626\n",
      "Epoch 15, Batch 206, loss_ca: 2.2570, adv_loss: 0.5546\n",
      "Epoch 15, Batch 207, loss_ca: 2.1286, adv_loss: 0.5114\n",
      "Epoch 15, Batch 208, loss_ca: 2.0278, adv_loss: 0.5240\n",
      "Epoch 15, Batch 209, loss_ca: 1.8111, adv_loss: 0.4900\n",
      "Epoch 15, Batch 210, loss_ca: 1.8976, adv_loss: 0.5670\n",
      "Epoch 15, Batch 211, loss_ca: 2.2162, adv_loss: 0.6386\n",
      "Epoch 15, Batch 212, loss_ca: 2.1668, adv_loss: 0.6692\n",
      "Epoch 15, Batch 213, loss_ca: 2.3519, adv_loss: 0.6699\n",
      "Epoch 15, Batch 214, loss_ca: 2.0616, adv_loss: 0.6208\n",
      "Epoch 15, Batch 215, loss_ca: 1.9460, adv_loss: 0.6567\n",
      "Epoch 15, Batch 216, loss_ca: 1.9749, adv_loss: 0.6558\n",
      "Epoch 15, Batch 217, loss_ca: 1.9123, adv_loss: 0.6269\n",
      "Epoch 15, Batch 218, loss_ca: 1.9324, adv_loss: 0.6217\n",
      "Epoch 15, Batch 219, loss_ca: 2.2350, adv_loss: 0.6256\n",
      "Epoch 15, Batch 220, loss_ca: 2.1416, adv_loss: 0.6178\n",
      "Epoch 15, Batch 221, loss_ca: 1.9611, adv_loss: 0.5615\n",
      "Epoch 15, Batch 222, loss_ca: 1.8348, adv_loss: 0.5297\n",
      "Epoch 15, Batch 223, loss_ca: 1.7913, adv_loss: 0.4917\n",
      "Epoch 15, Batch 224, loss_ca: 1.8016, adv_loss: 0.4733\n",
      "Epoch 15, Batch 225, loss_ca: 1.7694, adv_loss: 0.4592\n",
      "Epoch 15, Batch 226, loss_ca: 1.7971, adv_loss: 0.4743\n",
      "Epoch 15, Batch 227, loss_ca: 1.7675, adv_loss: 0.4847\n",
      "Epoch 15, Batch 228, loss_ca: 1.7889, adv_loss: 0.4690\n",
      "Epoch 16, Batch 16, loss_ca: 1.9526, adv_loss: 0.4899\n",
      "Epoch 16, Batch 17, loss_ca: 1.8884, adv_loss: 0.4988\n",
      "Epoch 16, Batch 18, loss_ca: 1.8941, adv_loss: 0.5020\n",
      "Epoch 16, Batch 19, loss_ca: 1.9258, adv_loss: 0.5319\n",
      "Epoch 16, Batch 20, loss_ca: 1.8961, adv_loss: 0.5037\n",
      "Epoch 16, Batch 21, loss_ca: 1.8324, adv_loss: 0.5625\n",
      "Epoch 16, Batch 22, loss_ca: 1.7591, adv_loss: 0.5458\n",
      "Epoch 16, Batch 23, loss_ca: 1.8664, adv_loss: 0.5243\n",
      "Epoch 16, Batch 24, loss_ca: 2.0888, adv_loss: 0.4797\n",
      "Epoch 16, Batch 25, loss_ca: 2.1503, adv_loss: 0.4860\n",
      "Epoch 16, Batch 26, loss_ca: 2.4252, adv_loss: 0.4945\n",
      "Epoch 16, Batch 27, loss_ca: 2.1702, adv_loss: 0.4687\n",
      "Epoch 16, Batch 28, loss_ca: 2.0244, adv_loss: 0.4828\n",
      "Epoch 16, Batch 29, loss_ca: 2.0059, adv_loss: 0.4658\n",
      "Epoch 16, Batch 30, loss_ca: 2.0910, adv_loss: 0.4864\n",
      "Epoch 16, Batch 31, loss_ca: 2.0657, adv_loss: 0.4635\n",
      "Epoch 16, Batch 32, loss_ca: 1.9141, adv_loss: 0.4839\n",
      "Epoch 16, Batch 33, loss_ca: 2.0900, adv_loss: 0.5753\n",
      "Epoch 16, Batch 34, loss_ca: 2.1341, adv_loss: 0.5561\n",
      "Epoch 16, Batch 35, loss_ca: 2.0881, adv_loss: 0.5830\n",
      "Epoch 16, Batch 36, loss_ca: 2.0833, adv_loss: 0.5838\n",
      "Epoch 16, Batch 37, loss_ca: 2.0992, adv_loss: 0.5758\n",
      "Epoch 16, Batch 38, loss_ca: 2.0920, adv_loss: 0.5772\n",
      "Epoch 16, Batch 39, loss_ca: 2.0471, adv_loss: 0.5932\n",
      "Epoch 16, Batch 40, loss_ca: 2.0002, adv_loss: 0.5845\n",
      "Epoch 16, Batch 41, loss_ca: 2.1319, adv_loss: 0.5848\n",
      "Epoch 16, Batch 42, loss_ca: 2.0817, adv_loss: 0.5812\n",
      "Epoch 16, Batch 43, loss_ca: 1.9280, adv_loss: 0.5679\n",
      "Epoch 16, Batch 44, loss_ca: 1.8534, adv_loss: 0.5620\n",
      "Epoch 16, Batch 45, loss_ca: 1.8485, adv_loss: 0.5656\n",
      "Epoch 16, Batch 46, loss_ca: 1.8284, adv_loss: 0.5689\n",
      "Epoch 16, Batch 47, loss_ca: 1.8673, adv_loss: 0.5728\n",
      "Epoch 16, Batch 48, loss_ca: 1.8938, adv_loss: 0.6134\n",
      "Epoch 16, Batch 49, loss_ca: 1.8979, adv_loss: 0.6089\n",
      "Epoch 16, Batch 50, loss_ca: 1.8561, adv_loss: 0.6036\n",
      "Epoch 16, Batch 51, loss_ca: 1.8196, adv_loss: 0.5969\n",
      "Epoch 16, Batch 52, loss_ca: 1.8827, adv_loss: 0.5685\n",
      "Epoch 16, Batch 53, loss_ca: 1.8026, adv_loss: 0.5685\n",
      "Epoch 16, Batch 54, loss_ca: 1.7799, adv_loss: 0.5534\n",
      "Epoch 16, Batch 55, loss_ca: 1.7387, adv_loss: 0.5585\n",
      "Epoch 16, Batch 56, loss_ca: 1.7299, adv_loss: 0.5413\n",
      "Epoch 16, Batch 57, loss_ca: 1.7614, adv_loss: 0.5284\n",
      "Epoch 16, Batch 58, loss_ca: 1.9119, adv_loss: 0.5270\n",
      "Epoch 16, Batch 59, loss_ca: 1.9438, adv_loss: 0.5097\n",
      "Epoch 16, Batch 60, loss_ca: 1.9659, adv_loss: 0.5267\n",
      "Epoch 16, Batch 61, loss_ca: 1.9144, adv_loss: 0.5264\n",
      "Epoch 16, Batch 62, loss_ca: 2.0838, adv_loss: 0.5213\n",
      "Epoch 16, Batch 63, loss_ca: 2.1188, adv_loss: 0.5055\n",
      "Epoch 16, Batch 64, loss_ca: 2.1135, adv_loss: 0.5062\n",
      "Epoch 16, Batch 65, loss_ca: 2.0143, adv_loss: 0.5141\n",
      "Epoch 16, Batch 66, loss_ca: 1.8940, adv_loss: 0.5414\n",
      "Epoch 16, Batch 67, loss_ca: 1.7819, adv_loss: 0.5149\n",
      "Epoch 16, Batch 68, loss_ca: 1.8036, adv_loss: 0.5334\n",
      "Epoch 16, Batch 69, loss_ca: 1.8322, adv_loss: 0.5438\n",
      "Epoch 16, Batch 70, loss_ca: 1.8379, adv_loss: 0.5386\n",
      "Epoch 16, Batch 71, loss_ca: 1.7611, adv_loss: 0.5450\n",
      "Epoch 16, Batch 72, loss_ca: 1.7804, adv_loss: 0.5312\n",
      "Epoch 16, Batch 73, loss_ca: 1.7083, adv_loss: 0.5183\n",
      "Epoch 16, Batch 74, loss_ca: 1.7130, adv_loss: 0.5006\n",
      "Epoch 16, Batch 75, loss_ca: 1.6770, adv_loss: 0.4842\n",
      "Epoch 16, Batch 76, loss_ca: 1.6288, adv_loss: 0.4850\n",
      "Epoch 16, Batch 77, loss_ca: 1.6223, adv_loss: 0.4849\n",
      "Epoch 16, Batch 78, loss_ca: 1.6997, adv_loss: 0.4890\n",
      "Epoch 16, Batch 79, loss_ca: 1.7214, adv_loss: 0.4967\n",
      "Epoch 16, Batch 80, loss_ca: 1.6593, adv_loss: 0.4993\n",
      "Epoch 16, Batch 81, loss_ca: 1.6453, adv_loss: 0.5042\n",
      "Epoch 16, Batch 82, loss_ca: 1.6919, adv_loss: 0.5061\n",
      "Epoch 16, Batch 83, loss_ca: 1.7341, adv_loss: 0.5045\n",
      "Epoch 16, Batch 84, loss_ca: 1.7722, adv_loss: 0.5044\n",
      "Epoch 16, Batch 85, loss_ca: 1.8067, adv_loss: 0.4872\n",
      "Epoch 16, Batch 86, loss_ca: 1.9339, adv_loss: 0.4862\n",
      "Epoch 16, Batch 87, loss_ca: 1.9417, adv_loss: 0.4961\n",
      "Epoch 16, Batch 88, loss_ca: 1.8736, adv_loss: 0.4969\n",
      "Epoch 16, Batch 89, loss_ca: 1.8580, adv_loss: 0.5130\n",
      "Epoch 16, Batch 90, loss_ca: 1.7346, adv_loss: 0.5094\n",
      "Epoch 16, Batch 91, loss_ca: 1.8618, adv_loss: 0.5177\n",
      "Epoch 16, Batch 92, loss_ca: 1.7800, adv_loss: 0.5191\n",
      "Epoch 16, Batch 93, loss_ca: 1.9203, adv_loss: 0.5276\n",
      "Epoch 16, Batch 94, loss_ca: 1.8330, adv_loss: 0.5254\n",
      "Epoch 16, Batch 95, loss_ca: 1.9976, adv_loss: 0.5017\n",
      "Epoch 16, Batch 96, loss_ca: 2.0485, adv_loss: 0.5322\n",
      "Epoch 16, Batch 97, loss_ca: 1.9876, adv_loss: 0.5416\n",
      "Epoch 16, Batch 98, loss_ca: 1.9413, adv_loss: 0.5479\n",
      "Epoch 16, Batch 99, loss_ca: 1.8501, adv_loss: 0.5290\n",
      "Epoch 16, Batch 100, loss_ca: 1.9169, adv_loss: 0.5133\n",
      "Epoch 16, Batch 101, loss_ca: 1.9863, adv_loss: 0.5264\n",
      "Epoch 16, Batch 102, loss_ca: 1.9210, adv_loss: 0.5287\n",
      "Epoch 16, Batch 103, loss_ca: 1.8892, adv_loss: 0.5364\n",
      "Epoch 16, Batch 104, loss_ca: 1.8997, adv_loss: 0.5548\n",
      "Epoch 16, Batch 105, loss_ca: 2.0800, adv_loss: 0.5698\n",
      "Epoch 16, Batch 106, loss_ca: 2.0672, adv_loss: 0.5907\n",
      "Epoch 16, Batch 107, loss_ca: 2.1863, adv_loss: 0.5752\n",
      "Epoch 16, Batch 108, loss_ca: 2.1062, adv_loss: 0.5850\n",
      "Epoch 16, Batch 109, loss_ca: 2.0648, adv_loss: 0.5729\n",
      "Epoch 16, Batch 110, loss_ca: 2.0041, adv_loss: 0.5754\n",
      "Epoch 16, Batch 111, loss_ca: 1.9672, adv_loss: 0.5645\n",
      "Epoch 16, Batch 112, loss_ca: 1.9537, adv_loss: 0.5581\n",
      "Epoch 16, Batch 113, loss_ca: 1.8386, adv_loss: 0.5413\n",
      "Epoch 16, Batch 114, loss_ca: 1.8273, adv_loss: 0.5374\n",
      "Epoch 16, Batch 115, loss_ca: 1.7895, adv_loss: 0.5293\n",
      "Epoch 16, Batch 116, loss_ca: 1.8536, adv_loss: 0.5617\n",
      "Epoch 16, Batch 117, loss_ca: 1.8774, adv_loss: 0.5523\n",
      "Epoch 16, Batch 118, loss_ca: 1.8490, adv_loss: 0.5481\n",
      "Epoch 16, Batch 119, loss_ca: 1.8371, adv_loss: 0.5767\n",
      "Epoch 16, Batch 120, loss_ca: 1.8042, adv_loss: 0.5826\n",
      "Epoch 16, Batch 121, loss_ca: 1.9264, adv_loss: 0.5743\n",
      "Epoch 16, Batch 122, loss_ca: 2.0168, adv_loss: 0.5533\n",
      "Epoch 16, Batch 123, loss_ca: 1.9993, adv_loss: 0.5503\n",
      "Epoch 16, Batch 124, loss_ca: 1.9774, adv_loss: 0.5541\n",
      "Epoch 16, Batch 125, loss_ca: 1.9522, adv_loss: 0.5244\n",
      "Epoch 16, Batch 126, loss_ca: 1.9962, adv_loss: 0.5348\n",
      "Epoch 16, Batch 127, loss_ca: 1.9297, adv_loss: 0.4925\n",
      "Epoch 16, Batch 128, loss_ca: 1.9494, adv_loss: 0.4940\n",
      "Epoch 16, Batch 129, loss_ca: 1.9644, adv_loss: 0.4588\n",
      "Epoch 16, Batch 130, loss_ca: 1.9343, adv_loss: 0.4656\n",
      "Epoch 16, Batch 131, loss_ca: 1.8814, adv_loss: 0.4442\n",
      "Epoch 16, Batch 132, loss_ca: 1.9486, adv_loss: 0.4674\n",
      "Epoch 16, Batch 133, loss_ca: 1.9463, adv_loss: 0.4833\n",
      "Epoch 16, Batch 134, loss_ca: 1.9605, adv_loss: 0.5213\n",
      "Epoch 16, Batch 135, loss_ca: 1.8934, adv_loss: 0.5322\n",
      "Epoch 16, Batch 136, loss_ca: 1.9116, adv_loss: 0.5376\n",
      "Epoch 16, Batch 137, loss_ca: 1.7582, adv_loss: 0.5466\n",
      "Epoch 16, Batch 138, loss_ca: 1.7644, adv_loss: 0.5538\n",
      "Epoch 16, Batch 139, loss_ca: 1.7378, adv_loss: 0.5506\n",
      "Epoch 16, Batch 140, loss_ca: 1.6968, adv_loss: 0.5481\n",
      "Epoch 16, Batch 141, loss_ca: 1.7628, adv_loss: 0.5477\n",
      "Epoch 16, Batch 142, loss_ca: 1.9373, adv_loss: 0.5807\n",
      "Epoch 16, Batch 143, loss_ca: 1.9110, adv_loss: 0.5478\n",
      "Epoch 16, Batch 144, loss_ca: 1.9145, adv_loss: 0.5447\n",
      "Epoch 16, Batch 145, loss_ca: 1.8667, adv_loss: 0.5298\n",
      "Epoch 16, Batch 146, loss_ca: 1.8161, adv_loss: 0.5074\n",
      "Epoch 16, Batch 147, loss_ca: 1.8837, adv_loss: 0.5103\n",
      "Epoch 16, Batch 148, loss_ca: 1.8687, adv_loss: 0.5059\n",
      "Epoch 16, Batch 149, loss_ca: 1.7412, adv_loss: 0.4906\n",
      "Epoch 16, Batch 150, loss_ca: 1.6882, adv_loss: 0.4704\n",
      "Epoch 16, Batch 151, loss_ca: 1.6310, adv_loss: 0.4685\n",
      "Epoch 16, Batch 152, loss_ca: 1.6129, adv_loss: 0.4705\n",
      "Epoch 16, Batch 153, loss_ca: 1.8218, adv_loss: 0.4866\n",
      "Epoch 16, Batch 154, loss_ca: 1.7932, adv_loss: 0.5039\n",
      "Epoch 16, Batch 155, loss_ca: 1.8238, adv_loss: 0.5291\n",
      "Epoch 16, Batch 156, loss_ca: 1.8083, adv_loss: 0.5231\n",
      "Epoch 16, Batch 157, loss_ca: 1.7949, adv_loss: 0.5211\n",
      "Epoch 16, Batch 158, loss_ca: 2.0533, adv_loss: 0.5112\n",
      "Epoch 16, Batch 159, loss_ca: 2.3074, adv_loss: 0.5358\n",
      "Epoch 16, Batch 160, loss_ca: 2.2359, adv_loss: 0.5133\n",
      "Epoch 16, Batch 161, loss_ca: 2.1452, adv_loss: 0.4421\n",
      "Epoch 16, Batch 162, loss_ca: 2.5010, adv_loss: 0.4739\n",
      "Epoch 16, Batch 163, loss_ca: 2.4400, adv_loss: 0.4691\n",
      "Epoch 16, Batch 164, loss_ca: 2.5118, adv_loss: 0.4809\n",
      "Epoch 16, Batch 165, loss_ca: 2.6368, adv_loss: 0.5212\n",
      "Epoch 16, Batch 166, loss_ca: 2.2766, adv_loss: 0.5272\n",
      "Epoch 16, Batch 167, loss_ca: 2.2520, adv_loss: 0.4990\n",
      "Epoch 16, Batch 168, loss_ca: 2.2823, adv_loss: 0.5718\n",
      "Epoch 16, Batch 169, loss_ca: 2.3654, adv_loss: 0.6172\n",
      "Epoch 16, Batch 170, loss_ca: 2.2849, adv_loss: 0.5268\n",
      "Epoch 16, Batch 171, loss_ca: 2.3240, adv_loss: 0.5622\n",
      "Epoch 16, Batch 172, loss_ca: 2.3023, adv_loss: 0.5848\n",
      "Epoch 16, Batch 173, loss_ca: 2.1511, adv_loss: 0.5351\n",
      "Epoch 16, Batch 174, loss_ca: 2.1663, adv_loss: 0.5288\n",
      "Epoch 16, Batch 175, loss_ca: 2.2313, adv_loss: 0.5223\n",
      "Epoch 16, Batch 176, loss_ca: 1.9689, adv_loss: 0.4826\n",
      "Epoch 16, Batch 177, loss_ca: 1.8249, adv_loss: 0.4730\n",
      "Epoch 16, Batch 178, loss_ca: 2.0250, adv_loss: 0.4915\n",
      "Epoch 16, Batch 179, loss_ca: 1.8298, adv_loss: 0.4759\n",
      "Epoch 16, Batch 180, loss_ca: 1.7158, adv_loss: 0.4839\n",
      "Epoch 16, Batch 181, loss_ca: 1.7841, adv_loss: 0.5208\n",
      "Epoch 16, Batch 182, loss_ca: 1.7968, adv_loss: 0.5260\n",
      "Epoch 16, Batch 183, loss_ca: 1.9896, adv_loss: 0.5125\n",
      "Epoch 16, Batch 184, loss_ca: 1.8469, adv_loss: 0.5162\n",
      "Epoch 16, Batch 185, loss_ca: 1.9398, adv_loss: 0.5214\n",
      "Epoch 16, Batch 186, loss_ca: 1.8315, adv_loss: 0.5069\n",
      "Epoch 16, Batch 187, loss_ca: 1.7591, adv_loss: 0.5059\n",
      "Epoch 16, Batch 188, loss_ca: 1.7059, adv_loss: 0.4936\n",
      "Epoch 16, Batch 189, loss_ca: 1.8101, adv_loss: 0.5276\n",
      "Epoch 16, Batch 190, loss_ca: 2.0502, adv_loss: 0.5449\n",
      "Epoch 16, Batch 191, loss_ca: 2.1037, adv_loss: 0.5090\n",
      "Epoch 16, Batch 192, loss_ca: 2.2941, adv_loss: 0.5612\n",
      "Epoch 16, Batch 193, loss_ca: 2.1202, adv_loss: 0.5342\n",
      "Epoch 16, Batch 194, loss_ca: 1.9959, adv_loss: 0.5591\n",
      "Epoch 16, Batch 195, loss_ca: 2.2308, adv_loss: 0.5720\n",
      "Epoch 16, Batch 196, loss_ca: 2.4026, adv_loss: 0.5282\n",
      "Epoch 16, Batch 197, loss_ca: 2.2018, adv_loss: 0.5237\n",
      "Epoch 16, Batch 198, loss_ca: 2.6699, adv_loss: 0.5110\n",
      "Epoch 16, Batch 199, loss_ca: 2.5037, adv_loss: 0.5442\n",
      "Epoch 16, Batch 200, loss_ca: 2.0979, adv_loss: 0.4857\n",
      "Epoch 16, Batch 201, loss_ca: 2.1236, adv_loss: 0.5551\n",
      "Epoch 16, Batch 202, loss_ca: 2.2106, adv_loss: 0.5831\n",
      "Epoch 16, Batch 203, loss_ca: 2.4839, adv_loss: 0.6268\n",
      "Epoch 16, Batch 204, loss_ca: 2.0799, adv_loss: 0.5807\n",
      "Epoch 16, Batch 205, loss_ca: 2.2666, adv_loss: 0.5785\n",
      "Epoch 16, Batch 206, loss_ca: 2.0693, adv_loss: 0.5569\n",
      "Epoch 16, Batch 207, loss_ca: 2.0707, adv_loss: 0.5334\n",
      "Epoch 16, Batch 208, loss_ca: 2.0086, adv_loss: 0.5179\n",
      "Epoch 16, Batch 209, loss_ca: 1.8883, adv_loss: 0.4819\n",
      "Epoch 16, Batch 210, loss_ca: 2.0069, adv_loss: 0.5172\n",
      "Epoch 16, Batch 211, loss_ca: 2.0419, adv_loss: 0.5402\n",
      "Epoch 16, Batch 212, loss_ca: 1.6800, adv_loss: 0.6000\n",
      "Epoch 16, Batch 213, loss_ca: 1.9835, adv_loss: 0.5119\n",
      "Epoch 16, Batch 214, loss_ca: 1.7979, adv_loss: 0.5311\n",
      "Epoch 16, Batch 215, loss_ca: 1.6710, adv_loss: 0.5988\n",
      "Epoch 16, Batch 216, loss_ca: 2.0017, adv_loss: 0.6599\n",
      "Epoch 16, Batch 217, loss_ca: 1.9719, adv_loss: 0.6136\n",
      "Epoch 16, Batch 218, loss_ca: 2.1518, adv_loss: 0.6450\n",
      "Epoch 16, Batch 219, loss_ca: 2.3932, adv_loss: 0.6301\n",
      "Epoch 16, Batch 220, loss_ca: 2.0508, adv_loss: 0.6038\n",
      "Epoch 16, Batch 221, loss_ca: 1.9918, adv_loss: 0.5831\n",
      "Epoch 16, Batch 222, loss_ca: 1.8716, adv_loss: 0.5680\n",
      "Epoch 16, Batch 223, loss_ca: 1.7635, adv_loss: 0.5146\n",
      "Epoch 16, Batch 224, loss_ca: 1.7724, adv_loss: 0.5326\n",
      "Epoch 16, Batch 225, loss_ca: 1.7041, adv_loss: 0.4790\n",
      "Epoch 16, Batch 226, loss_ca: 1.7917, adv_loss: 0.4612\n",
      "Epoch 16, Batch 227, loss_ca: 1.7343, adv_loss: 0.4681\n",
      "Epoch 16, Batch 228, loss_ca: 1.7604, adv_loss: 0.4550\n",
      "Epoch 17, Batch 17, loss_ca: 2.0936, adv_loss: 0.4931\n",
      "Epoch 17, Batch 18, loss_ca: 2.2288, adv_loss: 0.5314\n",
      "Epoch 17, Batch 19, loss_ca: 2.0445, adv_loss: 0.5350\n",
      "Epoch 17, Batch 20, loss_ca: 2.0813, adv_loss: 0.5603\n",
      "Epoch 17, Batch 21, loss_ca: 1.9268, adv_loss: 0.5651\n",
      "Epoch 17, Batch 22, loss_ca: 1.8807, adv_loss: 0.5693\n",
      "Epoch 17, Batch 23, loss_ca: 1.8191, adv_loss: 0.5493\n",
      "Epoch 17, Batch 24, loss_ca: 2.1189, adv_loss: 0.5322\n",
      "Epoch 17, Batch 25, loss_ca: 2.1475, adv_loss: 0.4932\n",
      "Epoch 17, Batch 26, loss_ca: 2.3427, adv_loss: 0.4819\n",
      "Epoch 17, Batch 27, loss_ca: 2.2144, adv_loss: 0.4846\n",
      "Epoch 17, Batch 28, loss_ca: 1.9830, adv_loss: 0.5054\n",
      "Epoch 17, Batch 29, loss_ca: 1.9329, adv_loss: 0.5271\n",
      "Epoch 17, Batch 30, loss_ca: 2.0642, adv_loss: 0.5397\n",
      "Epoch 17, Batch 31, loss_ca: 1.9833, adv_loss: 0.5280\n",
      "Epoch 17, Batch 32, loss_ca: 1.8854, adv_loss: 0.5283\n",
      "Epoch 17, Batch 33, loss_ca: 2.1202, adv_loss: 0.5621\n",
      "Epoch 17, Batch 34, loss_ca: 2.0559, adv_loss: 0.5090\n",
      "Epoch 17, Batch 35, loss_ca: 2.0413, adv_loss: 0.5242\n",
      "Epoch 17, Batch 36, loss_ca: 2.0800, adv_loss: 0.5138\n",
      "Epoch 17, Batch 37, loss_ca: 2.0087, adv_loss: 0.5455\n",
      "Epoch 17, Batch 38, loss_ca: 2.0548, adv_loss: 0.5685\n",
      "Epoch 17, Batch 39, loss_ca: 2.0972, adv_loss: 0.5960\n",
      "Epoch 17, Batch 40, loss_ca: 2.0743, adv_loss: 0.6179\n",
      "Epoch 17, Batch 41, loss_ca: 2.2035, adv_loss: 0.6235\n",
      "Epoch 17, Batch 42, loss_ca: 2.1347, adv_loss: 0.6400\n",
      "Epoch 17, Batch 43, loss_ca: 1.9869, adv_loss: 0.6252\n",
      "Epoch 17, Batch 44, loss_ca: 1.9204, adv_loss: 0.6258\n",
      "Epoch 17, Batch 45, loss_ca: 1.8876, adv_loss: 0.6410\n",
      "Epoch 17, Batch 46, loss_ca: 1.8602, adv_loss: 0.6343\n",
      "Epoch 17, Batch 47, loss_ca: 1.9254, adv_loss: 0.6409\n",
      "Epoch 17, Batch 48, loss_ca: 1.9147, adv_loss: 0.6525\n",
      "Epoch 17, Batch 49, loss_ca: 1.8797, adv_loss: 0.6579\n",
      "Epoch 17, Batch 50, loss_ca: 1.8538, adv_loss: 0.6445\n",
      "Epoch 17, Batch 51, loss_ca: 1.8603, adv_loss: 0.6326\n",
      "Epoch 17, Batch 52, loss_ca: 1.8929, adv_loss: 0.5999\n",
      "Epoch 17, Batch 53, loss_ca: 1.8497, adv_loss: 0.5802\n",
      "Epoch 17, Batch 54, loss_ca: 1.7378, adv_loss: 0.5434\n",
      "Epoch 17, Batch 55, loss_ca: 1.7077, adv_loss: 0.5109\n",
      "Epoch 17, Batch 56, loss_ca: 1.6924, adv_loss: 0.5055\n",
      "Epoch 17, Batch 57, loss_ca: 1.7024, adv_loss: 0.4962\n",
      "Epoch 17, Batch 58, loss_ca: 1.9395, adv_loss: 0.4974\n",
      "Epoch 17, Batch 59, loss_ca: 1.8470, adv_loss: 0.5075\n",
      "Epoch 17, Batch 60, loss_ca: 1.8754, adv_loss: 0.5153\n",
      "Epoch 17, Batch 61, loss_ca: 1.8467, adv_loss: 0.5054\n",
      "Epoch 17, Batch 62, loss_ca: 1.8853, adv_loss: 0.5212\n",
      "Epoch 17, Batch 63, loss_ca: 1.9356, adv_loss: 0.5383\n",
      "Epoch 17, Batch 64, loss_ca: 1.8984, adv_loss: 0.5107\n",
      "Epoch 17, Batch 65, loss_ca: 2.0093, adv_loss: 0.5255\n",
      "Epoch 17, Batch 66, loss_ca: 1.9969, adv_loss: 0.5366\n",
      "Epoch 17, Batch 67, loss_ca: 1.8405, adv_loss: 0.5289\n",
      "Epoch 17, Batch 68, loss_ca: 1.8207, adv_loss: 0.5093\n",
      "Epoch 17, Batch 69, loss_ca: 1.8643, adv_loss: 0.4935\n",
      "Epoch 17, Batch 70, loss_ca: 1.7940, adv_loss: 0.4999\n",
      "Epoch 17, Batch 71, loss_ca: 1.6933, adv_loss: 0.4782\n",
      "Epoch 17, Batch 72, loss_ca: 1.6931, adv_loss: 0.4801\n",
      "Epoch 17, Batch 73, loss_ca: 1.6695, adv_loss: 0.4809\n",
      "Epoch 17, Batch 74, loss_ca: 1.6869, adv_loss: 0.4957\n",
      "Epoch 17, Batch 75, loss_ca: 1.6659, adv_loss: 0.5011\n",
      "Epoch 17, Batch 76, loss_ca: 1.6597, adv_loss: 0.5111\n",
      "Epoch 17, Batch 77, loss_ca: 1.6683, adv_loss: 0.5289\n",
      "Epoch 17, Batch 78, loss_ca: 1.6934, adv_loss: 0.5266\n",
      "Epoch 17, Batch 79, loss_ca: 1.6627, adv_loss: 0.5304\n",
      "Epoch 17, Batch 80, loss_ca: 1.6476, adv_loss: 0.5325\n",
      "Epoch 17, Batch 81, loss_ca: 1.6401, adv_loss: 0.5149\n",
      "Epoch 17, Batch 82, loss_ca: 1.7121, adv_loss: 0.5098\n",
      "Epoch 17, Batch 83, loss_ca: 1.7153, adv_loss: 0.5027\n",
      "Epoch 17, Batch 84, loss_ca: 1.7490, adv_loss: 0.5011\n",
      "Epoch 17, Batch 85, loss_ca: 1.7496, adv_loss: 0.4900\n",
      "Epoch 17, Batch 86, loss_ca: 1.9340, adv_loss: 0.5068\n",
      "Epoch 17, Batch 87, loss_ca: 1.9376, adv_loss: 0.5020\n",
      "Epoch 17, Batch 88, loss_ca: 1.8735, adv_loss: 0.5047\n",
      "Epoch 17, Batch 89, loss_ca: 1.8604, adv_loss: 0.5135\n",
      "Epoch 17, Batch 90, loss_ca: 1.7901, adv_loss: 0.5191\n",
      "Epoch 17, Batch 91, loss_ca: 1.8692, adv_loss: 0.5243\n",
      "Epoch 17, Batch 92, loss_ca: 1.7855, adv_loss: 0.5016\n",
      "Epoch 17, Batch 93, loss_ca: 1.9005, adv_loss: 0.5220\n",
      "Epoch 17, Batch 94, loss_ca: 1.8197, adv_loss: 0.5091\n",
      "Epoch 17, Batch 95, loss_ca: 1.9145, adv_loss: 0.5120\n",
      "Epoch 17, Batch 96, loss_ca: 1.9982, adv_loss: 0.5233\n",
      "Epoch 17, Batch 97, loss_ca: 1.9361, adv_loss: 0.5196\n",
      "Epoch 17, Batch 98, loss_ca: 1.9107, adv_loss: 0.5222\n",
      "Epoch 17, Batch 99, loss_ca: 1.8132, adv_loss: 0.4792\n",
      "Epoch 17, Batch 100, loss_ca: 1.9185, adv_loss: 0.4723\n",
      "Epoch 17, Batch 101, loss_ca: 1.9433, adv_loss: 0.4967\n",
      "Epoch 17, Batch 102, loss_ca: 1.8750, adv_loss: 0.5037\n",
      "Epoch 17, Batch 103, loss_ca: 1.8800, adv_loss: 0.5355\n",
      "Epoch 17, Batch 104, loss_ca: 1.8996, adv_loss: 0.5367\n",
      "Epoch 17, Batch 105, loss_ca: 2.0284, adv_loss: 0.5751\n",
      "Epoch 17, Batch 106, loss_ca: 2.0239, adv_loss: 0.5926\n",
      "Epoch 17, Batch 107, loss_ca: 2.1373, adv_loss: 0.5887\n",
      "Epoch 17, Batch 108, loss_ca: 2.0257, adv_loss: 0.5893\n",
      "Epoch 17, Batch 109, loss_ca: 2.1967, adv_loss: 0.6431\n",
      "Epoch 17, Batch 110, loss_ca: 2.1287, adv_loss: 0.6462\n",
      "Epoch 17, Batch 111, loss_ca: 2.0486, adv_loss: 0.6337\n",
      "Epoch 17, Batch 112, loss_ca: 2.0182, adv_loss: 0.6285\n",
      "Epoch 17, Batch 113, loss_ca: 1.8968, adv_loss: 0.6005\n",
      "Epoch 17, Batch 114, loss_ca: 1.8540, adv_loss: 0.5896\n",
      "Epoch 17, Batch 115, loss_ca: 1.7717, adv_loss: 0.5616\n",
      "Epoch 17, Batch 116, loss_ca: 1.8269, adv_loss: 0.5316\n",
      "Epoch 17, Batch 117, loss_ca: 1.8620, adv_loss: 0.5339\n",
      "Epoch 17, Batch 118, loss_ca: 1.8715, adv_loss: 0.5439\n",
      "Epoch 17, Batch 119, loss_ca: 1.8377, adv_loss: 0.5370\n",
      "Epoch 17, Batch 120, loss_ca: 1.7821, adv_loss: 0.5392\n",
      "Epoch 17, Batch 121, loss_ca: 1.9212, adv_loss: 0.5298\n",
      "Epoch 17, Batch 122, loss_ca: 2.0409, adv_loss: 0.5832\n",
      "Epoch 17, Batch 123, loss_ca: 2.0182, adv_loss: 0.5712\n",
      "Epoch 17, Batch 124, loss_ca: 2.0029, adv_loss: 0.5734\n",
      "Epoch 17, Batch 125, loss_ca: 1.9616, adv_loss: 0.5462\n",
      "Epoch 17, Batch 126, loss_ca: 2.0075, adv_loss: 0.5603\n",
      "Epoch 17, Batch 127, loss_ca: 2.0096, adv_loss: 0.5305\n",
      "Epoch 17, Batch 128, loss_ca: 1.9948, adv_loss: 0.5230\n",
      "Epoch 17, Batch 129, loss_ca: 1.9782, adv_loss: 0.5193\n",
      "Epoch 17, Batch 130, loss_ca: 1.9163, adv_loss: 0.5242\n",
      "Epoch 17, Batch 131, loss_ca: 1.8931, adv_loss: 0.5079\n",
      "Epoch 17, Batch 132, loss_ca: 1.9672, adv_loss: 0.5598\n",
      "Epoch 17, Batch 133, loss_ca: 1.9333, adv_loss: 0.5163\n",
      "Epoch 17, Batch 134, loss_ca: 1.9055, adv_loss: 0.5500\n",
      "Epoch 17, Batch 135, loss_ca: 1.8896, adv_loss: 0.5741\n",
      "Epoch 17, Batch 136, loss_ca: 1.9104, adv_loss: 0.5490\n",
      "Epoch 17, Batch 137, loss_ca: 1.7594, adv_loss: 0.5494\n",
      "Epoch 17, Batch 138, loss_ca: 1.6861, adv_loss: 0.5411\n",
      "Epoch 17, Batch 139, loss_ca: 1.6550, adv_loss: 0.5254\n",
      "Epoch 17, Batch 140, loss_ca: 1.7062, adv_loss: 0.5266\n",
      "Epoch 17, Batch 141, loss_ca: 1.7930, adv_loss: 0.5315\n",
      "Epoch 17, Batch 142, loss_ca: 1.8408, adv_loss: 0.5332\n",
      "Epoch 17, Batch 143, loss_ca: 1.7717, adv_loss: 0.5339\n",
      "Epoch 17, Batch 144, loss_ca: 1.7445, adv_loss: 0.5382\n",
      "Epoch 17, Batch 145, loss_ca: 1.7502, adv_loss: 0.5433\n",
      "Epoch 17, Batch 146, loss_ca: 1.7829, adv_loss: 0.5306\n",
      "Epoch 17, Batch 147, loss_ca: 1.9071, adv_loss: 0.5272\n",
      "Epoch 17, Batch 148, loss_ca: 1.9447, adv_loss: 0.5207\n",
      "Epoch 17, Batch 149, loss_ca: 1.8722, adv_loss: 0.5073\n",
      "Epoch 17, Batch 150, loss_ca: 1.8142, adv_loss: 0.4769\n",
      "Epoch 17, Batch 151, loss_ca: 1.7513, adv_loss: 0.4828\n",
      "Epoch 17, Batch 152, loss_ca: 1.6455, adv_loss: 0.4734\n",
      "Epoch 17, Batch 153, loss_ca: 1.7844, adv_loss: 0.4719\n",
      "Epoch 17, Batch 154, loss_ca: 1.8134, adv_loss: 0.4864\n",
      "Epoch 17, Batch 155, loss_ca: 1.8149, adv_loss: 0.4990\n",
      "Epoch 17, Batch 156, loss_ca: 1.7877, adv_loss: 0.5107\n",
      "Epoch 17, Batch 157, loss_ca: 1.8034, adv_loss: 0.5273\n",
      "Epoch 17, Batch 158, loss_ca: 2.1311, adv_loss: 0.5610\n",
      "Epoch 17, Batch 159, loss_ca: 2.3120, adv_loss: 0.6010\n",
      "Epoch 17, Batch 160, loss_ca: 2.1723, adv_loss: 0.5302\n",
      "Epoch 17, Batch 161, loss_ca: 1.9467, adv_loss: 0.4925\n",
      "Epoch 17, Batch 162, loss_ca: 2.4199, adv_loss: 0.4944\n",
      "Epoch 17, Batch 163, loss_ca: 2.3118, adv_loss: 0.4711\n",
      "Epoch 17, Batch 164, loss_ca: 2.6206, adv_loss: 0.4878\n",
      "Epoch 17, Batch 165, loss_ca: 2.8477, adv_loss: 0.5085\n",
      "Epoch 17, Batch 166, loss_ca: 2.4339, adv_loss: 0.5351\n",
      "Epoch 17, Batch 167, loss_ca: 2.2031, adv_loss: 0.4892\n",
      "Epoch 17, Batch 168, loss_ca: 2.3869, adv_loss: 0.5107\n",
      "Epoch 17, Batch 169, loss_ca: 2.2513, adv_loss: 0.5803\n",
      "Epoch 17, Batch 170, loss_ca: 2.2063, adv_loss: 0.5373\n",
      "Epoch 17, Batch 171, loss_ca: 2.2301, adv_loss: 0.5571\n",
      "Epoch 17, Batch 172, loss_ca: 2.2705, adv_loss: 0.6085\n",
      "Epoch 17, Batch 173, loss_ca: 2.1005, adv_loss: 0.5731\n",
      "Epoch 17, Batch 174, loss_ca: 1.9527, adv_loss: 0.5291\n",
      "Epoch 17, Batch 175, loss_ca: 2.2198, adv_loss: 0.5686\n",
      "Epoch 17, Batch 176, loss_ca: 2.0988, adv_loss: 0.5542\n",
      "Epoch 17, Batch 177, loss_ca: 2.0911, adv_loss: 0.5445\n",
      "Epoch 17, Batch 178, loss_ca: 2.0353, adv_loss: 0.5150\n",
      "Epoch 17, Batch 179, loss_ca: 1.9231, adv_loss: 0.5042\n",
      "Epoch 17, Batch 180, loss_ca: 1.7656, adv_loss: 0.5457\n",
      "Epoch 17, Batch 181, loss_ca: 1.7671, adv_loss: 0.5308\n",
      "Epoch 17, Batch 182, loss_ca: 1.8506, adv_loss: 0.5453\n",
      "Epoch 17, Batch 183, loss_ca: 2.0143, adv_loss: 0.5598\n",
      "Epoch 17, Batch 184, loss_ca: 1.8188, adv_loss: 0.5625\n",
      "Epoch 17, Batch 185, loss_ca: 1.8084, adv_loss: 0.5444\n",
      "Epoch 17, Batch 186, loss_ca: 1.7720, adv_loss: 0.5175\n",
      "Epoch 17, Batch 187, loss_ca: 1.7786, adv_loss: 0.5019\n",
      "Epoch 17, Batch 188, loss_ca: 1.8141, adv_loss: 0.4788\n",
      "Epoch 17, Batch 189, loss_ca: 1.8004, adv_loss: 0.5044\n",
      "Epoch 17, Batch 190, loss_ca: 2.0661, adv_loss: 0.5336\n",
      "Epoch 17, Batch 191, loss_ca: 2.0723, adv_loss: 0.5204\n",
      "Epoch 17, Batch 192, loss_ca: 2.1810, adv_loss: 0.5257\n",
      "Epoch 17, Batch 193, loss_ca: 2.2131, adv_loss: 0.5681\n",
      "Epoch 17, Batch 194, loss_ca: 2.2889, adv_loss: 0.5971\n",
      "Epoch 17, Batch 195, loss_ca: 2.0456, adv_loss: 0.6008\n",
      "Epoch 17, Batch 196, loss_ca: 2.3458, adv_loss: 0.5826\n",
      "Epoch 17, Batch 197, loss_ca: 2.2149, adv_loss: 0.5763\n",
      "Epoch 17, Batch 198, loss_ca: 2.5673, adv_loss: 0.5719\n",
      "Epoch 17, Batch 199, loss_ca: 2.3684, adv_loss: 0.5585\n",
      "Epoch 17, Batch 200, loss_ca: 1.9686, adv_loss: 0.5249\n",
      "Epoch 17, Batch 201, loss_ca: 1.9623, adv_loss: 0.4969\n",
      "Epoch 17, Batch 202, loss_ca: 2.0422, adv_loss: 0.4849\n",
      "Epoch 17, Batch 203, loss_ca: 2.4386, adv_loss: 0.4833\n",
      "Epoch 17, Batch 204, loss_ca: 1.9707, adv_loss: 0.4834\n",
      "Epoch 17, Batch 205, loss_ca: 2.2130, adv_loss: 0.4900\n",
      "Epoch 17, Batch 206, loss_ca: 1.9426, adv_loss: 0.4592\n",
      "Epoch 17, Batch 207, loss_ca: 2.2090, adv_loss: 0.4955\n",
      "Epoch 17, Batch 208, loss_ca: 2.0810, adv_loss: 0.5088\n",
      "Epoch 17, Batch 209, loss_ca: 1.8779, adv_loss: 0.4772\n",
      "Epoch 17, Batch 210, loss_ca: 1.8932, adv_loss: 0.4761\n",
      "Epoch 17, Batch 211, loss_ca: 2.2660, adv_loss: 0.6218\n",
      "Epoch 17, Batch 212, loss_ca: 2.1418, adv_loss: 0.6315\n",
      "Epoch 17, Batch 213, loss_ca: 2.1482, adv_loss: 0.6045\n",
      "Epoch 17, Batch 214, loss_ca: 1.8437, adv_loss: 0.5442\n",
      "Epoch 17, Batch 215, loss_ca: 1.7072, adv_loss: 0.5709\n",
      "Epoch 17, Batch 216, loss_ca: 1.9268, adv_loss: 0.5537\n",
      "Epoch 17, Batch 217, loss_ca: 1.8467, adv_loss: 0.5622\n",
      "Epoch 17, Batch 218, loss_ca: 1.8226, adv_loss: 0.5995\n",
      "Epoch 17, Batch 219, loss_ca: 2.2358, adv_loss: 0.5789\n",
      "Epoch 17, Batch 220, loss_ca: 2.0710, adv_loss: 0.5937\n",
      "Epoch 17, Batch 221, loss_ca: 1.9296, adv_loss: 0.5883\n",
      "Epoch 17, Batch 222, loss_ca: 1.8354, adv_loss: 0.5941\n",
      "Epoch 17, Batch 223, loss_ca: 1.7958, adv_loss: 0.5750\n",
      "Epoch 17, Batch 224, loss_ca: 1.7701, adv_loss: 0.5487\n",
      "Epoch 17, Batch 225, loss_ca: 1.7578, adv_loss: 0.5411\n",
      "Epoch 17, Batch 226, loss_ca: 1.7598, adv_loss: 0.5103\n",
      "Epoch 17, Batch 227, loss_ca: 1.8266, adv_loss: 0.4727\n",
      "Epoch 17, Batch 228, loss_ca: 1.7780, adv_loss: 0.4865\n",
      "Epoch 18, Batch 18, loss_ca: 2.0470, adv_loss: 0.5334\n",
      "Epoch 18, Batch 19, loss_ca: 1.8802, adv_loss: 0.5178\n",
      "Epoch 18, Batch 20, loss_ca: 1.9431, adv_loss: 0.5802\n",
      "Epoch 18, Batch 21, loss_ca: 1.8845, adv_loss: 0.5428\n",
      "Epoch 18, Batch 22, loss_ca: 1.8887, adv_loss: 0.5462\n",
      "Epoch 18, Batch 23, loss_ca: 1.7977, adv_loss: 0.5329\n",
      "Epoch 18, Batch 24, loss_ca: 2.1045, adv_loss: 0.5332\n",
      "Epoch 18, Batch 25, loss_ca: 2.0684, adv_loss: 0.5002\n",
      "Epoch 18, Batch 26, loss_ca: 2.2658, adv_loss: 0.4823\n",
      "Epoch 18, Batch 27, loss_ca: 2.1221, adv_loss: 0.4757\n",
      "Epoch 18, Batch 28, loss_ca: 1.9936, adv_loss: 0.4793\n",
      "Epoch 18, Batch 29, loss_ca: 1.9504, adv_loss: 0.4776\n",
      "Epoch 18, Batch 30, loss_ca: 2.0534, adv_loss: 0.4860\n",
      "Epoch 18, Batch 31, loss_ca: 2.0207, adv_loss: 0.4970\n",
      "Epoch 18, Batch 32, loss_ca: 1.8793, adv_loss: 0.5036\n",
      "Epoch 18, Batch 33, loss_ca: 1.9008, adv_loss: 0.5530\n",
      "Epoch 18, Batch 34, loss_ca: 2.1106, adv_loss: 0.5544\n",
      "Epoch 18, Batch 35, loss_ca: 2.0077, adv_loss: 0.5686\n",
      "Epoch 18, Batch 36, loss_ca: 2.0253, adv_loss: 0.5789\n",
      "Epoch 18, Batch 37, loss_ca: 2.0306, adv_loss: 0.5936\n",
      "Epoch 18, Batch 38, loss_ca: 2.0558, adv_loss: 0.5923\n",
      "Epoch 18, Batch 39, loss_ca: 2.0988, adv_loss: 0.6033\n",
      "Epoch 18, Batch 40, loss_ca: 2.0798, adv_loss: 0.6068\n",
      "Epoch 18, Batch 41, loss_ca: 2.2285, adv_loss: 0.6123\n",
      "Epoch 18, Batch 42, loss_ca: 2.1979, adv_loss: 0.6164\n",
      "Epoch 18, Batch 43, loss_ca: 2.1002, adv_loss: 0.6228\n",
      "Epoch 18, Batch 44, loss_ca: 1.9801, adv_loss: 0.6220\n",
      "Epoch 18, Batch 45, loss_ca: 1.8790, adv_loss: 0.6106\n",
      "Epoch 18, Batch 46, loss_ca: 1.8740, adv_loss: 0.6045\n",
      "Epoch 18, Batch 47, loss_ca: 1.8625, adv_loss: 0.5841\n",
      "Epoch 18, Batch 48, loss_ca: 1.8525, adv_loss: 0.5877\n",
      "Epoch 18, Batch 49, loss_ca: 1.8570, adv_loss: 0.5967\n",
      "Epoch 18, Batch 50, loss_ca: 1.8762, adv_loss: 0.5889\n",
      "Epoch 18, Batch 51, loss_ca: 1.8898, adv_loss: 0.5954\n",
      "Epoch 18, Batch 52, loss_ca: 1.8649, adv_loss: 0.5755\n",
      "Epoch 18, Batch 53, loss_ca: 1.8425, adv_loss: 0.5540\n",
      "Epoch 18, Batch 54, loss_ca: 1.7555, adv_loss: 0.5236\n",
      "Epoch 18, Batch 55, loss_ca: 1.7259, adv_loss: 0.5159\n",
      "Epoch 18, Batch 56, loss_ca: 1.7031, adv_loss: 0.5125\n",
      "Epoch 18, Batch 57, loss_ca: 1.7068, adv_loss: 0.5353\n",
      "Epoch 18, Batch 58, loss_ca: 1.9184, adv_loss: 0.5975\n",
      "Epoch 18, Batch 59, loss_ca: 1.9406, adv_loss: 0.5434\n",
      "Epoch 18, Batch 60, loss_ca: 1.8763, adv_loss: 0.5251\n",
      "Epoch 18, Batch 61, loss_ca: 1.8101, adv_loss: 0.5617\n",
      "Epoch 18, Batch 62, loss_ca: 1.9261, adv_loss: 0.5139\n",
      "Epoch 18, Batch 63, loss_ca: 2.0073, adv_loss: 0.5421\n",
      "Epoch 18, Batch 64, loss_ca: 1.9229, adv_loss: 0.5418\n",
      "Epoch 18, Batch 65, loss_ca: 1.8072, adv_loss: 0.5256\n",
      "Epoch 18, Batch 66, loss_ca: 1.8387, adv_loss: 0.5628\n",
      "Epoch 18, Batch 67, loss_ca: 1.8341, adv_loss: 0.5650\n",
      "Epoch 18, Batch 68, loss_ca: 1.7958, adv_loss: 0.5649\n",
      "Epoch 18, Batch 69, loss_ca: 1.8405, adv_loss: 0.5176\n",
      "Epoch 18, Batch 70, loss_ca: 1.8386, adv_loss: 0.5219\n",
      "Epoch 18, Batch 71, loss_ca: 1.8696, adv_loss: 0.5201\n",
      "Epoch 18, Batch 72, loss_ca: 1.8912, adv_loss: 0.5051\n",
      "Epoch 18, Batch 73, loss_ca: 1.8332, adv_loss: 0.4981\n",
      "Epoch 18, Batch 74, loss_ca: 1.7732, adv_loss: 0.4986\n",
      "Epoch 18, Batch 75, loss_ca: 1.6707, adv_loss: 0.4823\n",
      "Epoch 18, Batch 76, loss_ca: 1.6293, adv_loss: 0.4841\n",
      "Epoch 18, Batch 77, loss_ca: 1.6714, adv_loss: 0.4830\n",
      "Epoch 18, Batch 78, loss_ca: 1.6722, adv_loss: 0.4711\n",
      "Epoch 18, Batch 79, loss_ca: 1.7001, adv_loss: 0.4887\n",
      "Epoch 18, Batch 80, loss_ca: 1.6771, adv_loss: 0.4822\n",
      "Epoch 18, Batch 81, loss_ca: 1.7092, adv_loss: 0.5051\n",
      "Epoch 18, Batch 82, loss_ca: 1.7779, adv_loss: 0.4913\n",
      "Epoch 18, Batch 83, loss_ca: 1.7956, adv_loss: 0.4927\n",
      "Epoch 18, Batch 84, loss_ca: 1.7839, adv_loss: 0.4933\n",
      "Epoch 18, Batch 85, loss_ca: 1.7489, adv_loss: 0.4752\n",
      "Epoch 18, Batch 86, loss_ca: 1.8671, adv_loss: 0.4868\n",
      "Epoch 18, Batch 87, loss_ca: 1.9090, adv_loss: 0.4891\n",
      "Epoch 18, Batch 88, loss_ca: 1.8474, adv_loss: 0.4887\n",
      "Epoch 18, Batch 89, loss_ca: 1.8612, adv_loss: 0.5006\n",
      "Epoch 18, Batch 90, loss_ca: 1.7451, adv_loss: 0.5118\n",
      "Epoch 18, Batch 91, loss_ca: 1.8693, adv_loss: 0.5133\n",
      "Epoch 18, Batch 92, loss_ca: 1.8226, adv_loss: 0.5135\n",
      "Epoch 18, Batch 93, loss_ca: 1.9240, adv_loss: 0.5287\n",
      "Epoch 18, Batch 94, loss_ca: 1.8358, adv_loss: 0.5428\n",
      "Epoch 18, Batch 95, loss_ca: 1.8634, adv_loss: 0.5490\n",
      "Epoch 18, Batch 96, loss_ca: 1.9334, adv_loss: 0.5520\n",
      "Epoch 18, Batch 97, loss_ca: 1.8984, adv_loss: 0.5496\n",
      "Epoch 18, Batch 98, loss_ca: 1.8704, adv_loss: 0.5507\n",
      "Epoch 18, Batch 99, loss_ca: 1.8463, adv_loss: 0.5314\n",
      "Epoch 18, Batch 100, loss_ca: 1.9076, adv_loss: 0.5202\n",
      "Epoch 18, Batch 101, loss_ca: 2.0213, adv_loss: 0.5325\n",
      "Epoch 18, Batch 102, loss_ca: 1.9776, adv_loss: 0.5374\n",
      "Epoch 18, Batch 103, loss_ca: 1.9553, adv_loss: 0.5431\n",
      "Epoch 18, Batch 104, loss_ca: 2.0164, adv_loss: 0.5524\n",
      "Epoch 18, Batch 105, loss_ca: 2.0363, adv_loss: 0.5632\n",
      "Epoch 18, Batch 106, loss_ca: 1.9708, adv_loss: 0.5884\n",
      "Epoch 18, Batch 107, loss_ca: 2.0847, adv_loss: 0.5903\n",
      "Epoch 18, Batch 108, loss_ca: 2.0507, adv_loss: 0.5916\n",
      "Epoch 18, Batch 109, loss_ca: 2.0545, adv_loss: 0.5927\n",
      "Epoch 18, Batch 110, loss_ca: 2.0116, adv_loss: 0.5977\n",
      "Epoch 18, Batch 111, loss_ca: 1.9551, adv_loss: 0.5942\n",
      "Epoch 18, Batch 112, loss_ca: 1.9406, adv_loss: 0.5868\n",
      "Epoch 18, Batch 113, loss_ca: 1.8230, adv_loss: 0.5737\n",
      "Epoch 18, Batch 114, loss_ca: 1.8404, adv_loss: 0.5567\n",
      "Epoch 18, Batch 115, loss_ca: 1.7787, adv_loss: 0.5440\n",
      "Epoch 18, Batch 116, loss_ca: 1.7768, adv_loss: 0.5319\n",
      "Epoch 18, Batch 117, loss_ca: 1.8231, adv_loss: 0.5241\n",
      "Epoch 18, Batch 118, loss_ca: 1.8440, adv_loss: 0.5388\n",
      "Epoch 18, Batch 119, loss_ca: 1.8034, adv_loss: 0.5293\n",
      "Epoch 18, Batch 120, loss_ca: 1.7532, adv_loss: 0.5236\n",
      "Epoch 18, Batch 121, loss_ca: 1.9182, adv_loss: 0.5189\n",
      "Epoch 18, Batch 122, loss_ca: 1.9199, adv_loss: 0.5000\n",
      "Epoch 18, Batch 123, loss_ca: 1.8907, adv_loss: 0.5007\n",
      "Epoch 18, Batch 124, loss_ca: 1.8868, adv_loss: 0.5139\n",
      "Epoch 18, Batch 125, loss_ca: 1.8836, adv_loss: 0.5041\n",
      "Epoch 18, Batch 126, loss_ca: 1.9779, adv_loss: 0.5360\n",
      "Epoch 18, Batch 127, loss_ca: 2.0405, adv_loss: 0.5871\n",
      "Epoch 18, Batch 128, loss_ca: 2.0496, adv_loss: 0.5853\n",
      "Epoch 18, Batch 129, loss_ca: 1.9991, adv_loss: 0.5661\n",
      "Epoch 18, Batch 130, loss_ca: 1.9045, adv_loss: 0.5524\n",
      "Epoch 18, Batch 131, loss_ca: 1.9302, adv_loss: 0.5730\n",
      "Epoch 18, Batch 132, loss_ca: 1.9415, adv_loss: 0.5878\n",
      "Epoch 18, Batch 133, loss_ca: 1.9215, adv_loss: 0.6013\n",
      "Epoch 18, Batch 134, loss_ca: 1.9134, adv_loss: 0.5986\n",
      "Epoch 18, Batch 135, loss_ca: 1.8912, adv_loss: 0.6004\n",
      "Epoch 18, Batch 136, loss_ca: 1.9561, adv_loss: 0.5778\n",
      "Epoch 18, Batch 137, loss_ca: 1.7825, adv_loss: 0.5557\n",
      "Epoch 18, Batch 138, loss_ca: 1.7220, adv_loss: 0.5422\n",
      "Epoch 18, Batch 139, loss_ca: 1.6695, adv_loss: 0.5298\n",
      "Epoch 18, Batch 140, loss_ca: 1.6692, adv_loss: 0.5220\n",
      "Epoch 18, Batch 141, loss_ca: 1.7166, adv_loss: 0.5274\n",
      "Epoch 18, Batch 142, loss_ca: 1.8262, adv_loss: 0.5169\n",
      "Epoch 18, Batch 143, loss_ca: 1.7565, adv_loss: 0.5238\n",
      "Epoch 18, Batch 144, loss_ca: 1.7697, adv_loss: 0.5234\n",
      "Epoch 18, Batch 145, loss_ca: 1.7735, adv_loss: 0.5396\n",
      "Epoch 18, Batch 146, loss_ca: 1.8754, adv_loss: 0.5293\n",
      "Epoch 18, Batch 147, loss_ca: 1.9445, adv_loss: 0.5339\n",
      "Epoch 18, Batch 148, loss_ca: 1.9975, adv_loss: 0.5385\n",
      "Epoch 18, Batch 149, loss_ca: 1.8924, adv_loss: 0.5469\n",
      "Epoch 18, Batch 150, loss_ca: 1.8714, adv_loss: 0.5482\n",
      "Epoch 18, Batch 151, loss_ca: 1.7803, adv_loss: 0.5382\n",
      "Epoch 18, Batch 152, loss_ca: 1.6880, adv_loss: 0.5204\n",
      "Epoch 18, Batch 153, loss_ca: 1.8119, adv_loss: 0.5193\n",
      "Epoch 18, Batch 154, loss_ca: 1.7710, adv_loss: 0.4573\n",
      "Epoch 18, Batch 155, loss_ca: 1.8071, adv_loss: 0.4687\n",
      "Epoch 18, Batch 156, loss_ca: 1.8271, adv_loss: 0.4601\n",
      "Epoch 18, Batch 157, loss_ca: 1.7936, adv_loss: 0.4622\n",
      "Epoch 18, Batch 158, loss_ca: 1.9758, adv_loss: 0.4577\n",
      "Epoch 18, Batch 159, loss_ca: 2.2381, adv_loss: 0.5076\n",
      "Epoch 18, Batch 160, loss_ca: 2.0953, adv_loss: 0.5391\n",
      "Epoch 18, Batch 161, loss_ca: 1.8861, adv_loss: 0.4345\n",
      "Epoch 18, Batch 162, loss_ca: 2.2150, adv_loss: 0.5070\n",
      "Epoch 18, Batch 163, loss_ca: 2.3469, adv_loss: 0.4747\n",
      "Epoch 18, Batch 164, loss_ca: 2.3629, adv_loss: 0.5146\n",
      "Epoch 18, Batch 165, loss_ca: 2.4627, adv_loss: 0.5703\n",
      "Epoch 18, Batch 166, loss_ca: 2.4001, adv_loss: 0.5809\n",
      "Epoch 18, Batch 167, loss_ca: 2.3475, adv_loss: 0.5130\n",
      "Epoch 18, Batch 168, loss_ca: 2.5672, adv_loss: 0.6374\n",
      "Epoch 18, Batch 169, loss_ca: 2.2206, adv_loss: 0.6311\n",
      "Epoch 18, Batch 170, loss_ca: 2.0834, adv_loss: 0.5309\n",
      "Epoch 18, Batch 171, loss_ca: 2.1459, adv_loss: 0.5338\n",
      "Epoch 18, Batch 172, loss_ca: 2.2468, adv_loss: 0.5529\n",
      "Epoch 18, Batch 173, loss_ca: 2.0560, adv_loss: 0.5595\n",
      "Epoch 18, Batch 174, loss_ca: 1.9485, adv_loss: 0.5113\n",
      "Epoch 18, Batch 175, loss_ca: 1.9250, adv_loss: 0.5373\n",
      "Epoch 18, Batch 176, loss_ca: 1.8858, adv_loss: 0.5064\n",
      "Epoch 18, Batch 177, loss_ca: 1.8644, adv_loss: 0.5119\n",
      "Epoch 18, Batch 178, loss_ca: 1.9595, adv_loss: 0.5082\n",
      "Epoch 18, Batch 179, loss_ca: 1.9589, adv_loss: 0.5222\n",
      "Epoch 18, Batch 180, loss_ca: 1.8129, adv_loss: 0.5066\n",
      "Epoch 18, Batch 181, loss_ca: 1.8834, adv_loss: 0.5308\n",
      "Epoch 18, Batch 182, loss_ca: 1.7434, adv_loss: 0.5258\n",
      "Epoch 18, Batch 183, loss_ca: 1.9384, adv_loss: 0.5436\n",
      "Epoch 18, Batch 184, loss_ca: 1.9630, adv_loss: 0.5495\n",
      "Epoch 18, Batch 185, loss_ca: 1.8274, adv_loss: 0.5512\n",
      "Epoch 18, Batch 186, loss_ca: 1.7710, adv_loss: 0.5357\n",
      "Epoch 18, Batch 187, loss_ca: 1.7399, adv_loss: 0.5270\n",
      "Epoch 18, Batch 188, loss_ca: 1.7022, adv_loss: 0.5301\n",
      "Epoch 18, Batch 189, loss_ca: 1.8032, adv_loss: 0.5250\n",
      "Epoch 18, Batch 190, loss_ca: 2.0693, adv_loss: 0.5568\n",
      "Epoch 18, Batch 191, loss_ca: 2.0708, adv_loss: 0.5148\n",
      "Epoch 18, Batch 192, loss_ca: 2.1837, adv_loss: 0.5492\n",
      "Epoch 18, Batch 193, loss_ca: 2.0820, adv_loss: 0.5298\n",
      "Epoch 18, Batch 194, loss_ca: 2.0240, adv_loss: 0.5392\n",
      "Epoch 18, Batch 195, loss_ca: 2.0615, adv_loss: 0.5320\n",
      "Epoch 18, Batch 196, loss_ca: 2.8678, adv_loss: 0.6456\n",
      "Epoch 18, Batch 197, loss_ca: 2.3398, adv_loss: 0.5617\n",
      "Epoch 18, Batch 198, loss_ca: 2.2196, adv_loss: 0.5538\n",
      "Epoch 18, Batch 199, loss_ca: 2.0446, adv_loss: 0.5307\n",
      "Epoch 18, Batch 200, loss_ca: 1.8493, adv_loss: 0.5332\n",
      "Epoch 18, Batch 201, loss_ca: 2.0208, adv_loss: 0.5330\n",
      "Epoch 18, Batch 202, loss_ca: 2.0138, adv_loss: 0.5330\n",
      "Epoch 18, Batch 203, loss_ca: 2.4016, adv_loss: 0.5668\n",
      "Epoch 18, Batch 204, loss_ca: 1.9297, adv_loss: 0.5451\n",
      "Epoch 18, Batch 205, loss_ca: 2.3479, adv_loss: 0.6271\n",
      "Epoch 18, Batch 206, loss_ca: 2.0506, adv_loss: 0.5773\n",
      "Epoch 18, Batch 207, loss_ca: 2.2641, adv_loss: 0.5661\n",
      "Epoch 18, Batch 208, loss_ca: 2.0291, adv_loss: 0.5531\n",
      "Epoch 18, Batch 209, loss_ca: 1.8670, adv_loss: 0.5928\n",
      "Epoch 18, Batch 210, loss_ca: 1.8991, adv_loss: 0.5528\n",
      "Epoch 18, Batch 211, loss_ca: 2.0149, adv_loss: 0.6021\n",
      "Epoch 18, Batch 212, loss_ca: 1.6974, adv_loss: 0.6075\n",
      "Epoch 18, Batch 213, loss_ca: 2.0503, adv_loss: 0.5872\n",
      "Epoch 18, Batch 214, loss_ca: 2.3282, adv_loss: 0.6290\n",
      "Epoch 18, Batch 215, loss_ca: 1.8846, adv_loss: 0.5757\n",
      "Epoch 18, Batch 216, loss_ca: 1.8646, adv_loss: 0.5662\n",
      "Epoch 18, Batch 217, loss_ca: 1.7758, adv_loss: 0.5920\n",
      "Epoch 18, Batch 218, loss_ca: 1.7169, adv_loss: 0.6353\n",
      "Epoch 18, Batch 219, loss_ca: 2.1767, adv_loss: 0.5877\n",
      "Epoch 18, Batch 220, loss_ca: 1.9227, adv_loss: 0.5882\n",
      "Epoch 18, Batch 221, loss_ca: 1.7361, adv_loss: 0.5209\n",
      "Epoch 18, Batch 222, loss_ca: 1.7020, adv_loss: 0.5334\n",
      "Epoch 18, Batch 223, loss_ca: 1.7049, adv_loss: 0.5072\n",
      "Epoch 18, Batch 224, loss_ca: 1.8304, adv_loss: 0.5259\n",
      "Epoch 18, Batch 225, loss_ca: 1.8356, adv_loss: 0.4992\n",
      "Epoch 18, Batch 226, loss_ca: 1.7597, adv_loss: 0.4931\n",
      "Epoch 18, Batch 227, loss_ca: 1.8179, adv_loss: 0.4948\n",
      "Epoch 18, Batch 228, loss_ca: 1.7706, adv_loss: 0.5305\n",
      "Epoch 19, Batch 19, loss_ca: 1.9561, adv_loss: 0.5110\n",
      "Epoch 19, Batch 20, loss_ca: 1.9604, adv_loss: 0.5869\n",
      "Epoch 19, Batch 21, loss_ca: 1.9066, adv_loss: 0.5502\n",
      "Epoch 19, Batch 22, loss_ca: 1.8967, adv_loss: 0.5414\n",
      "Epoch 19, Batch 23, loss_ca: 1.8607, adv_loss: 0.5529\n",
      "Epoch 19, Batch 24, loss_ca: 2.1167, adv_loss: 0.5091\n",
      "Epoch 19, Batch 25, loss_ca: 2.1037, adv_loss: 0.5099\n",
      "Epoch 19, Batch 26, loss_ca: 2.2367, adv_loss: 0.4935\n",
      "Epoch 19, Batch 27, loss_ca: 2.1151, adv_loss: 0.4695\n",
      "Epoch 19, Batch 28, loss_ca: 1.9691, adv_loss: 0.4780\n",
      "Epoch 19, Batch 29, loss_ca: 1.9335, adv_loss: 0.4764\n",
      "Epoch 19, Batch 30, loss_ca: 2.0340, adv_loss: 0.4825\n",
      "Epoch 19, Batch 31, loss_ca: 1.9624, adv_loss: 0.4678\n",
      "Epoch 19, Batch 32, loss_ca: 1.8141, adv_loss: 0.4689\n",
      "Epoch 19, Batch 33, loss_ca: 1.8833, adv_loss: 0.5458\n",
      "Epoch 19, Batch 34, loss_ca: 2.1761, adv_loss: 0.5246\n",
      "Epoch 19, Batch 35, loss_ca: 2.0480, adv_loss: 0.5131\n",
      "Epoch 19, Batch 36, loss_ca: 2.0289, adv_loss: 0.5501\n",
      "Epoch 19, Batch 37, loss_ca: 2.0270, adv_loss: 0.5564\n",
      "Epoch 19, Batch 38, loss_ca: 2.0520, adv_loss: 0.5713\n",
      "Epoch 19, Batch 39, loss_ca: 2.0590, adv_loss: 0.5978\n",
      "Epoch 19, Batch 40, loss_ca: 2.0370, adv_loss: 0.6249\n",
      "Epoch 19, Batch 41, loss_ca: 2.1424, adv_loss: 0.6287\n",
      "Epoch 19, Batch 42, loss_ca: 2.1287, adv_loss: 0.6404\n",
      "Epoch 19, Batch 43, loss_ca: 2.0589, adv_loss: 0.6334\n",
      "Epoch 19, Batch 44, loss_ca: 1.9438, adv_loss: 0.6064\n",
      "Epoch 19, Batch 45, loss_ca: 1.8499, adv_loss: 0.5882\n",
      "Epoch 19, Batch 46, loss_ca: 1.8090, adv_loss: 0.5668\n",
      "Epoch 19, Batch 47, loss_ca: 1.9612, adv_loss: 0.5241\n",
      "Epoch 19, Batch 48, loss_ca: 1.9404, adv_loss: 0.5649\n",
      "Epoch 19, Batch 49, loss_ca: 1.8581, adv_loss: 0.5873\n",
      "Epoch 19, Batch 50, loss_ca: 1.8269, adv_loss: 0.5988\n",
      "Epoch 19, Batch 51, loss_ca: 1.8430, adv_loss: 0.6159\n",
      "Epoch 19, Batch 52, loss_ca: 1.9084, adv_loss: 0.6061\n",
      "Epoch 19, Batch 53, loss_ca: 1.9301, adv_loss: 0.6182\n",
      "Epoch 19, Batch 54, loss_ca: 1.8597, adv_loss: 0.6110\n",
      "Epoch 19, Batch 55, loss_ca: 1.8370, adv_loss: 0.6015\n",
      "Epoch 19, Batch 56, loss_ca: 1.7757, adv_loss: 0.5939\n",
      "Epoch 19, Batch 57, loss_ca: 1.7304, adv_loss: 0.5821\n",
      "Epoch 19, Batch 58, loss_ca: 1.8441, adv_loss: 0.5579\n",
      "Epoch 19, Batch 59, loss_ca: 1.8248, adv_loss: 0.5478\n",
      "Epoch 19, Batch 60, loss_ca: 1.8227, adv_loss: 0.5257\n",
      "Epoch 19, Batch 61, loss_ca: 1.8222, adv_loss: 0.5264\n",
      "Epoch 19, Batch 62, loss_ca: 1.9217, adv_loss: 0.5078\n",
      "Epoch 19, Batch 63, loss_ca: 1.9474, adv_loss: 0.5530\n",
      "Epoch 19, Batch 64, loss_ca: 1.9135, adv_loss: 0.5597\n",
      "Epoch 19, Batch 65, loss_ca: 1.8539, adv_loss: 0.5694\n",
      "Epoch 19, Batch 66, loss_ca: 1.8018, adv_loss: 0.5808\n",
      "Epoch 19, Batch 67, loss_ca: 1.8141, adv_loss: 0.5413\n",
      "Epoch 19, Batch 68, loss_ca: 1.8803, adv_loss: 0.5392\n",
      "Epoch 19, Batch 69, loss_ca: 1.8129, adv_loss: 0.5257\n",
      "Epoch 19, Batch 70, loss_ca: 1.8003, adv_loss: 0.5274\n",
      "Epoch 19, Batch 71, loss_ca: 1.8151, adv_loss: 0.5176\n",
      "Epoch 19, Batch 72, loss_ca: 1.7290, adv_loss: 0.5075\n",
      "Epoch 19, Batch 73, loss_ca: 1.7088, adv_loss: 0.5164\n",
      "Epoch 19, Batch 74, loss_ca: 1.7154, adv_loss: 0.5170\n",
      "Epoch 19, Batch 75, loss_ca: 1.6841, adv_loss: 0.5156\n",
      "Epoch 19, Batch 76, loss_ca: 1.6557, adv_loss: 0.5197\n",
      "Epoch 19, Batch 77, loss_ca: 1.6457, adv_loss: 0.4943\n",
      "Epoch 19, Batch 78, loss_ca: 1.6791, adv_loss: 0.4858\n",
      "Epoch 19, Batch 79, loss_ca: 1.6783, adv_loss: 0.4904\n",
      "Epoch 19, Batch 80, loss_ca: 1.6685, adv_loss: 0.4836\n",
      "Epoch 19, Batch 81, loss_ca: 1.6395, adv_loss: 0.5055\n",
      "Epoch 19, Batch 82, loss_ca: 1.7017, adv_loss: 0.4978\n",
      "Epoch 19, Batch 83, loss_ca: 1.7078, adv_loss: 0.4945\n",
      "Epoch 19, Batch 84, loss_ca: 1.7325, adv_loss: 0.4975\n",
      "Epoch 19, Batch 85, loss_ca: 1.8030, adv_loss: 0.4668\n",
      "Epoch 19, Batch 86, loss_ca: 1.9248, adv_loss: 0.4632\n",
      "Epoch 19, Batch 87, loss_ca: 1.8710, adv_loss: 0.4597\n",
      "Epoch 19, Batch 88, loss_ca: 1.8428, adv_loss: 0.4608\n",
      "Epoch 19, Batch 89, loss_ca: 1.8478, adv_loss: 0.4756\n",
      "Epoch 19, Batch 90, loss_ca: 1.7461, adv_loss: 0.4783\n",
      "Epoch 19, Batch 91, loss_ca: 1.8710, adv_loss: 0.4799\n",
      "Epoch 19, Batch 92, loss_ca: 1.7928, adv_loss: 0.4817\n",
      "Epoch 19, Batch 93, loss_ca: 1.8904, adv_loss: 0.4851\n",
      "Epoch 19, Batch 94, loss_ca: 1.8422, adv_loss: 0.5122\n",
      "Epoch 19, Batch 95, loss_ca: 1.9190, adv_loss: 0.5091\n",
      "Epoch 19, Batch 96, loss_ca: 1.9772, adv_loss: 0.5231\n",
      "Epoch 19, Batch 97, loss_ca: 1.9450, adv_loss: 0.5258\n",
      "Epoch 19, Batch 98, loss_ca: 1.9281, adv_loss: 0.5311\n",
      "Epoch 19, Batch 99, loss_ca: 1.8304, adv_loss: 0.5228\n",
      "Epoch 19, Batch 100, loss_ca: 1.9305, adv_loss: 0.4821\n",
      "Epoch 19, Batch 101, loss_ca: 1.9660, adv_loss: 0.4969\n",
      "Epoch 19, Batch 102, loss_ca: 1.8917, adv_loss: 0.5165\n",
      "Epoch 19, Batch 103, loss_ca: 1.9068, adv_loss: 0.5293\n",
      "Epoch 19, Batch 104, loss_ca: 1.9497, adv_loss: 0.5458\n",
      "Epoch 19, Batch 105, loss_ca: 2.0107, adv_loss: 0.5694\n",
      "Epoch 19, Batch 106, loss_ca: 2.0099, adv_loss: 0.5787\n",
      "Epoch 19, Batch 107, loss_ca: 2.1122, adv_loss: 0.5827\n",
      "Epoch 19, Batch 108, loss_ca: 1.9820, adv_loss: 0.5821\n",
      "Epoch 19, Batch 109, loss_ca: 2.1157, adv_loss: 0.5880\n",
      "Epoch 19, Batch 110, loss_ca: 2.0680, adv_loss: 0.5834\n",
      "Epoch 19, Batch 111, loss_ca: 2.0435, adv_loss: 0.5861\n",
      "Epoch 19, Batch 112, loss_ca: 2.0058, adv_loss: 0.5766\n",
      "Epoch 19, Batch 113, loss_ca: 1.8335, adv_loss: 0.5710\n",
      "Epoch 19, Batch 114, loss_ca: 1.8369, adv_loss: 0.5661\n",
      "Epoch 19, Batch 115, loss_ca: 1.7556, adv_loss: 0.5495\n",
      "Epoch 19, Batch 116, loss_ca: 1.7876, adv_loss: 0.5532\n",
      "Epoch 19, Batch 117, loss_ca: 1.8116, adv_loss: 0.5434\n",
      "Epoch 19, Batch 118, loss_ca: 1.8420, adv_loss: 0.5423\n",
      "Epoch 19, Batch 119, loss_ca: 1.8226, adv_loss: 0.5382\n",
      "Epoch 19, Batch 120, loss_ca: 1.7875, adv_loss: 0.5336\n",
      "Epoch 19, Batch 121, loss_ca: 1.9313, adv_loss: 0.4979\n",
      "Epoch 19, Batch 122, loss_ca: 1.9242, adv_loss: 0.4937\n",
      "Epoch 19, Batch 123, loss_ca: 1.9192, adv_loss: 0.5170\n",
      "Epoch 19, Batch 124, loss_ca: 1.8773, adv_loss: 0.5184\n",
      "Epoch 19, Batch 125, loss_ca: 1.8988, adv_loss: 0.5249\n",
      "Epoch 19, Batch 126, loss_ca: 1.9828, adv_loss: 0.5737\n",
      "Epoch 19, Batch 127, loss_ca: 2.0586, adv_loss: 0.5360\n",
      "Epoch 19, Batch 128, loss_ca: 2.0080, adv_loss: 0.5302\n",
      "Epoch 19, Batch 129, loss_ca: 2.0185, adv_loss: 0.5301\n",
      "Epoch 19, Batch 130, loss_ca: 1.9945, adv_loss: 0.5168\n",
      "Epoch 19, Batch 131, loss_ca: 1.9717, adv_loss: 0.5305\n",
      "Epoch 19, Batch 132, loss_ca: 1.9224, adv_loss: 0.5242\n",
      "Epoch 19, Batch 133, loss_ca: 1.8959, adv_loss: 0.5331\n",
      "Epoch 19, Batch 134, loss_ca: 1.8809, adv_loss: 0.5505\n",
      "Epoch 19, Batch 135, loss_ca: 1.8813, adv_loss: 0.5624\n",
      "Epoch 19, Batch 136, loss_ca: 1.9236, adv_loss: 0.5787\n",
      "Epoch 19, Batch 137, loss_ca: 1.8008, adv_loss: 0.5653\n",
      "Epoch 19, Batch 138, loss_ca: 1.7402, adv_loss: 0.5519\n",
      "Epoch 19, Batch 139, loss_ca: 1.6727, adv_loss: 0.5524\n",
      "Epoch 19, Batch 140, loss_ca: 1.7243, adv_loss: 0.5455\n",
      "Epoch 19, Batch 141, loss_ca: 1.7611, adv_loss: 0.5438\n",
      "Epoch 19, Batch 142, loss_ca: 1.8430, adv_loss: 0.5391\n",
      "Epoch 19, Batch 143, loss_ca: 1.7377, adv_loss: 0.5458\n",
      "Epoch 19, Batch 144, loss_ca: 1.7363, adv_loss: 0.5604\n",
      "Epoch 19, Batch 145, loss_ca: 1.7782, adv_loss: 0.5785\n",
      "Epoch 19, Batch 146, loss_ca: 1.7751, adv_loss: 0.5674\n",
      "Epoch 19, Batch 147, loss_ca: 1.9840, adv_loss: 0.5578\n",
      "Epoch 19, Batch 148, loss_ca: 2.0406, adv_loss: 0.5580\n",
      "Epoch 19, Batch 149, loss_ca: 1.9597, adv_loss: 0.5468\n",
      "Epoch 19, Batch 150, loss_ca: 1.9176, adv_loss: 0.5526\n",
      "Epoch 19, Batch 151, loss_ca: 1.8258, adv_loss: 0.5468\n",
      "Epoch 19, Batch 152, loss_ca: 1.7499, adv_loss: 0.5336\n",
      "Epoch 19, Batch 153, loss_ca: 1.8375, adv_loss: 0.5287\n",
      "Epoch 19, Batch 154, loss_ca: 1.8702, adv_loss: 0.5041\n",
      "Epoch 19, Batch 155, loss_ca: 1.9865, adv_loss: 0.5417\n",
      "Epoch 19, Batch 156, loss_ca: 1.9060, adv_loss: 0.5235\n",
      "Epoch 19, Batch 157, loss_ca: 1.8148, adv_loss: 0.4966\n",
      "Epoch 19, Batch 158, loss_ca: 1.9588, adv_loss: 0.4865\n",
      "Epoch 19, Batch 159, loss_ca: 2.2078, adv_loss: 0.5200\n",
      "Epoch 19, Batch 160, loss_ca: 2.2142, adv_loss: 0.5066\n",
      "Epoch 19, Batch 161, loss_ca: 1.9070, adv_loss: 0.4955\n",
      "Epoch 19, Batch 162, loss_ca: 2.3724, adv_loss: 0.5203\n",
      "Epoch 19, Batch 163, loss_ca: 2.2051, adv_loss: 0.5154\n",
      "Epoch 19, Batch 164, loss_ca: 2.2506, adv_loss: 0.5272\n",
      "Epoch 19, Batch 165, loss_ca: 2.3518, adv_loss: 0.5596\n",
      "Epoch 19, Batch 166, loss_ca: 2.1701, adv_loss: 0.5533\n",
      "Epoch 19, Batch 167, loss_ca: 2.1136, adv_loss: 0.4600\n",
      "Epoch 19, Batch 168, loss_ca: 2.3811, adv_loss: 0.4632\n",
      "Epoch 19, Batch 169, loss_ca: 2.3165, adv_loss: 0.5694\n",
      "Epoch 19, Batch 170, loss_ca: 2.7576, adv_loss: 0.6004\n",
      "Epoch 19, Batch 171, loss_ca: 2.3939, adv_loss: 0.5602\n",
      "Epoch 19, Batch 172, loss_ca: 2.7881, adv_loss: 0.6267\n",
      "Epoch 19, Batch 173, loss_ca: 2.0761, adv_loss: 0.4810\n",
      "Epoch 19, Batch 174, loss_ca: 2.1863, adv_loss: 0.4941\n",
      "Epoch 19, Batch 175, loss_ca: 1.8850, adv_loss: 0.4627\n",
      "Epoch 19, Batch 176, loss_ca: 1.8163, adv_loss: 0.4318\n",
      "Epoch 19, Batch 177, loss_ca: 1.7439, adv_loss: 0.4576\n",
      "Epoch 19, Batch 178, loss_ca: 1.9375, adv_loss: 0.4762\n",
      "Epoch 19, Batch 179, loss_ca: 1.9348, adv_loss: 0.4878\n",
      "Epoch 19, Batch 180, loss_ca: 1.9522, adv_loss: 0.5061\n",
      "Epoch 19, Batch 181, loss_ca: 1.8123, adv_loss: 0.4861\n",
      "Epoch 19, Batch 182, loss_ca: 1.7961, adv_loss: 0.5088\n",
      "Epoch 19, Batch 183, loss_ca: 1.8993, adv_loss: 0.4887\n",
      "Epoch 19, Batch 184, loss_ca: 1.7696, adv_loss: 0.5067\n",
      "Epoch 19, Batch 185, loss_ca: 1.7145, adv_loss: 0.5178\n",
      "Epoch 19, Batch 186, loss_ca: 1.7099, adv_loss: 0.5145\n",
      "Epoch 19, Batch 187, loss_ca: 1.7327, adv_loss: 0.5124\n",
      "Epoch 19, Batch 188, loss_ca: 1.6731, adv_loss: 0.5159\n",
      "Epoch 19, Batch 189, loss_ca: 1.7784, adv_loss: 0.5260\n",
      "Epoch 19, Batch 190, loss_ca: 2.0111, adv_loss: 0.5363\n",
      "Epoch 19, Batch 191, loss_ca: 2.0534, adv_loss: 0.5456\n",
      "Epoch 19, Batch 192, loss_ca: 2.1701, adv_loss: 0.5655\n",
      "Epoch 19, Batch 193, loss_ca: 2.0218, adv_loss: 0.5635\n",
      "Epoch 19, Batch 194, loss_ca: 2.0336, adv_loss: 0.5933\n",
      "Epoch 19, Batch 195, loss_ca: 1.9532, adv_loss: 0.5864\n",
      "Epoch 19, Batch 196, loss_ca: 2.4237, adv_loss: 0.6008\n",
      "Epoch 19, Batch 197, loss_ca: 2.0875, adv_loss: 0.5621\n",
      "Epoch 19, Batch 198, loss_ca: 2.6364, adv_loss: 0.5653\n",
      "Epoch 19, Batch 199, loss_ca: 2.4865, adv_loss: 0.5660\n",
      "Epoch 19, Batch 200, loss_ca: 2.3043, adv_loss: 0.5662\n",
      "Epoch 19, Batch 201, loss_ca: 2.1430, adv_loss: 0.5700\n",
      "Epoch 19, Batch 202, loss_ca: 2.0059, adv_loss: 0.5327\n",
      "Epoch 19, Batch 203, loss_ca: 2.3423, adv_loss: 0.6064\n",
      "Epoch 19, Batch 204, loss_ca: 1.8734, adv_loss: 0.4608\n",
      "Epoch 19, Batch 205, loss_ca: 2.1744, adv_loss: 0.5353\n",
      "Epoch 19, Batch 206, loss_ca: 1.9667, adv_loss: 0.4980\n",
      "Epoch 19, Batch 207, loss_ca: 2.0919, adv_loss: 0.5198\n",
      "Epoch 19, Batch 208, loss_ca: 2.0613, adv_loss: 0.5226\n",
      "Epoch 19, Batch 209, loss_ca: 1.9544, adv_loss: 0.5204\n",
      "Epoch 19, Batch 210, loss_ca: 2.0891, adv_loss: 0.5385\n",
      "Epoch 19, Batch 211, loss_ca: 2.1342, adv_loss: 0.5458\n",
      "Epoch 19, Batch 212, loss_ca: 1.7172, adv_loss: 0.5214\n",
      "Epoch 19, Batch 213, loss_ca: 2.1541, adv_loss: 0.5841\n",
      "Epoch 19, Batch 214, loss_ca: 2.0078, adv_loss: 0.5674\n",
      "Epoch 19, Batch 215, loss_ca: 1.8420, adv_loss: 0.5758\n",
      "Epoch 19, Batch 216, loss_ca: 1.9268, adv_loss: 0.6268\n",
      "Epoch 19, Batch 217, loss_ca: 1.8203, adv_loss: 0.6143\n",
      "Epoch 19, Batch 218, loss_ca: 2.1385, adv_loss: 0.6529\n",
      "Epoch 19, Batch 219, loss_ca: 2.5520, adv_loss: 0.6293\n",
      "Epoch 19, Batch 220, loss_ca: 2.2115, adv_loss: 0.5700\n",
      "Epoch 19, Batch 221, loss_ca: 2.0445, adv_loss: 0.5435\n",
      "Epoch 19, Batch 222, loss_ca: 1.8192, adv_loss: 0.5330\n",
      "Epoch 19, Batch 223, loss_ca: 1.7729, adv_loss: 0.5431\n",
      "Epoch 19, Batch 224, loss_ca: 1.7822, adv_loss: 0.5323\n",
      "Epoch 19, Batch 225, loss_ca: 1.7025, adv_loss: 0.4960\n",
      "Epoch 19, Batch 226, loss_ca: 1.7529, adv_loss: 0.5041\n",
      "Epoch 19, Batch 227, loss_ca: 1.8276, adv_loss: 0.4882\n",
      "Epoch 19, Batch 228, loss_ca: 1.8293, adv_loss: 0.5121\n",
      "Epoch 20, Batch 20, loss_ca: 2.0357, adv_loss: 0.5389\n",
      "Epoch 20, Batch 21, loss_ca: 1.9884, adv_loss: 0.5532\n",
      "Epoch 20, Batch 22, loss_ca: 1.9689, adv_loss: 0.5642\n",
      "Epoch 20, Batch 23, loss_ca: 1.9576, adv_loss: 0.5645\n",
      "Epoch 20, Batch 24, loss_ca: 2.2063, adv_loss: 0.5491\n",
      "Epoch 20, Batch 25, loss_ca: 2.1384, adv_loss: 0.5323\n",
      "Epoch 20, Batch 26, loss_ca: 2.2262, adv_loss: 0.5378\n",
      "Epoch 20, Batch 27, loss_ca: 2.1146, adv_loss: 0.5226\n",
      "Epoch 20, Batch 28, loss_ca: 2.0024, adv_loss: 0.5309\n",
      "Epoch 20, Batch 29, loss_ca: 1.9572, adv_loss: 0.5274\n",
      "Epoch 20, Batch 30, loss_ca: 2.0307, adv_loss: 0.5234\n",
      "Epoch 20, Batch 31, loss_ca: 2.0353, adv_loss: 0.5234\n",
      "Epoch 20, Batch 32, loss_ca: 1.8873, adv_loss: 0.5272\n",
      "Epoch 20, Batch 33, loss_ca: 1.9501, adv_loss: 0.5670\n",
      "Epoch 20, Batch 34, loss_ca: 2.1078, adv_loss: 0.5310\n",
      "Epoch 20, Batch 35, loss_ca: 1.9714, adv_loss: 0.5374\n",
      "Epoch 20, Batch 36, loss_ca: 1.9645, adv_loss: 0.5578\n",
      "Epoch 20, Batch 37, loss_ca: 1.9875, adv_loss: 0.5537\n",
      "Epoch 20, Batch 38, loss_ca: 1.9551, adv_loss: 0.5599\n",
      "Epoch 20, Batch 39, loss_ca: 1.9960, adv_loss: 0.5790\n",
      "Epoch 20, Batch 40, loss_ca: 2.0265, adv_loss: 0.5894\n",
      "Epoch 20, Batch 41, loss_ca: 2.1919, adv_loss: 0.5832\n",
      "Epoch 20, Batch 42, loss_ca: 2.1729, adv_loss: 0.5881\n",
      "Epoch 20, Batch 43, loss_ca: 2.1005, adv_loss: 0.5838\n",
      "Epoch 20, Batch 44, loss_ca: 2.0180, adv_loss: 0.5742\n",
      "Epoch 20, Batch 45, loss_ca: 1.9505, adv_loss: 0.5719\n",
      "Epoch 20, Batch 46, loss_ca: 1.8762, adv_loss: 0.5778\n",
      "Epoch 20, Batch 47, loss_ca: 1.9024, adv_loss: 0.5952\n",
      "Epoch 20, Batch 48, loss_ca: 1.9351, adv_loss: 0.6199\n",
      "Epoch 20, Batch 49, loss_ca: 1.9035, adv_loss: 0.6126\n",
      "Epoch 20, Batch 50, loss_ca: 1.8693, adv_loss: 0.6125\n",
      "Epoch 20, Batch 51, loss_ca: 1.8330, adv_loss: 0.5861\n",
      "Epoch 20, Batch 52, loss_ca: 1.8843, adv_loss: 0.5670\n",
      "Epoch 20, Batch 53, loss_ca: 1.9345, adv_loss: 0.5692\n",
      "Epoch 20, Batch 54, loss_ca: 1.8841, adv_loss: 0.5563\n",
      "Epoch 20, Batch 55, loss_ca: 1.8424, adv_loss: 0.5445\n",
      "Epoch 20, Batch 56, loss_ca: 1.7641, adv_loss: 0.5562\n",
      "Epoch 20, Batch 57, loss_ca: 1.7316, adv_loss: 0.5419\n",
      "Epoch 20, Batch 58, loss_ca: 1.8006, adv_loss: 0.5636\n",
      "Epoch 20, Batch 59, loss_ca: 1.8475, adv_loss: 0.5468\n",
      "Epoch 20, Batch 60, loss_ca: 1.8289, adv_loss: 0.5206\n",
      "Epoch 20, Batch 61, loss_ca: 1.8328, adv_loss: 0.5240\n",
      "Epoch 20, Batch 62, loss_ca: 1.8435, adv_loss: 0.5080\n",
      "Epoch 20, Batch 63, loss_ca: 1.9579, adv_loss: 0.4944\n",
      "Epoch 20, Batch 64, loss_ca: 1.9619, adv_loss: 0.5201\n",
      "Epoch 20, Batch 65, loss_ca: 1.9699, adv_loss: 0.4895\n",
      "Epoch 20, Batch 66, loss_ca: 1.9762, adv_loss: 0.4894\n",
      "Epoch 20, Batch 67, loss_ca: 1.8579, adv_loss: 0.5691\n",
      "Epoch 20, Batch 68, loss_ca: 1.7941, adv_loss: 0.5363\n",
      "Epoch 20, Batch 69, loss_ca: 1.8371, adv_loss: 0.5474\n",
      "Epoch 20, Batch 70, loss_ca: 1.8758, adv_loss: 0.5422\n",
      "Epoch 20, Batch 71, loss_ca: 1.7272, adv_loss: 0.5202\n",
      "Epoch 20, Batch 72, loss_ca: 1.7212, adv_loss: 0.5161\n",
      "Epoch 20, Batch 73, loss_ca: 1.6578, adv_loss: 0.5109\n",
      "Epoch 20, Batch 74, loss_ca: 1.6884, adv_loss: 0.5193\n",
      "Epoch 20, Batch 75, loss_ca: 1.6821, adv_loss: 0.5140\n",
      "Epoch 20, Batch 76, loss_ca: 1.6491, adv_loss: 0.5123\n",
      "Epoch 20, Batch 77, loss_ca: 1.6425, adv_loss: 0.5282\n",
      "Epoch 20, Batch 78, loss_ca: 1.6681, adv_loss: 0.5291\n",
      "Epoch 20, Batch 79, loss_ca: 1.6716, adv_loss: 0.5360\n",
      "Epoch 20, Batch 80, loss_ca: 1.6612, adv_loss: 0.5214\n",
      "Epoch 20, Batch 81, loss_ca: 1.6643, adv_loss: 0.5144\n",
      "Epoch 20, Batch 82, loss_ca: 1.7190, adv_loss: 0.5264\n",
      "Epoch 20, Batch 83, loss_ca: 1.7334, adv_loss: 0.5041\n",
      "Epoch 20, Batch 84, loss_ca: 1.7276, adv_loss: 0.5026\n",
      "Epoch 20, Batch 85, loss_ca: 1.7451, adv_loss: 0.4970\n",
      "Epoch 20, Batch 86, loss_ca: 1.8210, adv_loss: 0.4844\n",
      "Epoch 20, Batch 87, loss_ca: 1.7910, adv_loss: 0.4773\n",
      "Epoch 20, Batch 88, loss_ca: 1.7629, adv_loss: 0.4772\n",
      "Epoch 20, Batch 89, loss_ca: 1.8202, adv_loss: 0.4737\n",
      "Epoch 20, Batch 90, loss_ca: 1.7399, adv_loss: 0.4824\n",
      "Epoch 20, Batch 91, loss_ca: 1.8556, adv_loss: 0.4899\n",
      "Epoch 20, Batch 92, loss_ca: 1.8161, adv_loss: 0.4804\n",
      "Epoch 20, Batch 93, loss_ca: 1.9035, adv_loss: 0.4977\n",
      "Epoch 20, Batch 94, loss_ca: 1.8357, adv_loss: 0.4936\n",
      "Epoch 20, Batch 95, loss_ca: 1.8477, adv_loss: 0.5126\n",
      "Epoch 20, Batch 96, loss_ca: 1.9382, adv_loss: 0.5256\n",
      "Epoch 20, Batch 97, loss_ca: 1.8716, adv_loss: 0.5223\n",
      "Epoch 20, Batch 98, loss_ca: 1.8469, adv_loss: 0.5205\n",
      "Epoch 20, Batch 99, loss_ca: 1.7453, adv_loss: 0.4894\n",
      "Epoch 20, Batch 100, loss_ca: 1.9223, adv_loss: 0.4722\n",
      "Epoch 20, Batch 101, loss_ca: 1.9561, adv_loss: 0.4944\n",
      "Epoch 20, Batch 102, loss_ca: 1.9091, adv_loss: 0.5147\n",
      "Epoch 20, Batch 103, loss_ca: 1.9101, adv_loss: 0.5114\n",
      "Epoch 20, Batch 104, loss_ca: 1.9273, adv_loss: 0.5097\n",
      "Epoch 20, Batch 105, loss_ca: 1.9797, adv_loss: 0.5416\n",
      "Epoch 20, Batch 106, loss_ca: 1.9560, adv_loss: 0.5783\n",
      "Epoch 20, Batch 107, loss_ca: 2.0755, adv_loss: 0.6028\n",
      "Epoch 20, Batch 108, loss_ca: 1.9612, adv_loss: 0.5955\n",
      "Epoch 20, Batch 109, loss_ca: 2.0970, adv_loss: 0.6094\n",
      "Epoch 20, Batch 110, loss_ca: 2.0446, adv_loss: 0.5980\n",
      "Epoch 20, Batch 111, loss_ca: 2.0155, adv_loss: 0.5975\n",
      "Epoch 20, Batch 112, loss_ca: 2.0065, adv_loss: 0.5923\n",
      "Epoch 20, Batch 113, loss_ca: 1.8483, adv_loss: 0.5755\n",
      "Epoch 20, Batch 114, loss_ca: 1.8166, adv_loss: 0.5727\n",
      "Epoch 20, Batch 115, loss_ca: 1.7471, adv_loss: 0.5680\n",
      "Epoch 20, Batch 116, loss_ca: 1.7738, adv_loss: 0.5626\n",
      "Epoch 20, Batch 117, loss_ca: 1.8385, adv_loss: 0.5601\n",
      "Epoch 20, Batch 118, loss_ca: 1.8194, adv_loss: 0.5396\n",
      "Epoch 20, Batch 119, loss_ca: 1.8281, adv_loss: 0.5315\n",
      "Epoch 20, Batch 120, loss_ca: 1.7808, adv_loss: 0.5124\n",
      "Epoch 20, Batch 121, loss_ca: 1.8923, adv_loss: 0.4857\n",
      "Epoch 20, Batch 122, loss_ca: 1.9461, adv_loss: 0.4951\n",
      "Epoch 20, Batch 123, loss_ca: 1.9022, adv_loss: 0.5031\n",
      "Epoch 20, Batch 124, loss_ca: 1.9263, adv_loss: 0.5191\n",
      "Epoch 20, Batch 125, loss_ca: 1.9154, adv_loss: 0.5246\n",
      "Epoch 20, Batch 126, loss_ca: 2.1670, adv_loss: 0.6003\n",
      "Epoch 20, Batch 127, loss_ca: 2.1074, adv_loss: 0.5238\n",
      "Epoch 20, Batch 128, loss_ca: 2.0261, adv_loss: 0.5075\n",
      "Epoch 20, Batch 129, loss_ca: 1.9943, adv_loss: 0.5042\n",
      "Epoch 20, Batch 130, loss_ca: 1.9591, adv_loss: 0.5314\n",
      "Epoch 20, Batch 131, loss_ca: 1.9933, adv_loss: 0.5317\n",
      "Epoch 20, Batch 132, loss_ca: 1.8838, adv_loss: 0.5388\n",
      "Epoch 20, Batch 133, loss_ca: 1.9491, adv_loss: 0.5559\n",
      "Epoch 20, Batch 134, loss_ca: 1.9627, adv_loss: 0.5648\n",
      "Epoch 20, Batch 135, loss_ca: 1.9298, adv_loss: 0.5689\n",
      "Epoch 20, Batch 136, loss_ca: 1.9567, adv_loss: 0.5556\n",
      "Epoch 20, Batch 137, loss_ca: 1.8287, adv_loss: 0.5401\n",
      "Epoch 20, Batch 138, loss_ca: 1.7376, adv_loss: 0.5301\n",
      "Epoch 20, Batch 139, loss_ca: 1.7169, adv_loss: 0.5247\n",
      "Epoch 20, Batch 140, loss_ca: 1.7341, adv_loss: 0.5195\n",
      "Epoch 20, Batch 141, loss_ca: 1.7811, adv_loss: 0.5119\n",
      "Epoch 20, Batch 142, loss_ca: 1.8920, adv_loss: 0.5190\n",
      "Epoch 20, Batch 143, loss_ca: 1.8450, adv_loss: 0.5323\n",
      "Epoch 20, Batch 144, loss_ca: 1.7892, adv_loss: 0.5341\n",
      "Epoch 20, Batch 145, loss_ca: 1.7935, adv_loss: 0.5498\n",
      "Epoch 20, Batch 146, loss_ca: 1.7494, adv_loss: 0.5375\n",
      "Epoch 20, Batch 147, loss_ca: 1.8494, adv_loss: 0.5140\n",
      "Epoch 20, Batch 148, loss_ca: 1.9079, adv_loss: 0.5188\n",
      "Epoch 20, Batch 149, loss_ca: 1.7531, adv_loss: 0.5121\n",
      "Epoch 20, Batch 150, loss_ca: 1.6727, adv_loss: 0.5238\n",
      "Epoch 20, Batch 151, loss_ca: 1.6253, adv_loss: 0.5173\n",
      "Epoch 20, Batch 152, loss_ca: 1.6461, adv_loss: 0.5210\n",
      "Epoch 20, Batch 153, loss_ca: 1.8261, adv_loss: 0.5164\n",
      "Epoch 20, Batch 154, loss_ca: 1.8470, adv_loss: 0.5221\n",
      "Epoch 20, Batch 155, loss_ca: 1.8584, adv_loss: 0.5197\n",
      "Epoch 20, Batch 156, loss_ca: 1.8846, adv_loss: 0.5221\n",
      "Epoch 20, Batch 157, loss_ca: 1.8910, adv_loss: 0.5456\n",
      "Epoch 20, Batch 158, loss_ca: 2.1115, adv_loss: 0.5848\n",
      "Epoch 20, Batch 159, loss_ca: 2.2752, adv_loss: 0.6135\n",
      "Epoch 20, Batch 160, loss_ca: 2.2671, adv_loss: 0.5479\n",
      "Epoch 20, Batch 161, loss_ca: 2.1823, adv_loss: 0.5401\n",
      "Epoch 20, Batch 162, loss_ca: 2.3402, adv_loss: 0.5405\n",
      "Epoch 20, Batch 163, loss_ca: 2.2751, adv_loss: 0.4728\n",
      "Epoch 20, Batch 164, loss_ca: 2.3864, adv_loss: 0.5247\n",
      "Epoch 20, Batch 165, loss_ca: 2.5780, adv_loss: 0.5414\n",
      "Epoch 20, Batch 166, loss_ca: 2.3111, adv_loss: 0.5398\n",
      "Epoch 20, Batch 167, loss_ca: 2.2259, adv_loss: 0.5055\n",
      "Epoch 20, Batch 168, loss_ca: 2.3480, adv_loss: 0.5540\n",
      "Epoch 20, Batch 169, loss_ca: 2.2664, adv_loss: 0.5894\n",
      "Epoch 20, Batch 170, loss_ca: 2.2501, adv_loss: 0.5196\n",
      "Epoch 20, Batch 171, loss_ca: 2.2958, adv_loss: 0.5544\n",
      "Epoch 20, Batch 172, loss_ca: 2.3288, adv_loss: 0.5699\n",
      "Epoch 20, Batch 173, loss_ca: 2.1993, adv_loss: 0.5541\n",
      "Epoch 20, Batch 174, loss_ca: 2.1251, adv_loss: 0.5057\n",
      "Epoch 20, Batch 175, loss_ca: 2.1591, adv_loss: 0.5247\n",
      "Epoch 20, Batch 176, loss_ca: 2.1097, adv_loss: 0.5330\n",
      "Epoch 20, Batch 177, loss_ca: 1.8262, adv_loss: 0.5049\n",
      "Epoch 20, Batch 178, loss_ca: 1.8523, adv_loss: 0.4866\n",
      "Epoch 20, Batch 179, loss_ca: 1.8543, adv_loss: 0.4811\n",
      "Epoch 20, Batch 180, loss_ca: 1.7505, adv_loss: 0.4913\n",
      "Epoch 20, Batch 181, loss_ca: 1.7920, adv_loss: 0.5184\n",
      "Epoch 20, Batch 182, loss_ca: 1.8185, adv_loss: 0.5066\n",
      "Epoch 20, Batch 183, loss_ca: 2.0254, adv_loss: 0.5284\n",
      "Epoch 20, Batch 184, loss_ca: 1.8980, adv_loss: 0.5304\n",
      "Epoch 20, Batch 185, loss_ca: 1.7445, adv_loss: 0.5179\n",
      "Epoch 20, Batch 186, loss_ca: 1.7346, adv_loss: 0.5119\n",
      "Epoch 20, Batch 187, loss_ca: 1.7331, adv_loss: 0.4988\n",
      "Epoch 20, Batch 188, loss_ca: 1.6822, adv_loss: 0.4973\n",
      "Epoch 20, Batch 189, loss_ca: 1.9140, adv_loss: 0.5130\n",
      "Epoch 20, Batch 190, loss_ca: 1.9721, adv_loss: 0.5287\n",
      "Epoch 20, Batch 191, loss_ca: 2.0864, adv_loss: 0.5373\n",
      "Epoch 20, Batch 192, loss_ca: 2.1400, adv_loss: 0.5474\n",
      "Epoch 20, Batch 193, loss_ca: 2.2333, adv_loss: 0.5490\n",
      "Epoch 20, Batch 194, loss_ca: 1.8859, adv_loss: 0.5603\n",
      "Epoch 20, Batch 195, loss_ca: 2.0003, adv_loss: 0.5684\n",
      "Epoch 20, Batch 196, loss_ca: 2.3766, adv_loss: 0.5696\n",
      "Epoch 20, Batch 197, loss_ca: 2.0211, adv_loss: 0.5127\n",
      "Epoch 20, Batch 198, loss_ca: 2.3449, adv_loss: 0.4203\n",
      "Epoch 20, Batch 199, loss_ca: 1.9478, adv_loss: 0.4556\n",
      "Epoch 20, Batch 200, loss_ca: 1.8179, adv_loss: 0.4650\n",
      "Epoch 20, Batch 201, loss_ca: 2.2477, adv_loss: 0.5524\n",
      "Epoch 20, Batch 202, loss_ca: 2.4971, adv_loss: 0.6114\n",
      "Epoch 20, Batch 203, loss_ca: 2.3887, adv_loss: 0.5908\n",
      "Epoch 20, Batch 204, loss_ca: 1.8733, adv_loss: 0.4419\n",
      "Epoch 20, Batch 205, loss_ca: 2.1678, adv_loss: 0.4566\n",
      "Epoch 20, Batch 206, loss_ca: 1.9404, adv_loss: 0.4995\n",
      "Epoch 20, Batch 207, loss_ca: 2.1442, adv_loss: 0.5049\n",
      "Epoch 20, Batch 208, loss_ca: 1.9789, adv_loss: 0.5327\n",
      "Epoch 20, Batch 209, loss_ca: 2.2862, adv_loss: 0.5578\n",
      "Epoch 20, Batch 210, loss_ca: 2.3925, adv_loss: 0.5744\n",
      "Epoch 20, Batch 211, loss_ca: 2.1825, adv_loss: 0.6136\n",
      "Epoch 20, Batch 212, loss_ca: 1.9969, adv_loss: 0.6043\n",
      "Epoch 20, Batch 213, loss_ca: 2.0658, adv_loss: 0.6036\n",
      "Epoch 20, Batch 214, loss_ca: 1.8721, adv_loss: 0.5709\n",
      "Epoch 20, Batch 215, loss_ca: 1.7733, adv_loss: 0.5602\n",
      "Epoch 20, Batch 216, loss_ca: 1.8975, adv_loss: 0.5605\n",
      "Epoch 20, Batch 217, loss_ca: 1.7169, adv_loss: 0.5733\n",
      "Epoch 20, Batch 218, loss_ca: 1.7509, adv_loss: 0.6154\n",
      "Epoch 20, Batch 219, loss_ca: 2.2420, adv_loss: 0.5631\n",
      "Epoch 20, Batch 220, loss_ca: 1.9059, adv_loss: 0.5610\n",
      "Epoch 20, Batch 221, loss_ca: 1.8113, adv_loss: 0.5034\n",
      "Epoch 20, Batch 222, loss_ca: 1.7457, adv_loss: 0.4951\n",
      "Epoch 20, Batch 223, loss_ca: 1.8502, adv_loss: 0.4730\n",
      "Epoch 20, Batch 224, loss_ca: 1.7909, adv_loss: 0.4981\n",
      "Epoch 20, Batch 225, loss_ca: 1.6941, adv_loss: 0.5019\n",
      "Epoch 20, Batch 226, loss_ca: 1.7642, adv_loss: 0.4995\n",
      "Epoch 20, Batch 227, loss_ca: 1.7608, adv_loss: 0.5388\n",
      "Epoch 20, Batch 228, loss_ca: 1.7842, adv_loss: 0.5475\n",
      "Epoch 21, Batch 21, loss_ca: 1.9777, adv_loss: 0.5705\n",
      "Epoch 21, Batch 22, loss_ca: 1.9749, adv_loss: 0.5634\n",
      "Epoch 21, Batch 23, loss_ca: 1.8869, adv_loss: 0.5353\n",
      "Epoch 21, Batch 24, loss_ca: 2.1859, adv_loss: 0.5527\n",
      "Epoch 21, Batch 25, loss_ca: 2.1037, adv_loss: 0.5595\n",
      "Epoch 21, Batch 26, loss_ca: 2.2044, adv_loss: 0.5252\n",
      "Epoch 21, Batch 27, loss_ca: 2.1005, adv_loss: 0.5151\n",
      "Epoch 21, Batch 28, loss_ca: 1.9464, adv_loss: 0.5222\n",
      "Epoch 21, Batch 29, loss_ca: 1.9635, adv_loss: 0.5263\n",
      "Epoch 21, Batch 30, loss_ca: 2.0546, adv_loss: 0.5198\n",
      "Epoch 21, Batch 31, loss_ca: 2.0256, adv_loss: 0.5152\n",
      "Epoch 21, Batch 32, loss_ca: 1.9280, adv_loss: 0.5022\n",
      "Epoch 21, Batch 33, loss_ca: 1.8945, adv_loss: 0.5193\n",
      "Epoch 21, Batch 34, loss_ca: 2.1742, adv_loss: 0.4949\n",
      "Epoch 21, Batch 35, loss_ca: 2.0292, adv_loss: 0.5155\n",
      "Epoch 21, Batch 36, loss_ca: 2.0125, adv_loss: 0.5391\n",
      "Epoch 21, Batch 37, loss_ca: 2.0303, adv_loss: 0.5320\n",
      "Epoch 21, Batch 38, loss_ca: 2.0612, adv_loss: 0.5504\n",
      "Epoch 21, Batch 39, loss_ca: 2.0811, adv_loss: 0.5843\n",
      "Epoch 21, Batch 40, loss_ca: 2.0761, adv_loss: 0.5819\n",
      "Epoch 21, Batch 41, loss_ca: 2.1799, adv_loss: 0.5740\n",
      "Epoch 21, Batch 42, loss_ca: 2.1379, adv_loss: 0.5814\n",
      "Epoch 21, Batch 43, loss_ca: 2.0341, adv_loss: 0.5710\n",
      "Epoch 21, Batch 44, loss_ca: 1.9035, adv_loss: 0.5671\n",
      "Epoch 21, Batch 45, loss_ca: 1.8689, adv_loss: 0.5596\n",
      "Epoch 21, Batch 46, loss_ca: 1.8728, adv_loss: 0.5848\n",
      "Epoch 21, Batch 47, loss_ca: 1.8679, adv_loss: 0.5821\n",
      "Epoch 21, Batch 48, loss_ca: 1.8807, adv_loss: 0.5946\n",
      "Epoch 21, Batch 49, loss_ca: 1.8709, adv_loss: 0.5983\n",
      "Epoch 21, Batch 50, loss_ca: 1.8703, adv_loss: 0.6042\n",
      "Epoch 21, Batch 51, loss_ca: 1.8977, adv_loss: 0.6096\n",
      "Epoch 21, Batch 52, loss_ca: 1.9399, adv_loss: 0.5962\n",
      "Epoch 21, Batch 53, loss_ca: 1.8555, adv_loss: 0.5907\n",
      "Epoch 21, Batch 54, loss_ca: 1.7884, adv_loss: 0.5810\n",
      "Epoch 21, Batch 55, loss_ca: 1.7435, adv_loss: 0.5633\n",
      "Epoch 21, Batch 56, loss_ca: 1.7275, adv_loss: 0.5487\n",
      "Epoch 21, Batch 57, loss_ca: 1.7229, adv_loss: 0.5448\n",
      "Epoch 21, Batch 58, loss_ca: 1.8380, adv_loss: 0.5567\n",
      "Epoch 21, Batch 59, loss_ca: 1.8733, adv_loss: 0.5459\n",
      "Epoch 21, Batch 60, loss_ca: 1.8333, adv_loss: 0.5522\n",
      "Epoch 21, Batch 61, loss_ca: 1.8121, adv_loss: 0.5622\n",
      "Epoch 21, Batch 62, loss_ca: 1.8591, adv_loss: 0.5403\n",
      "Epoch 21, Batch 63, loss_ca: 1.8817, adv_loss: 0.5501\n",
      "Epoch 21, Batch 64, loss_ca: 1.8755, adv_loss: 0.5565\n",
      "Epoch 21, Batch 65, loss_ca: 1.8185, adv_loss: 0.5462\n",
      "Epoch 21, Batch 66, loss_ca: 1.8112, adv_loss: 0.5510\n",
      "Epoch 21, Batch 67, loss_ca: 2.2748, adv_loss: 0.6195\n",
      "Epoch 21, Batch 68, loss_ca: 1.7994, adv_loss: 0.5279\n",
      "Epoch 21, Batch 69, loss_ca: 1.7769, adv_loss: 0.5294\n",
      "Epoch 21, Batch 70, loss_ca: 1.7835, adv_loss: 0.5751\n",
      "Epoch 21, Batch 71, loss_ca: 1.7333, adv_loss: 0.5418\n",
      "Epoch 21, Batch 72, loss_ca: 1.7461, adv_loss: 0.5118\n",
      "Epoch 21, Batch 73, loss_ca: 1.6948, adv_loss: 0.5167\n",
      "Epoch 21, Batch 74, loss_ca: 1.6880, adv_loss: 0.5213\n",
      "Epoch 21, Batch 75, loss_ca: 1.6603, adv_loss: 0.5244\n",
      "Epoch 21, Batch 76, loss_ca: 1.6416, adv_loss: 0.5301\n",
      "Epoch 21, Batch 77, loss_ca: 1.6436, adv_loss: 0.5277\n",
      "Epoch 21, Batch 78, loss_ca: 1.6896, adv_loss: 0.5257\n",
      "Epoch 21, Batch 79, loss_ca: 1.6858, adv_loss: 0.5412\n",
      "Epoch 21, Batch 80, loss_ca: 1.7201, adv_loss: 0.5404\n",
      "Epoch 21, Batch 81, loss_ca: 1.7188, adv_loss: 0.5380\n",
      "Epoch 21, Batch 82, loss_ca: 1.7154, adv_loss: 0.5391\n",
      "Epoch 21, Batch 83, loss_ca: 1.7096, adv_loss: 0.5136\n",
      "Epoch 21, Batch 84, loss_ca: 1.7083, adv_loss: 0.5000\n",
      "Epoch 21, Batch 85, loss_ca: 1.7623, adv_loss: 0.4826\n",
      "Epoch 21, Batch 86, loss_ca: 1.8191, adv_loss: 0.4754\n",
      "Epoch 21, Batch 87, loss_ca: 1.7870, adv_loss: 0.4839\n",
      "Epoch 21, Batch 88, loss_ca: 1.7751, adv_loss: 0.4807\n",
      "Epoch 21, Batch 89, loss_ca: 1.8323, adv_loss: 0.4673\n",
      "Epoch 21, Batch 90, loss_ca: 1.7245, adv_loss: 0.4597\n",
      "Epoch 21, Batch 91, loss_ca: 1.8559, adv_loss: 0.4695\n",
      "Epoch 21, Batch 92, loss_ca: 1.8294, adv_loss: 0.4767\n",
      "Epoch 21, Batch 93, loss_ca: 1.9209, adv_loss: 0.4867\n",
      "Epoch 21, Batch 94, loss_ca: 1.8285, adv_loss: 0.4898\n",
      "Epoch 21, Batch 95, loss_ca: 1.8441, adv_loss: 0.4840\n",
      "Epoch 21, Batch 96, loss_ca: 1.9273, adv_loss: 0.5137\n",
      "Epoch 21, Batch 97, loss_ca: 1.9022, adv_loss: 0.5260\n",
      "Epoch 21, Batch 98, loss_ca: 1.8512, adv_loss: 0.5267\n",
      "Epoch 21, Batch 99, loss_ca: 1.7607, adv_loss: 0.5131\n",
      "Epoch 21, Batch 100, loss_ca: 1.9072, adv_loss: 0.5061\n",
      "Epoch 21, Batch 101, loss_ca: 1.9629, adv_loss: 0.5176\n",
      "Epoch 21, Batch 102, loss_ca: 1.8864, adv_loss: 0.5193\n",
      "Epoch 21, Batch 103, loss_ca: 1.8969, adv_loss: 0.5364\n",
      "Epoch 21, Batch 104, loss_ca: 1.8711, adv_loss: 0.5403\n",
      "Epoch 21, Batch 105, loss_ca: 1.9538, adv_loss: 0.5450\n",
      "Epoch 21, Batch 106, loss_ca: 1.9517, adv_loss: 0.5752\n",
      "Epoch 21, Batch 107, loss_ca: 2.0875, adv_loss: 0.5772\n",
      "Epoch 21, Batch 108, loss_ca: 1.9773, adv_loss: 0.5916\n",
      "Epoch 21, Batch 109, loss_ca: 2.0480, adv_loss: 0.5787\n",
      "Epoch 21, Batch 110, loss_ca: 1.9976, adv_loss: 0.5699\n",
      "Epoch 21, Batch 111, loss_ca: 1.9377, adv_loss: 0.5577\n",
      "Epoch 21, Batch 112, loss_ca: 1.9256, adv_loss: 0.5336\n",
      "Epoch 21, Batch 113, loss_ca: 1.7774, adv_loss: 0.5075\n",
      "Epoch 21, Batch 114, loss_ca: 1.8326, adv_loss: 0.4953\n",
      "Epoch 21, Batch 115, loss_ca: 1.7544, adv_loss: 0.4906\n",
      "Epoch 21, Batch 116, loss_ca: 1.9087, adv_loss: 0.5276\n",
      "Epoch 21, Batch 117, loss_ca: 1.8583, adv_loss: 0.5239\n",
      "Epoch 21, Batch 118, loss_ca: 1.7909, adv_loss: 0.5012\n",
      "Epoch 21, Batch 119, loss_ca: 1.7584, adv_loss: 0.5133\n",
      "Epoch 21, Batch 120, loss_ca: 1.7490, adv_loss: 0.5095\n",
      "Epoch 21, Batch 121, loss_ca: 1.9373, adv_loss: 0.4881\n",
      "Epoch 21, Batch 122, loss_ca: 2.0036, adv_loss: 0.4980\n",
      "Epoch 21, Batch 123, loss_ca: 2.0021, adv_loss: 0.5163\n",
      "Epoch 21, Batch 124, loss_ca: 1.9908, adv_loss: 0.5573\n",
      "Epoch 21, Batch 125, loss_ca: 1.9387, adv_loss: 0.5542\n",
      "Epoch 21, Batch 126, loss_ca: 1.9698, adv_loss: 0.5684\n",
      "Epoch 21, Batch 127, loss_ca: 2.0443, adv_loss: 0.5792\n",
      "Epoch 21, Batch 128, loss_ca: 2.0181, adv_loss: 0.5644\n",
      "Epoch 21, Batch 129, loss_ca: 2.0123, adv_loss: 0.5597\n",
      "Epoch 21, Batch 130, loss_ca: 1.9434, adv_loss: 0.5613\n",
      "Epoch 21, Batch 131, loss_ca: 1.8923, adv_loss: 0.5444\n",
      "Epoch 21, Batch 132, loss_ca: 1.9340, adv_loss: 0.5677\n",
      "Epoch 21, Batch 133, loss_ca: 1.9118, adv_loss: 0.5581\n",
      "Epoch 21, Batch 134, loss_ca: 1.8954, adv_loss: 0.5638\n",
      "Epoch 21, Batch 135, loss_ca: 1.8719, adv_loss: 0.5547\n",
      "Epoch 21, Batch 136, loss_ca: 1.8834, adv_loss: 0.5564\n",
      "Epoch 21, Batch 137, loss_ca: 1.7507, adv_loss: 0.5473\n",
      "Epoch 21, Batch 138, loss_ca: 1.7167, adv_loss: 0.5410\n",
      "Epoch 21, Batch 139, loss_ca: 1.6789, adv_loss: 0.5346\n",
      "Epoch 21, Batch 140, loss_ca: 1.6700, adv_loss: 0.5150\n",
      "Epoch 21, Batch 141, loss_ca: 1.7091, adv_loss: 0.5187\n",
      "Epoch 21, Batch 142, loss_ca: 1.8617, adv_loss: 0.5380\n",
      "Epoch 21, Batch 143, loss_ca: 1.8029, adv_loss: 0.5474\n",
      "Epoch 21, Batch 144, loss_ca: 1.7906, adv_loss: 0.5788\n",
      "Epoch 21, Batch 145, loss_ca: 1.7893, adv_loss: 0.5824\n",
      "Epoch 21, Batch 146, loss_ca: 1.7501, adv_loss: 0.5508\n",
      "Epoch 21, Batch 147, loss_ca: 1.8776, adv_loss: 0.5516\n",
      "Epoch 21, Batch 148, loss_ca: 1.9004, adv_loss: 0.5395\n",
      "Epoch 21, Batch 149, loss_ca: 1.7810, adv_loss: 0.4989\n",
      "Epoch 21, Batch 150, loss_ca: 1.7243, adv_loss: 0.5036\n",
      "Epoch 21, Batch 151, loss_ca: 1.6459, adv_loss: 0.4938\n",
      "Epoch 21, Batch 152, loss_ca: 1.6145, adv_loss: 0.4698\n",
      "Epoch 21, Batch 153, loss_ca: 1.7925, adv_loss: 0.4576\n",
      "Epoch 21, Batch 154, loss_ca: 1.7780, adv_loss: 0.4342\n",
      "Epoch 21, Batch 155, loss_ca: 1.8320, adv_loss: 0.4701\n",
      "Epoch 21, Batch 156, loss_ca: 1.8102, adv_loss: 0.4822\n",
      "Epoch 21, Batch 157, loss_ca: 1.8671, adv_loss: 0.5064\n",
      "Epoch 21, Batch 158, loss_ca: 2.0477, adv_loss: 0.5227\n",
      "Epoch 21, Batch 159, loss_ca: 2.3795, adv_loss: 0.5470\n",
      "Epoch 21, Batch 160, loss_ca: 2.2279, adv_loss: 0.5521\n",
      "Epoch 21, Batch 161, loss_ca: 1.9079, adv_loss: 0.5068\n",
      "Epoch 21, Batch 162, loss_ca: 2.2784, adv_loss: 0.5676\n",
      "Epoch 21, Batch 163, loss_ca: 2.1438, adv_loss: 0.5548\n",
      "Epoch 21, Batch 164, loss_ca: 2.5859, adv_loss: 0.5521\n",
      "Epoch 21, Batch 165, loss_ca: 2.3511, adv_loss: 0.5231\n",
      "Epoch 21, Batch 166, loss_ca: 2.2186, adv_loss: 0.5836\n",
      "Epoch 21, Batch 167, loss_ca: 2.1897, adv_loss: 0.5081\n",
      "Epoch 21, Batch 168, loss_ca: 2.2305, adv_loss: 0.5425\n",
      "Epoch 21, Batch 169, loss_ca: 2.1997, adv_loss: 0.5766\n",
      "Epoch 21, Batch 170, loss_ca: 2.2097, adv_loss: 0.5437\n",
      "Epoch 21, Batch 171, loss_ca: 2.3120, adv_loss: 0.5325\n",
      "Epoch 21, Batch 172, loss_ca: 2.2655, adv_loss: 0.5679\n",
      "Epoch 21, Batch 173, loss_ca: 2.1245, adv_loss: 0.5520\n",
      "Epoch 21, Batch 174, loss_ca: 1.9678, adv_loss: 0.5300\n",
      "Epoch 21, Batch 175, loss_ca: 1.9872, adv_loss: 0.5353\n",
      "Epoch 21, Batch 176, loss_ca: 1.9392, adv_loss: 0.5126\n",
      "Epoch 21, Batch 177, loss_ca: 1.7971, adv_loss: 0.5133\n",
      "Epoch 21, Batch 178, loss_ca: 1.9097, adv_loss: 0.4914\n",
      "Epoch 21, Batch 179, loss_ca: 1.8041, adv_loss: 0.4817\n",
      "Epoch 21, Batch 180, loss_ca: 1.7537, adv_loss: 0.4892\n",
      "Epoch 21, Batch 181, loss_ca: 1.8063, adv_loss: 0.4841\n",
      "Epoch 21, Batch 182, loss_ca: 1.6850, adv_loss: 0.4517\n",
      "Epoch 21, Batch 183, loss_ca: 1.9577, adv_loss: 0.4925\n",
      "Epoch 21, Batch 184, loss_ca: 1.9792, adv_loss: 0.4818\n",
      "Epoch 21, Batch 185, loss_ca: 1.7654, adv_loss: 0.4534\n",
      "Epoch 21, Batch 186, loss_ca: 1.7018, adv_loss: 0.4484\n",
      "Epoch 21, Batch 187, loss_ca: 1.7217, adv_loss: 0.4726\n",
      "Epoch 21, Batch 188, loss_ca: 1.6770, adv_loss: 0.4876\n",
      "Epoch 21, Batch 189, loss_ca: 1.8200, adv_loss: 0.5303\n",
      "Epoch 21, Batch 190, loss_ca: 2.1886, adv_loss: 0.5963\n",
      "Epoch 21, Batch 191, loss_ca: 2.2507, adv_loss: 0.6125\n",
      "Epoch 21, Batch 192, loss_ca: 2.6829, adv_loss: 0.6804\n",
      "Epoch 21, Batch 193, loss_ca: 1.9840, adv_loss: 0.5836\n",
      "Epoch 21, Batch 194, loss_ca: 1.9936, adv_loss: 0.5932\n",
      "Epoch 21, Batch 195, loss_ca: 1.8107, adv_loss: 0.5485\n",
      "Epoch 21, Batch 196, loss_ca: 2.3484, adv_loss: 0.5795\n",
      "Epoch 21, Batch 197, loss_ca: 2.0155, adv_loss: 0.5108\n",
      "Epoch 21, Batch 198, loss_ca: 1.9758, adv_loss: 0.5829\n",
      "Epoch 21, Batch 199, loss_ca: 2.0363, adv_loss: 0.5592\n",
      "Epoch 21, Batch 200, loss_ca: 1.8658, adv_loss: 0.5618\n",
      "Epoch 21, Batch 201, loss_ca: 2.1878, adv_loss: 0.4912\n",
      "Epoch 21, Batch 202, loss_ca: 2.0508, adv_loss: 0.5423\n",
      "Epoch 21, Batch 203, loss_ca: 2.4823, adv_loss: 0.5618\n",
      "Epoch 21, Batch 204, loss_ca: 2.1901, adv_loss: 0.5779\n",
      "Epoch 21, Batch 205, loss_ca: 2.2886, adv_loss: 0.5926\n",
      "Epoch 21, Batch 206, loss_ca: 2.2609, adv_loss: 0.5574\n",
      "Epoch 21, Batch 207, loss_ca: 2.2202, adv_loss: 0.5445\n",
      "Epoch 21, Batch 208, loss_ca: 2.1871, adv_loss: 0.5586\n",
      "Epoch 21, Batch 209, loss_ca: 1.9858, adv_loss: 0.5538\n",
      "Epoch 21, Batch 210, loss_ca: 2.0056, adv_loss: 0.5169\n",
      "Epoch 21, Batch 211, loss_ca: 1.9968, adv_loss: 0.5479\n",
      "Epoch 21, Batch 212, loss_ca: 1.7811, adv_loss: 0.5479\n",
      "Epoch 21, Batch 213, loss_ca: 2.0330, adv_loss: 0.5743\n",
      "Epoch 21, Batch 214, loss_ca: 1.9152, adv_loss: 0.5263\n",
      "Epoch 21, Batch 215, loss_ca: 1.8053, adv_loss: 0.5531\n",
      "Epoch 21, Batch 216, loss_ca: 1.9475, adv_loss: 0.5879\n",
      "Epoch 21, Batch 217, loss_ca: 1.8996, adv_loss: 0.5872\n",
      "Epoch 21, Batch 218, loss_ca: 1.9057, adv_loss: 0.5817\n",
      "Epoch 21, Batch 219, loss_ca: 2.1780, adv_loss: 0.5612\n",
      "Epoch 21, Batch 220, loss_ca: 1.9666, adv_loss: 0.5833\n",
      "Epoch 21, Batch 221, loss_ca: 1.8908, adv_loss: 0.5550\n",
      "Epoch 21, Batch 222, loss_ca: 1.8733, adv_loss: 0.5634\n",
      "Epoch 21, Batch 223, loss_ca: 1.8334, adv_loss: 0.5498\n",
      "Epoch 21, Batch 224, loss_ca: 1.6997, adv_loss: 0.5143\n",
      "Epoch 21, Batch 225, loss_ca: 1.6754, adv_loss: 0.5088\n",
      "Epoch 21, Batch 226, loss_ca: 1.7284, adv_loss: 0.4958\n",
      "Epoch 21, Batch 227, loss_ca: 1.8115, adv_loss: 0.4953\n",
      "Epoch 21, Batch 228, loss_ca: 1.7562, adv_loss: 0.5091\n",
      "Epoch 22, Batch 22, loss_ca: 1.9972, adv_loss: 0.5057\n",
      "Epoch 22, Batch 23, loss_ca: 1.8632, adv_loss: 0.5168\n",
      "Epoch 22, Batch 24, loss_ca: 2.1051, adv_loss: 0.5297\n",
      "Epoch 22, Batch 25, loss_ca: 2.0435, adv_loss: 0.5366\n",
      "Epoch 22, Batch 26, loss_ca: 2.1288, adv_loss: 0.5403\n",
      "Epoch 22, Batch 27, loss_ca: 2.0373, adv_loss: 0.5497\n",
      "Epoch 22, Batch 28, loss_ca: 1.9409, adv_loss: 0.5512\n",
      "Epoch 22, Batch 29, loss_ca: 1.9424, adv_loss: 0.5498\n",
      "Epoch 22, Batch 30, loss_ca: 1.9741, adv_loss: 0.5410\n",
      "Epoch 22, Batch 31, loss_ca: 1.9686, adv_loss: 0.5095\n",
      "Epoch 22, Batch 32, loss_ca: 1.8941, adv_loss: 0.5433\n",
      "Epoch 22, Batch 33, loss_ca: 1.8187, adv_loss: 0.5665\n",
      "Epoch 22, Batch 34, loss_ca: 2.1580, adv_loss: 0.5722\n",
      "Epoch 22, Batch 35, loss_ca: 2.0717, adv_loss: 0.5966\n",
      "Epoch 22, Batch 36, loss_ca: 2.0254, adv_loss: 0.6103\n",
      "Epoch 22, Batch 37, loss_ca: 2.0291, adv_loss: 0.5941\n",
      "Epoch 22, Batch 38, loss_ca: 2.0676, adv_loss: 0.5910\n",
      "Epoch 22, Batch 39, loss_ca: 2.0500, adv_loss: 0.6206\n",
      "Epoch 22, Batch 40, loss_ca: 2.0447, adv_loss: 0.5891\n",
      "Epoch 22, Batch 41, loss_ca: 2.1661, adv_loss: 0.5968\n",
      "Epoch 22, Batch 42, loss_ca: 2.1123, adv_loss: 0.5823\n",
      "Epoch 22, Batch 43, loss_ca: 2.0045, adv_loss: 0.5764\n",
      "Epoch 22, Batch 44, loss_ca: 1.8845, adv_loss: 0.5682\n",
      "Epoch 22, Batch 45, loss_ca: 1.8435, adv_loss: 0.5581\n",
      "Epoch 22, Batch 46, loss_ca: 1.8317, adv_loss: 0.5391\n",
      "Epoch 22, Batch 47, loss_ca: 1.8768, adv_loss: 0.5468\n",
      "Epoch 22, Batch 48, loss_ca: 1.8472, adv_loss: 0.5240\n",
      "Epoch 22, Batch 49, loss_ca: 1.8399, adv_loss: 0.5192\n",
      "Epoch 22, Batch 50, loss_ca: 1.8218, adv_loss: 0.5176\n",
      "Epoch 22, Batch 51, loss_ca: 1.8311, adv_loss: 0.5105\n",
      "Epoch 22, Batch 52, loss_ca: 1.7167, adv_loss: 0.4876\n",
      "Epoch 22, Batch 53, loss_ca: 1.9500, adv_loss: 0.5417\n",
      "Epoch 22, Batch 54, loss_ca: 1.8905, adv_loss: 0.5355\n",
      "Epoch 22, Batch 55, loss_ca: 1.7949, adv_loss: 0.5237\n",
      "Epoch 22, Batch 56, loss_ca: 1.7092, adv_loss: 0.5041\n",
      "Epoch 22, Batch 57, loss_ca: 1.6763, adv_loss: 0.4840\n",
      "Epoch 22, Batch 58, loss_ca: 1.8141, adv_loss: 0.4823\n",
      "Epoch 22, Batch 59, loss_ca: 1.8108, adv_loss: 0.5039\n",
      "Epoch 22, Batch 60, loss_ca: 1.8203, adv_loss: 0.4834\n",
      "Epoch 22, Batch 61, loss_ca: 1.8112, adv_loss: 0.4997\n",
      "Epoch 22, Batch 62, loss_ca: 1.8360, adv_loss: 0.4886\n",
      "Epoch 22, Batch 63, loss_ca: 1.9779, adv_loss: 0.5306\n",
      "Epoch 22, Batch 64, loss_ca: 1.9667, adv_loss: 0.5444\n",
      "Epoch 22, Batch 65, loss_ca: 1.9084, adv_loss: 0.5644\n",
      "Epoch 22, Batch 66, loss_ca: 1.8785, adv_loss: 0.5709\n",
      "Epoch 22, Batch 67, loss_ca: 1.8709, adv_loss: 0.5735\n",
      "Epoch 22, Batch 68, loss_ca: 1.7804, adv_loss: 0.5325\n",
      "Epoch 22, Batch 69, loss_ca: 1.9366, adv_loss: 0.5449\n",
      "Epoch 22, Batch 70, loss_ca: 1.8501, adv_loss: 0.5382\n",
      "Epoch 22, Batch 71, loss_ca: 1.7443, adv_loss: 0.5107\n",
      "Epoch 22, Batch 72, loss_ca: 1.7480, adv_loss: 0.4879\n",
      "Epoch 22, Batch 73, loss_ca: 1.6974, adv_loss: 0.4868\n",
      "Epoch 22, Batch 74, loss_ca: 1.7244, adv_loss: 0.4864\n",
      "Epoch 22, Batch 75, loss_ca: 1.6906, adv_loss: 0.4869\n",
      "Epoch 22, Batch 76, loss_ca: 1.6424, adv_loss: 0.4752\n",
      "Epoch 22, Batch 77, loss_ca: 1.6727, adv_loss: 0.4917\n",
      "Epoch 22, Batch 78, loss_ca: 1.7115, adv_loss: 0.5046\n",
      "Epoch 22, Batch 79, loss_ca: 1.7108, adv_loss: 0.5113\n",
      "Epoch 22, Batch 80, loss_ca: 1.7143, adv_loss: 0.5237\n",
      "Epoch 22, Batch 81, loss_ca: 1.6855, adv_loss: 0.5215\n",
      "Epoch 22, Batch 82, loss_ca: 1.7360, adv_loss: 0.5240\n",
      "Epoch 22, Batch 83, loss_ca: 1.7030, adv_loss: 0.5109\n",
      "Epoch 22, Batch 84, loss_ca: 1.7113, adv_loss: 0.5073\n",
      "Epoch 22, Batch 85, loss_ca: 1.7279, adv_loss: 0.5023\n",
      "Epoch 22, Batch 86, loss_ca: 1.7812, adv_loss: 0.4887\n",
      "Epoch 22, Batch 87, loss_ca: 1.8040, adv_loss: 0.4867\n",
      "Epoch 22, Batch 88, loss_ca: 1.7672, adv_loss: 0.4894\n",
      "Epoch 22, Batch 89, loss_ca: 1.8196, adv_loss: 0.4944\n",
      "Epoch 22, Batch 90, loss_ca: 1.7172, adv_loss: 0.5060\n",
      "Epoch 22, Batch 91, loss_ca: 1.8695, adv_loss: 0.5137\n",
      "Epoch 22, Batch 92, loss_ca: 1.8069, adv_loss: 0.5150\n",
      "Epoch 22, Batch 93, loss_ca: 1.9194, adv_loss: 0.5252\n",
      "Epoch 22, Batch 94, loss_ca: 1.8429, adv_loss: 0.5268\n",
      "Epoch 22, Batch 95, loss_ca: 1.8835, adv_loss: 0.5312\n",
      "Epoch 22, Batch 96, loss_ca: 1.9185, adv_loss: 0.5367\n",
      "Epoch 22, Batch 97, loss_ca: 1.8813, adv_loss: 0.5329\n",
      "Epoch 22, Batch 98, loss_ca: 1.8830, adv_loss: 0.5241\n",
      "Epoch 22, Batch 99, loss_ca: 1.7780, adv_loss: 0.5050\n",
      "Epoch 22, Batch 100, loss_ca: 1.8814, adv_loss: 0.4997\n",
      "Epoch 22, Batch 101, loss_ca: 1.9301, adv_loss: 0.5176\n",
      "Epoch 22, Batch 102, loss_ca: 1.9025, adv_loss: 0.5282\n",
      "Epoch 22, Batch 103, loss_ca: 1.8860, adv_loss: 0.5213\n",
      "Epoch 22, Batch 104, loss_ca: 1.9077, adv_loss: 0.4999\n",
      "Epoch 22, Batch 105, loss_ca: 1.9787, adv_loss: 0.5522\n",
      "Epoch 22, Batch 106, loss_ca: 1.9712, adv_loss: 0.5766\n",
      "Epoch 22, Batch 107, loss_ca: 2.0906, adv_loss: 0.5820\n",
      "Epoch 22, Batch 108, loss_ca: 1.9200, adv_loss: 0.5591\n",
      "Epoch 22, Batch 109, loss_ca: 2.2000, adv_loss: 0.5826\n",
      "Epoch 22, Batch 110, loss_ca: 2.0907, adv_loss: 0.5885\n",
      "Epoch 22, Batch 111, loss_ca: 2.0375, adv_loss: 0.5907\n",
      "Epoch 22, Batch 112, loss_ca: 1.9720, adv_loss: 0.5944\n",
      "Epoch 22, Batch 113, loss_ca: 1.8486, adv_loss: 0.5743\n",
      "Epoch 22, Batch 114, loss_ca: 1.8419, adv_loss: 0.5625\n",
      "Epoch 22, Batch 115, loss_ca: 1.7617, adv_loss: 0.5539\n",
      "Epoch 22, Batch 116, loss_ca: 1.7489, adv_loss: 0.5418\n",
      "Epoch 22, Batch 117, loss_ca: 1.7637, adv_loss: 0.5391\n",
      "Epoch 22, Batch 118, loss_ca: 1.7926, adv_loss: 0.5345\n",
      "Epoch 22, Batch 119, loss_ca: 1.8448, adv_loss: 0.5289\n",
      "Epoch 22, Batch 120, loss_ca: 1.8592, adv_loss: 0.5066\n",
      "Epoch 22, Batch 121, loss_ca: 1.9994, adv_loss: 0.4964\n",
      "Epoch 22, Batch 122, loss_ca: 1.9801, adv_loss: 0.4704\n",
      "Epoch 22, Batch 123, loss_ca: 1.8922, adv_loss: 0.4842\n",
      "Epoch 22, Batch 124, loss_ca: 1.8732, adv_loss: 0.4889\n",
      "Epoch 22, Batch 125, loss_ca: 1.8779, adv_loss: 0.4802\n",
      "Epoch 22, Batch 126, loss_ca: 1.9889, adv_loss: 0.4737\n",
      "Epoch 22, Batch 127, loss_ca: 2.0790, adv_loss: 0.4402\n",
      "Epoch 22, Batch 128, loss_ca: 2.0371, adv_loss: 0.4333\n",
      "Epoch 22, Batch 129, loss_ca: 1.9839, adv_loss: 0.4504\n",
      "Epoch 22, Batch 130, loss_ca: 1.9231, adv_loss: 0.4579\n",
      "Epoch 22, Batch 131, loss_ca: 1.8804, adv_loss: 0.4643\n",
      "Epoch 22, Batch 132, loss_ca: 1.9120, adv_loss: 0.5046\n",
      "Epoch 22, Batch 133, loss_ca: 1.9079, adv_loss: 0.5523\n",
      "Epoch 22, Batch 134, loss_ca: 1.9149, adv_loss: 0.5771\n",
      "Epoch 22, Batch 135, loss_ca: 1.9098, adv_loss: 0.5881\n",
      "Epoch 22, Batch 136, loss_ca: 1.8852, adv_loss: 0.5564\n",
      "Epoch 22, Batch 137, loss_ca: 1.7797, adv_loss: 0.5448\n",
      "Epoch 22, Batch 138, loss_ca: 1.7209, adv_loss: 0.5413\n",
      "Epoch 22, Batch 139, loss_ca: 1.6625, adv_loss: 0.5318\n",
      "Epoch 22, Batch 140, loss_ca: 1.6672, adv_loss: 0.5122\n",
      "Epoch 22, Batch 141, loss_ca: 1.7105, adv_loss: 0.5334\n",
      "Epoch 22, Batch 142, loss_ca: 1.8451, adv_loss: 0.5556\n",
      "Epoch 22, Batch 143, loss_ca: 1.7901, adv_loss: 0.5434\n",
      "Epoch 22, Batch 144, loss_ca: 1.7909, adv_loss: 0.5410\n",
      "Epoch 22, Batch 145, loss_ca: 1.7710, adv_loss: 0.5499\n",
      "Epoch 22, Batch 146, loss_ca: 1.7586, adv_loss: 0.5445\n",
      "Epoch 22, Batch 147, loss_ca: 1.9006, adv_loss: 0.5353\n",
      "Epoch 22, Batch 148, loss_ca: 1.9623, adv_loss: 0.5414\n",
      "Epoch 22, Batch 149, loss_ca: 1.8620, adv_loss: 0.5468\n",
      "Epoch 22, Batch 150, loss_ca: 1.8004, adv_loss: 0.5390\n",
      "Epoch 22, Batch 151, loss_ca: 1.7164, adv_loss: 0.5286\n",
      "Epoch 22, Batch 152, loss_ca: 1.6773, adv_loss: 0.5327\n",
      "Epoch 22, Batch 153, loss_ca: 1.7945, adv_loss: 0.5343\n",
      "Epoch 22, Batch 154, loss_ca: 1.8433, adv_loss: 0.5233\n",
      "Epoch 22, Batch 155, loss_ca: 1.8636, adv_loss: 0.5364\n",
      "Epoch 22, Batch 156, loss_ca: 1.8933, adv_loss: 0.5501\n",
      "Epoch 22, Batch 157, loss_ca: 1.8556, adv_loss: 0.5239\n",
      "Epoch 22, Batch 158, loss_ca: 2.0082, adv_loss: 0.5684\n",
      "Epoch 22, Batch 159, loss_ca: 2.2520, adv_loss: 0.5796\n",
      "Epoch 22, Batch 160, loss_ca: 2.1800, adv_loss: 0.5577\n",
      "Epoch 22, Batch 161, loss_ca: 1.9168, adv_loss: 0.5460\n",
      "Epoch 22, Batch 162, loss_ca: 2.1529, adv_loss: 0.5445\n",
      "Epoch 22, Batch 163, loss_ca: 2.0437, adv_loss: 0.4795\n",
      "Epoch 22, Batch 164, loss_ca: 2.2011, adv_loss: 0.5546\n",
      "Epoch 22, Batch 165, loss_ca: 2.2740, adv_loss: 0.5539\n",
      "Epoch 22, Batch 166, loss_ca: 2.1343, adv_loss: 0.5847\n",
      "Epoch 22, Batch 167, loss_ca: 2.2555, adv_loss: 0.5245\n",
      "Epoch 22, Batch 168, loss_ca: 2.3444, adv_loss: 0.5909\n",
      "Epoch 22, Batch 169, loss_ca: 2.1636, adv_loss: 0.6083\n",
      "Epoch 22, Batch 170, loss_ca: 2.0831, adv_loss: 0.5087\n",
      "Epoch 22, Batch 171, loss_ca: 2.1053, adv_loss: 0.5093\n",
      "Epoch 22, Batch 172, loss_ca: 2.1153, adv_loss: 0.5711\n",
      "Epoch 22, Batch 173, loss_ca: 1.9468, adv_loss: 0.5046\n",
      "Epoch 22, Batch 174, loss_ca: 1.9279, adv_loss: 0.4472\n",
      "Epoch 22, Batch 175, loss_ca: 1.8261, adv_loss: 0.4582\n",
      "Epoch 22, Batch 176, loss_ca: 1.8249, adv_loss: 0.4676\n",
      "Epoch 22, Batch 177, loss_ca: 1.8309, adv_loss: 0.4927\n",
      "Epoch 22, Batch 178, loss_ca: 1.9419, adv_loss: 0.5165\n",
      "Epoch 22, Batch 179, loss_ca: 1.8773, adv_loss: 0.5146\n",
      "Epoch 22, Batch 180, loss_ca: 1.7816, adv_loss: 0.5296\n",
      "Epoch 22, Batch 181, loss_ca: 1.7879, adv_loss: 0.5325\n",
      "Epoch 22, Batch 182, loss_ca: 1.9010, adv_loss: 0.5237\n",
      "Epoch 22, Batch 183, loss_ca: 1.9486, adv_loss: 0.5374\n",
      "Epoch 22, Batch 184, loss_ca: 1.7852, adv_loss: 0.5017\n",
      "Epoch 22, Batch 185, loss_ca: 1.7558, adv_loss: 0.4997\n",
      "Epoch 22, Batch 186, loss_ca: 1.7295, adv_loss: 0.4964\n",
      "Epoch 22, Batch 187, loss_ca: 1.7310, adv_loss: 0.5031\n",
      "Epoch 22, Batch 188, loss_ca: 1.8599, adv_loss: 0.5397\n",
      "Epoch 22, Batch 189, loss_ca: 1.8620, adv_loss: 0.5434\n",
      "Epoch 22, Batch 190, loss_ca: 1.9606, adv_loss: 0.5460\n",
      "Epoch 22, Batch 191, loss_ca: 1.9469, adv_loss: 0.5855\n",
      "Epoch 22, Batch 192, loss_ca: 2.1721, adv_loss: 0.5565\n",
      "Epoch 22, Batch 193, loss_ca: 2.0565, adv_loss: 0.6002\n",
      "Epoch 22, Batch 194, loss_ca: 1.9556, adv_loss: 0.5298\n",
      "Epoch 22, Batch 195, loss_ca: 2.2860, adv_loss: 0.6122\n",
      "Epoch 22, Batch 196, loss_ca: 2.5684, adv_loss: 0.6245\n",
      "Epoch 22, Batch 197, loss_ca: 2.0833, adv_loss: 0.5420\n",
      "Epoch 22, Batch 198, loss_ca: 2.4132, adv_loss: 0.5017\n",
      "Epoch 22, Batch 199, loss_ca: 2.5450, adv_loss: 0.5025\n",
      "Epoch 22, Batch 200, loss_ca: 2.1108, adv_loss: 0.4928\n",
      "Epoch 22, Batch 201, loss_ca: 2.1757, adv_loss: 0.4930\n",
      "Epoch 22, Batch 202, loss_ca: 2.0309, adv_loss: 0.4773\n",
      "Epoch 22, Batch 203, loss_ca: 2.2782, adv_loss: 0.5099\n",
      "Epoch 22, Batch 204, loss_ca: 1.8820, adv_loss: 0.4790\n",
      "Epoch 22, Batch 205, loss_ca: 2.1072, adv_loss: 0.4893\n",
      "Epoch 22, Batch 206, loss_ca: 1.9052, adv_loss: 0.5003\n",
      "Epoch 22, Batch 207, loss_ca: 2.1306, adv_loss: 0.5088\n",
      "Epoch 22, Batch 208, loss_ca: 1.9983, adv_loss: 0.5181\n",
      "Epoch 22, Batch 209, loss_ca: 1.7738, adv_loss: 0.5196\n",
      "Epoch 22, Batch 210, loss_ca: 1.9463, adv_loss: 0.5758\n",
      "Epoch 22, Batch 211, loss_ca: 2.0360, adv_loss: 0.5617\n",
      "Epoch 22, Batch 212, loss_ca: 1.7112, adv_loss: 0.5763\n",
      "Epoch 22, Batch 213, loss_ca: 1.9843, adv_loss: 0.5831\n",
      "Epoch 22, Batch 214, loss_ca: 2.0142, adv_loss: 0.6080\n",
      "Epoch 22, Batch 215, loss_ca: 1.6481, adv_loss: 0.5947\n",
      "Epoch 22, Batch 216, loss_ca: 1.8903, adv_loss: 0.6566\n",
      "Epoch 22, Batch 217, loss_ca: 1.7483, adv_loss: 0.6338\n",
      "Epoch 22, Batch 218, loss_ca: 1.8299, adv_loss: 0.6253\n",
      "Epoch 22, Batch 219, loss_ca: 2.3582, adv_loss: 0.5985\n",
      "Epoch 22, Batch 220, loss_ca: 2.1099, adv_loss: 0.5638\n",
      "Epoch 22, Batch 221, loss_ca: 1.9049, adv_loss: 0.5266\n",
      "Epoch 22, Batch 222, loss_ca: 1.7455, adv_loss: 0.5150\n",
      "Epoch 22, Batch 223, loss_ca: 1.7007, adv_loss: 0.4967\n",
      "Epoch 22, Batch 224, loss_ca: 1.6772, adv_loss: 0.4819\n",
      "Epoch 22, Batch 225, loss_ca: 1.7332, adv_loss: 0.5060\n",
      "Epoch 22, Batch 226, loss_ca: 1.6898, adv_loss: 0.4892\n",
      "Epoch 22, Batch 227, loss_ca: 1.6873, adv_loss: 0.4838\n",
      "Epoch 22, Batch 228, loss_ca: 1.7254, adv_loss: 0.5119\n",
      "Epoch 23, Batch 23, loss_ca: 1.8524, adv_loss: 0.5354\n",
      "Epoch 23, Batch 24, loss_ca: 2.2127, adv_loss: 0.5523\n",
      "Epoch 23, Batch 25, loss_ca: 2.1641, adv_loss: 0.5386\n",
      "Epoch 23, Batch 26, loss_ca: 2.1863, adv_loss: 0.5252\n",
      "Epoch 23, Batch 27, loss_ca: 2.1176, adv_loss: 0.5273\n",
      "Epoch 23, Batch 28, loss_ca: 1.9390, adv_loss: 0.5205\n",
      "Epoch 23, Batch 29, loss_ca: 1.9503, adv_loss: 0.5098\n",
      "Epoch 23, Batch 30, loss_ca: 1.9906, adv_loss: 0.4940\n",
      "Epoch 23, Batch 31, loss_ca: 1.9654, adv_loss: 0.4924\n",
      "Epoch 23, Batch 32, loss_ca: 1.8263, adv_loss: 0.4875\n",
      "Epoch 23, Batch 33, loss_ca: 1.8448, adv_loss: 0.5078\n",
      "Epoch 23, Batch 34, loss_ca: 2.0569, adv_loss: 0.4954\n",
      "Epoch 23, Batch 35, loss_ca: 1.9518, adv_loss: 0.5524\n",
      "Epoch 23, Batch 36, loss_ca: 1.9731, adv_loss: 0.5689\n",
      "Epoch 23, Batch 37, loss_ca: 1.9809, adv_loss: 0.5645\n",
      "Epoch 23, Batch 38, loss_ca: 2.0042, adv_loss: 0.5774\n",
      "Epoch 23, Batch 39, loss_ca: 1.9939, adv_loss: 0.6091\n",
      "Epoch 23, Batch 40, loss_ca: 1.9953, adv_loss: 0.6102\n",
      "Epoch 23, Batch 41, loss_ca: 2.0357, adv_loss: 0.5874\n",
      "Epoch 23, Batch 42, loss_ca: 2.0246, adv_loss: 0.5907\n",
      "Epoch 23, Batch 43, loss_ca: 1.9308, adv_loss: 0.5944\n",
      "Epoch 23, Batch 44, loss_ca: 1.8647, adv_loss: 0.5943\n",
      "Epoch 23, Batch 45, loss_ca: 1.8312, adv_loss: 0.5953\n",
      "Epoch 23, Batch 46, loss_ca: 1.8325, adv_loss: 0.5882\n",
      "Epoch 23, Batch 47, loss_ca: 1.8805, adv_loss: 0.5933\n",
      "Epoch 23, Batch 48, loss_ca: 1.8306, adv_loss: 0.5922\n",
      "Epoch 23, Batch 49, loss_ca: 1.8244, adv_loss: 0.5925\n",
      "Epoch 23, Batch 50, loss_ca: 1.8165, adv_loss: 0.5931\n",
      "Epoch 23, Batch 51, loss_ca: 1.8219, adv_loss: 0.5791\n",
      "Epoch 23, Batch 52, loss_ca: 1.8869, adv_loss: 0.5715\n",
      "Epoch 23, Batch 53, loss_ca: 1.8163, adv_loss: 0.5721\n",
      "Epoch 23, Batch 54, loss_ca: 1.7705, adv_loss: 0.5672\n",
      "Epoch 23, Batch 55, loss_ca: 1.7444, adv_loss: 0.5553\n",
      "Epoch 23, Batch 56, loss_ca: 1.6971, adv_loss: 0.5392\n",
      "Epoch 23, Batch 57, loss_ca: 1.6924, adv_loss: 0.5305\n",
      "Epoch 23, Batch 58, loss_ca: 1.9244, adv_loss: 0.5398\n",
      "Epoch 23, Batch 59, loss_ca: 1.8535, adv_loss: 0.5479\n",
      "Epoch 23, Batch 60, loss_ca: 1.8211, adv_loss: 0.5589\n",
      "Epoch 23, Batch 61, loss_ca: 1.7943, adv_loss: 0.5530\n",
      "Epoch 23, Batch 62, loss_ca: 1.8538, adv_loss: 0.5521\n",
      "Epoch 23, Batch 63, loss_ca: 1.9073, adv_loss: 0.5495\n",
      "Epoch 23, Batch 64, loss_ca: 1.9197, adv_loss: 0.5570\n",
      "Epoch 23, Batch 65, loss_ca: 1.8418, adv_loss: 0.5416\n",
      "Epoch 23, Batch 66, loss_ca: 1.8289, adv_loss: 0.5285\n",
      "Epoch 23, Batch 67, loss_ca: 1.8413, adv_loss: 0.5418\n",
      "Epoch 23, Batch 68, loss_ca: 1.7695, adv_loss: 0.5106\n",
      "Epoch 23, Batch 69, loss_ca: 1.8050, adv_loss: 0.5155\n",
      "Epoch 23, Batch 70, loss_ca: 1.8083, adv_loss: 0.5225\n",
      "Epoch 23, Batch 71, loss_ca: 1.6872, adv_loss: 0.4923\n",
      "Epoch 23, Batch 72, loss_ca: 1.7613, adv_loss: 0.4980\n",
      "Epoch 23, Batch 73, loss_ca: 1.6472, adv_loss: 0.4914\n",
      "Epoch 23, Batch 74, loss_ca: 1.6677, adv_loss: 0.4943\n",
      "Epoch 23, Batch 75, loss_ca: 1.6473, adv_loss: 0.4865\n",
      "Epoch 23, Batch 76, loss_ca: 1.6259, adv_loss: 0.4982\n",
      "Epoch 23, Batch 77, loss_ca: 1.6608, adv_loss: 0.5038\n",
      "Epoch 23, Batch 78, loss_ca: 1.6735, adv_loss: 0.5107\n",
      "Epoch 23, Batch 79, loss_ca: 1.6785, adv_loss: 0.5203\n",
      "Epoch 23, Batch 80, loss_ca: 1.6795, adv_loss: 0.5367\n",
      "Epoch 23, Batch 81, loss_ca: 1.6673, adv_loss: 0.5380\n",
      "Epoch 23, Batch 82, loss_ca: 1.7142, adv_loss: 0.5427\n",
      "Epoch 23, Batch 83, loss_ca: 1.7296, adv_loss: 0.5401\n",
      "Epoch 23, Batch 84, loss_ca: 1.7436, adv_loss: 0.5286\n",
      "Epoch 23, Batch 85, loss_ca: 1.7146, adv_loss: 0.5179\n",
      "Epoch 23, Batch 86, loss_ca: 1.7449, adv_loss: 0.4984\n",
      "Epoch 23, Batch 87, loss_ca: 1.7959, adv_loss: 0.4922\n",
      "Epoch 23, Batch 88, loss_ca: 1.7753, adv_loss: 0.4846\n",
      "Epoch 23, Batch 89, loss_ca: 1.8204, adv_loss: 0.4897\n",
      "Epoch 23, Batch 90, loss_ca: 1.7095, adv_loss: 0.4928\n",
      "Epoch 23, Batch 91, loss_ca: 1.8352, adv_loss: 0.5125\n",
      "Epoch 23, Batch 92, loss_ca: 1.7515, adv_loss: 0.4904\n",
      "Epoch 23, Batch 93, loss_ca: 1.8612, adv_loss: 0.5242\n",
      "Epoch 23, Batch 94, loss_ca: 1.7902, adv_loss: 0.5140\n",
      "Epoch 23, Batch 95, loss_ca: 1.8581, adv_loss: 0.5411\n",
      "Epoch 23, Batch 96, loss_ca: 1.9455, adv_loss: 0.5608\n",
      "Epoch 23, Batch 97, loss_ca: 1.9369, adv_loss: 0.5639\n",
      "Epoch 23, Batch 98, loss_ca: 1.9130, adv_loss: 0.5614\n",
      "Epoch 23, Batch 99, loss_ca: 1.7782, adv_loss: 0.5176\n",
      "Epoch 23, Batch 100, loss_ca: 1.8971, adv_loss: 0.5129\n",
      "Epoch 23, Batch 101, loss_ca: 1.9555, adv_loss: 0.5137\n",
      "Epoch 23, Batch 102, loss_ca: 1.8978, adv_loss: 0.5212\n",
      "Epoch 23, Batch 103, loss_ca: 1.8458, adv_loss: 0.5172\n",
      "Epoch 23, Batch 104, loss_ca: 1.8965, adv_loss: 0.4866\n",
      "Epoch 23, Batch 105, loss_ca: 1.9989, adv_loss: 0.5420\n",
      "Epoch 23, Batch 106, loss_ca: 1.9727, adv_loss: 0.5518\n",
      "Epoch 23, Batch 107, loss_ca: 2.0705, adv_loss: 0.5372\n",
      "Epoch 23, Batch 108, loss_ca: 1.8902, adv_loss: 0.5085\n",
      "Epoch 23, Batch 109, loss_ca: 1.9241, adv_loss: 0.5228\n",
      "Epoch 23, Batch 110, loss_ca: 1.9248, adv_loss: 0.5259\n",
      "Epoch 23, Batch 111, loss_ca: 1.9220, adv_loss: 0.5386\n",
      "Epoch 23, Batch 112, loss_ca: 1.9252, adv_loss: 0.5363\n",
      "Epoch 23, Batch 113, loss_ca: 1.8398, adv_loss: 0.5482\n",
      "Epoch 23, Batch 114, loss_ca: 1.8122, adv_loss: 0.5443\n",
      "Epoch 23, Batch 115, loss_ca: 1.7547, adv_loss: 0.5425\n",
      "Epoch 23, Batch 116, loss_ca: 1.8122, adv_loss: 0.5251\n",
      "Epoch 23, Batch 117, loss_ca: 1.8331, adv_loss: 0.5170\n",
      "Epoch 23, Batch 118, loss_ca: 1.8139, adv_loss: 0.5060\n",
      "Epoch 23, Batch 119, loss_ca: 1.7887, adv_loss: 0.4951\n",
      "Epoch 23, Batch 120, loss_ca: 1.7836, adv_loss: 0.4868\n",
      "Epoch 23, Batch 121, loss_ca: 1.9060, adv_loss: 0.4928\n",
      "Epoch 23, Batch 122, loss_ca: 1.8932, adv_loss: 0.4997\n",
      "Epoch 23, Batch 123, loss_ca: 1.8654, adv_loss: 0.5241\n",
      "Epoch 23, Batch 124, loss_ca: 1.8609, adv_loss: 0.5364\n",
      "Epoch 23, Batch 125, loss_ca: 1.8595, adv_loss: 0.5429\n",
      "Epoch 23, Batch 126, loss_ca: 1.9575, adv_loss: 0.5443\n",
      "Epoch 23, Batch 127, loss_ca: 2.0397, adv_loss: 0.5177\n",
      "Epoch 23, Batch 128, loss_ca: 2.0125, adv_loss: 0.5007\n",
      "Epoch 23, Batch 129, loss_ca: 1.9716, adv_loss: 0.4976\n",
      "Epoch 23, Batch 130, loss_ca: 1.9200, adv_loss: 0.4911\n",
      "Epoch 23, Batch 131, loss_ca: 1.8760, adv_loss: 0.4634\n",
      "Epoch 23, Batch 132, loss_ca: 1.8525, adv_loss: 0.4881\n",
      "Epoch 23, Batch 133, loss_ca: 1.8570, adv_loss: 0.5067\n",
      "Epoch 23, Batch 134, loss_ca: 1.9082, adv_loss: 0.5575\n",
      "Epoch 23, Batch 135, loss_ca: 1.8872, adv_loss: 0.5434\n",
      "Epoch 23, Batch 136, loss_ca: 1.8195, adv_loss: 0.5209\n",
      "Epoch 23, Batch 137, loss_ca: 1.7493, adv_loss: 0.5205\n",
      "Epoch 23, Batch 138, loss_ca: 1.7601, adv_loss: 0.5348\n",
      "Epoch 23, Batch 139, loss_ca: 1.6898, adv_loss: 0.5309\n",
      "Epoch 23, Batch 140, loss_ca: 1.6631, adv_loss: 0.5226\n",
      "Epoch 23, Batch 141, loss_ca: 1.6955, adv_loss: 0.5134\n",
      "Epoch 23, Batch 142, loss_ca: 1.7931, adv_loss: 0.5212\n",
      "Epoch 23, Batch 143, loss_ca: 1.8792, adv_loss: 0.5209\n",
      "Epoch 23, Batch 144, loss_ca: 1.8600, adv_loss: 0.5356\n",
      "Epoch 23, Batch 145, loss_ca: 1.7973, adv_loss: 0.5314\n",
      "Epoch 23, Batch 146, loss_ca: 1.7040, adv_loss: 0.5193\n",
      "Epoch 23, Batch 147, loss_ca: 1.7841, adv_loss: 0.5347\n",
      "Epoch 23, Batch 148, loss_ca: 1.8143, adv_loss: 0.5355\n",
      "Epoch 23, Batch 149, loss_ca: 1.7923, adv_loss: 0.5304\n",
      "Epoch 23, Batch 150, loss_ca: 1.7231, adv_loss: 0.5426\n",
      "Epoch 23, Batch 151, loss_ca: 1.6563, adv_loss: 0.5511\n",
      "Epoch 23, Batch 152, loss_ca: 1.6741, adv_loss: 0.5453\n",
      "Epoch 23, Batch 153, loss_ca: 1.8261, adv_loss: 0.5369\n",
      "Epoch 23, Batch 154, loss_ca: 1.8599, adv_loss: 0.5198\n",
      "Epoch 23, Batch 155, loss_ca: 1.8702, adv_loss: 0.5314\n",
      "Epoch 23, Batch 156, loss_ca: 1.8426, adv_loss: 0.5302\n",
      "Epoch 23, Batch 157, loss_ca: 1.8935, adv_loss: 0.5229\n",
      "Epoch 23, Batch 158, loss_ca: 1.9352, adv_loss: 0.5157\n",
      "Epoch 23, Batch 159, loss_ca: 2.1525, adv_loss: 0.5263\n",
      "Epoch 23, Batch 160, loss_ca: 2.1118, adv_loss: 0.5262\n",
      "Epoch 23, Batch 161, loss_ca: 1.8331, adv_loss: 0.5126\n",
      "Epoch 23, Batch 162, loss_ca: 2.2781, adv_loss: 0.6127\n",
      "Epoch 23, Batch 163, loss_ca: 2.0631, adv_loss: 0.5706\n",
      "Epoch 23, Batch 164, loss_ca: 2.2691, adv_loss: 0.5952\n",
      "Epoch 23, Batch 165, loss_ca: 2.3953, adv_loss: 0.5956\n",
      "Epoch 23, Batch 166, loss_ca: 2.1687, adv_loss: 0.5678\n",
      "Epoch 23, Batch 167, loss_ca: 2.0686, adv_loss: 0.4799\n",
      "Epoch 23, Batch 168, loss_ca: 2.1800, adv_loss: 0.5254\n",
      "Epoch 23, Batch 169, loss_ca: 2.2773, adv_loss: 0.5374\n",
      "Epoch 23, Batch 170, loss_ca: 2.3918, adv_loss: 0.5262\n",
      "Epoch 23, Batch 171, loss_ca: 2.6593, adv_loss: 0.5217\n",
      "Epoch 23, Batch 172, loss_ca: 2.2564, adv_loss: 0.5520\n",
      "Epoch 23, Batch 173, loss_ca: 1.9363, adv_loss: 0.5220\n",
      "Epoch 23, Batch 174, loss_ca: 1.8645, adv_loss: 0.4656\n",
      "Epoch 23, Batch 175, loss_ca: 1.7403, adv_loss: 0.4601\n",
      "Epoch 23, Batch 176, loss_ca: 1.7764, adv_loss: 0.4651\n",
      "Epoch 23, Batch 177, loss_ca: 1.7504, adv_loss: 0.4858\n",
      "Epoch 23, Batch 178, loss_ca: 1.8739, adv_loss: 0.4991\n",
      "Epoch 23, Batch 179, loss_ca: 1.8318, adv_loss: 0.4992\n",
      "Epoch 23, Batch 180, loss_ca: 1.7439, adv_loss: 0.4950\n",
      "Epoch 23, Batch 181, loss_ca: 1.7426, adv_loss: 0.5082\n",
      "Epoch 23, Batch 182, loss_ca: 1.7171, adv_loss: 0.4945\n",
      "Epoch 23, Batch 183, loss_ca: 1.8391, adv_loss: 0.4969\n",
      "Epoch 23, Batch 184, loss_ca: 1.7473, adv_loss: 0.4998\n",
      "Epoch 23, Batch 185, loss_ca: 1.7092, adv_loss: 0.5003\n",
      "Epoch 23, Batch 186, loss_ca: 1.6790, adv_loss: 0.5067\n",
      "Epoch 23, Batch 187, loss_ca: 1.7386, adv_loss: 0.5063\n",
      "Epoch 23, Batch 188, loss_ca: 1.7262, adv_loss: 0.5110\n",
      "Epoch 23, Batch 189, loss_ca: 1.8573, adv_loss: 0.5347\n",
      "Epoch 23, Batch 190, loss_ca: 2.0592, adv_loss: 0.5499\n",
      "Epoch 23, Batch 191, loss_ca: 1.9755, adv_loss: 0.5518\n",
      "Epoch 23, Batch 192, loss_ca: 2.1931, adv_loss: 0.5936\n",
      "Epoch 23, Batch 193, loss_ca: 1.9825, adv_loss: 0.5742\n",
      "Epoch 23, Batch 194, loss_ca: 1.8599, adv_loss: 0.5717\n",
      "Epoch 23, Batch 195, loss_ca: 1.8623, adv_loss: 0.5621\n",
      "Epoch 23, Batch 196, loss_ca: 2.2478, adv_loss: 0.5794\n",
      "Epoch 23, Batch 197, loss_ca: 2.1212, adv_loss: 0.4812\n",
      "Epoch 23, Batch 198, loss_ca: 2.3399, adv_loss: 0.4951\n",
      "Epoch 23, Batch 199, loss_ca: 2.3144, adv_loss: 0.4744\n",
      "Epoch 23, Batch 200, loss_ca: 2.0260, adv_loss: 0.4738\n",
      "Epoch 23, Batch 201, loss_ca: 2.0338, adv_loss: 0.5116\n",
      "Epoch 23, Batch 202, loss_ca: 1.9644, adv_loss: 0.5446\n",
      "Epoch 23, Batch 203, loss_ca: 2.2194, adv_loss: 0.5809\n",
      "Epoch 23, Batch 204, loss_ca: 1.9731, adv_loss: 0.5199\n",
      "Epoch 23, Batch 205, loss_ca: 2.2907, adv_loss: 0.5641\n",
      "Epoch 23, Batch 206, loss_ca: 1.9190, adv_loss: 0.5267\n",
      "Epoch 23, Batch 207, loss_ca: 2.1084, adv_loss: 0.5395\n",
      "Epoch 23, Batch 208, loss_ca: 2.0042, adv_loss: 0.5068\n",
      "Epoch 23, Batch 209, loss_ca: 1.7416, adv_loss: 0.4989\n",
      "Epoch 23, Batch 210, loss_ca: 1.8505, adv_loss: 0.5244\n",
      "Epoch 23, Batch 211, loss_ca: 2.0335, adv_loss: 0.4892\n",
      "Epoch 23, Batch 212, loss_ca: 1.7076, adv_loss: 0.5800\n",
      "Epoch 23, Batch 213, loss_ca: 2.1573, adv_loss: 0.6617\n",
      "Epoch 23, Batch 214, loss_ca: 1.7365, adv_loss: 0.6130\n",
      "Epoch 23, Batch 215, loss_ca: 1.6054, adv_loss: 0.6260\n",
      "Epoch 23, Batch 216, loss_ca: 1.7780, adv_loss: 0.6721\n",
      "Epoch 23, Batch 217, loss_ca: 1.7876, adv_loss: 0.6692\n",
      "Epoch 23, Batch 218, loss_ca: 1.9338, adv_loss: 0.6296\n",
      "Epoch 23, Batch 219, loss_ca: 2.1232, adv_loss: 0.5491\n",
      "Epoch 23, Batch 220, loss_ca: 2.1214, adv_loss: 0.5957\n",
      "Epoch 23, Batch 221, loss_ca: 2.0094, adv_loss: 0.5565\n",
      "Epoch 23, Batch 222, loss_ca: 1.8791, adv_loss: 0.5416\n",
      "Epoch 23, Batch 223, loss_ca: 1.7743, adv_loss: 0.5278\n",
      "Epoch 23, Batch 224, loss_ca: 1.7040, adv_loss: 0.5162\n",
      "Epoch 23, Batch 225, loss_ca: 1.7018, adv_loss: 0.4995\n",
      "Epoch 23, Batch 226, loss_ca: 1.7416, adv_loss: 0.4893\n",
      "Epoch 23, Batch 227, loss_ca: 1.7298, adv_loss: 0.4873\n",
      "Epoch 23, Batch 228, loss_ca: 1.7474, adv_loss: 0.5022\n",
      "Epoch 24, Batch 24, loss_ca: 2.1397, adv_loss: 0.5626\n",
      "Epoch 24, Batch 25, loss_ca: 2.1160, adv_loss: 0.5456\n",
      "Epoch 24, Batch 26, loss_ca: 2.1818, adv_loss: 0.5198\n",
      "Epoch 24, Batch 27, loss_ca: 2.0610, adv_loss: 0.5316\n",
      "Epoch 24, Batch 28, loss_ca: 1.9395, adv_loss: 0.5253\n",
      "Epoch 24, Batch 29, loss_ca: 1.9811, adv_loss: 0.5332\n",
      "Epoch 24, Batch 30, loss_ca: 1.9598, adv_loss: 0.5089\n",
      "Epoch 24, Batch 31, loss_ca: 1.9438, adv_loss: 0.5130\n",
      "Epoch 24, Batch 32, loss_ca: 1.8466, adv_loss: 0.5007\n",
      "Epoch 24, Batch 33, loss_ca: 1.8344, adv_loss: 0.5102\n",
      "Epoch 24, Batch 34, loss_ca: 2.0633, adv_loss: 0.5002\n",
      "Epoch 24, Batch 35, loss_ca: 1.9775, adv_loss: 0.5104\n",
      "Epoch 24, Batch 36, loss_ca: 1.9986, adv_loss: 0.5364\n",
      "Epoch 24, Batch 37, loss_ca: 1.9908, adv_loss: 0.5434\n",
      "Epoch 24, Batch 38, loss_ca: 1.9954, adv_loss: 0.5541\n",
      "Epoch 24, Batch 39, loss_ca: 1.9859, adv_loss: 0.5814\n",
      "Epoch 24, Batch 40, loss_ca: 1.9257, adv_loss: 0.5932\n",
      "Epoch 24, Batch 41, loss_ca: 1.9924, adv_loss: 0.5729\n",
      "Epoch 24, Batch 42, loss_ca: 1.9994, adv_loss: 0.5751\n",
      "Epoch 24, Batch 43, loss_ca: 1.9784, adv_loss: 0.5695\n",
      "Epoch 24, Batch 44, loss_ca: 1.9542, adv_loss: 0.5654\n",
      "Epoch 24, Batch 45, loss_ca: 1.8532, adv_loss: 0.5646\n",
      "Epoch 24, Batch 46, loss_ca: 1.8527, adv_loss: 0.5640\n",
      "Epoch 24, Batch 47, loss_ca: 1.8380, adv_loss: 0.5626\n",
      "Epoch 24, Batch 48, loss_ca: 1.7859, adv_loss: 0.5619\n",
      "Epoch 24, Batch 49, loss_ca: 1.7765, adv_loss: 0.5609\n",
      "Epoch 24, Batch 50, loss_ca: 1.7330, adv_loss: 0.5458\n",
      "Epoch 24, Batch 51, loss_ca: 1.7477, adv_loss: 0.5554\n",
      "Epoch 24, Batch 52, loss_ca: 1.8043, adv_loss: 0.5525\n",
      "Epoch 24, Batch 53, loss_ca: 1.8468, adv_loss: 0.5588\n",
      "Epoch 24, Batch 54, loss_ca: 1.8476, adv_loss: 0.5600\n",
      "Epoch 24, Batch 55, loss_ca: 1.8158, adv_loss: 0.5549\n",
      "Epoch 24, Batch 56, loss_ca: 1.7282, adv_loss: 0.5437\n",
      "Epoch 24, Batch 57, loss_ca: 1.6871, adv_loss: 0.5333\n",
      "Epoch 24, Batch 58, loss_ca: 1.7924, adv_loss: 0.5535\n",
      "Epoch 24, Batch 59, loss_ca: 1.7049, adv_loss: 0.5222\n",
      "Epoch 24, Batch 60, loss_ca: 1.7563, adv_loss: 0.5133\n",
      "Epoch 24, Batch 61, loss_ca: 1.7061, adv_loss: 0.5160\n",
      "Epoch 24, Batch 62, loss_ca: 1.8624, adv_loss: 0.5120\n",
      "Epoch 24, Batch 63, loss_ca: 1.9384, adv_loss: 0.4962\n",
      "Epoch 24, Batch 64, loss_ca: 1.8813, adv_loss: 0.4840\n",
      "Epoch 24, Batch 65, loss_ca: 1.8321, adv_loss: 0.4982\n",
      "Epoch 24, Batch 66, loss_ca: 1.8135, adv_loss: 0.5081\n",
      "Epoch 24, Batch 67, loss_ca: 1.8434, adv_loss: 0.5265\n",
      "Epoch 24, Batch 68, loss_ca: 1.7907, adv_loss: 0.5397\n",
      "Epoch 24, Batch 69, loss_ca: 1.8297, adv_loss: 0.5359\n",
      "Epoch 24, Batch 70, loss_ca: 1.8743, adv_loss: 0.5527\n",
      "Epoch 24, Batch 71, loss_ca: 1.7924, adv_loss: 0.5278\n",
      "Epoch 24, Batch 72, loss_ca: 1.7462, adv_loss: 0.5074\n",
      "Epoch 24, Batch 73, loss_ca: 1.6888, adv_loss: 0.4999\n",
      "Epoch 24, Batch 74, loss_ca: 1.6831, adv_loss: 0.5054\n",
      "Epoch 24, Batch 75, loss_ca: 1.6335, adv_loss: 0.5018\n",
      "Epoch 24, Batch 76, loss_ca: 1.5948, adv_loss: 0.4896\n",
      "Epoch 24, Batch 77, loss_ca: 1.6096, adv_loss: 0.4904\n",
      "Epoch 24, Batch 78, loss_ca: 1.6514, adv_loss: 0.4774\n",
      "Epoch 24, Batch 79, loss_ca: 1.6689, adv_loss: 0.4854\n",
      "Epoch 24, Batch 80, loss_ca: 1.6569, adv_loss: 0.4861\n",
      "Epoch 24, Batch 81, loss_ca: 1.6585, adv_loss: 0.5014\n",
      "Epoch 24, Batch 82, loss_ca: 1.6966, adv_loss: 0.5150\n",
      "Epoch 24, Batch 83, loss_ca: 1.6927, adv_loss: 0.5164\n",
      "Epoch 24, Batch 84, loss_ca: 1.7178, adv_loss: 0.5198\n",
      "Epoch 24, Batch 85, loss_ca: 1.7117, adv_loss: 0.5171\n",
      "Epoch 24, Batch 86, loss_ca: 1.8074, adv_loss: 0.5013\n",
      "Epoch 24, Batch 87, loss_ca: 1.8099, adv_loss: 0.5041\n",
      "Epoch 24, Batch 88, loss_ca: 1.7821, adv_loss: 0.4951\n",
      "Epoch 24, Batch 89, loss_ca: 1.8097, adv_loss: 0.4896\n",
      "Epoch 24, Batch 90, loss_ca: 1.7026, adv_loss: 0.4899\n",
      "Epoch 24, Batch 91, loss_ca: 1.8488, adv_loss: 0.5305\n",
      "Epoch 24, Batch 92, loss_ca: 1.7909, adv_loss: 0.5169\n",
      "Epoch 24, Batch 93, loss_ca: 1.8874, adv_loss: 0.5380\n",
      "Epoch 24, Batch 94, loss_ca: 1.8074, adv_loss: 0.5230\n",
      "Epoch 24, Batch 95, loss_ca: 1.8246, adv_loss: 0.5284\n",
      "Epoch 24, Batch 96, loss_ca: 1.9072, adv_loss: 0.5461\n",
      "Epoch 24, Batch 97, loss_ca: 1.8873, adv_loss: 0.5465\n",
      "Epoch 24, Batch 98, loss_ca: 1.8585, adv_loss: 0.5453\n",
      "Epoch 24, Batch 99, loss_ca: 1.8095, adv_loss: 0.5246\n",
      "Epoch 24, Batch 100, loss_ca: 1.8978, adv_loss: 0.5365\n",
      "Epoch 24, Batch 101, loss_ca: 1.9619, adv_loss: 0.5373\n",
      "Epoch 24, Batch 102, loss_ca: 1.9078, adv_loss: 0.5356\n",
      "Epoch 24, Batch 103, loss_ca: 1.8803, adv_loss: 0.5265\n",
      "Epoch 24, Batch 104, loss_ca: 1.9842, adv_loss: 0.5390\n",
      "Epoch 24, Batch 105, loss_ca: 1.9938, adv_loss: 0.5557\n",
      "Epoch 24, Batch 106, loss_ca: 1.9613, adv_loss: 0.5585\n",
      "Epoch 24, Batch 107, loss_ca: 2.0790, adv_loss: 0.5623\n",
      "Epoch 24, Batch 108, loss_ca: 1.9487, adv_loss: 0.5724\n",
      "Epoch 24, Batch 109, loss_ca: 1.9471, adv_loss: 0.5606\n",
      "Epoch 24, Batch 110, loss_ca: 1.8819, adv_loss: 0.5563\n",
      "Epoch 24, Batch 111, loss_ca: 1.8754, adv_loss: 0.5687\n",
      "Epoch 24, Batch 112, loss_ca: 1.9071, adv_loss: 0.5665\n",
      "Epoch 24, Batch 113, loss_ca: 1.8475, adv_loss: 0.5533\n",
      "Epoch 24, Batch 114, loss_ca: 1.8456, adv_loss: 0.5549\n",
      "Epoch 24, Batch 115, loss_ca: 1.7865, adv_loss: 0.5321\n",
      "Epoch 24, Batch 116, loss_ca: 1.8829, adv_loss: 0.5285\n",
      "Epoch 24, Batch 117, loss_ca: 1.9278, adv_loss: 0.5269\n",
      "Epoch 24, Batch 118, loss_ca: 1.8311, adv_loss: 0.5484\n",
      "Epoch 24, Batch 119, loss_ca: 1.7928, adv_loss: 0.5425\n",
      "Epoch 24, Batch 120, loss_ca: 1.7882, adv_loss: 0.5376\n",
      "Epoch 24, Batch 121, loss_ca: 1.9702, adv_loss: 0.5480\n",
      "Epoch 24, Batch 122, loss_ca: 1.9451, adv_loss: 0.5508\n",
      "Epoch 24, Batch 123, loss_ca: 1.9639, adv_loss: 0.5491\n",
      "Epoch 24, Batch 124, loss_ca: 1.9200, adv_loss: 0.5413\n",
      "Epoch 24, Batch 125, loss_ca: 1.8917, adv_loss: 0.5588\n",
      "Epoch 24, Batch 126, loss_ca: 2.0206, adv_loss: 0.5909\n",
      "Epoch 24, Batch 127, loss_ca: 2.0502, adv_loss: 0.5622\n",
      "Epoch 24, Batch 128, loss_ca: 1.9737, adv_loss: 0.5467\n",
      "Epoch 24, Batch 129, loss_ca: 1.9579, adv_loss: 0.5676\n",
      "Epoch 24, Batch 130, loss_ca: 1.9133, adv_loss: 0.5199\n",
      "Epoch 24, Batch 131, loss_ca: 1.8966, adv_loss: 0.4848\n",
      "Epoch 24, Batch 132, loss_ca: 1.8476, adv_loss: 0.5383\n",
      "Epoch 24, Batch 133, loss_ca: 1.9656, adv_loss: 0.5133\n",
      "Epoch 24, Batch 134, loss_ca: 1.9204, adv_loss: 0.5190\n",
      "Epoch 24, Batch 135, loss_ca: 1.8843, adv_loss: 0.5001\n",
      "Epoch 24, Batch 136, loss_ca: 1.9440, adv_loss: 0.5194\n",
      "Epoch 24, Batch 137, loss_ca: 1.8052, adv_loss: 0.4984\n",
      "Epoch 24, Batch 138, loss_ca: 1.7757, adv_loss: 0.5148\n",
      "Epoch 24, Batch 139, loss_ca: 1.7844, adv_loss: 0.5314\n",
      "Epoch 24, Batch 140, loss_ca: 1.6981, adv_loss: 0.5299\n",
      "Epoch 24, Batch 141, loss_ca: 1.6980, adv_loss: 0.5241\n",
      "Epoch 24, Batch 142, loss_ca: 1.8394, adv_loss: 0.5559\n",
      "Epoch 24, Batch 143, loss_ca: 1.8208, adv_loss: 0.5406\n",
      "Epoch 24, Batch 144, loss_ca: 1.8154, adv_loss: 0.5333\n",
      "Epoch 24, Batch 145, loss_ca: 1.8398, adv_loss: 0.5333\n",
      "Epoch 24, Batch 146, loss_ca: 1.7850, adv_loss: 0.5028\n",
      "Epoch 24, Batch 147, loss_ca: 1.8927, adv_loss: 0.5058\n",
      "Epoch 24, Batch 148, loss_ca: 1.8936, adv_loss: 0.5289\n",
      "Epoch 24, Batch 149, loss_ca: 1.7681, adv_loss: 0.5380\n",
      "Epoch 24, Batch 150, loss_ca: 1.6968, adv_loss: 0.5418\n",
      "Epoch 24, Batch 151, loss_ca: 1.6479, adv_loss: 0.5621\n",
      "Epoch 24, Batch 152, loss_ca: 1.6592, adv_loss: 0.5518\n",
      "Epoch 24, Batch 153, loss_ca: 1.7735, adv_loss: 0.5514\n",
      "Epoch 24, Batch 154, loss_ca: 1.8239, adv_loss: 0.5403\n",
      "Epoch 24, Batch 155, loss_ca: 1.8194, adv_loss: 0.5462\n",
      "Epoch 24, Batch 156, loss_ca: 1.8465, adv_loss: 0.5549\n",
      "Epoch 24, Batch 157, loss_ca: 1.8396, adv_loss: 0.5176\n",
      "Epoch 24, Batch 158, loss_ca: 2.0861, adv_loss: 0.5624\n",
      "Epoch 24, Batch 159, loss_ca: 2.2979, adv_loss: 0.6119\n",
      "Epoch 24, Batch 160, loss_ca: 2.1023, adv_loss: 0.5820\n",
      "Epoch 24, Batch 161, loss_ca: 1.9419, adv_loss: 0.4907\n",
      "Epoch 24, Batch 162, loss_ca: 2.2984, adv_loss: 0.5165\n",
      "Epoch 24, Batch 163, loss_ca: 2.0959, adv_loss: 0.5104\n",
      "Epoch 24, Batch 164, loss_ca: 2.2393, adv_loss: 0.5491\n",
      "Epoch 24, Batch 165, loss_ca: 2.3288, adv_loss: 0.5541\n",
      "Epoch 24, Batch 166, loss_ca: 2.1399, adv_loss: 0.5597\n",
      "Epoch 24, Batch 167, loss_ca: 2.0921, adv_loss: 0.5789\n",
      "Epoch 24, Batch 168, loss_ca: 2.1277, adv_loss: 0.5802\n",
      "Epoch 24, Batch 169, loss_ca: 2.1313, adv_loss: 0.5738\n",
      "Epoch 24, Batch 170, loss_ca: 2.1329, adv_loss: 0.5362\n",
      "Epoch 24, Batch 171, loss_ca: 2.1997, adv_loss: 0.5539\n",
      "Epoch 24, Batch 172, loss_ca: 2.2081, adv_loss: 0.5712\n",
      "Epoch 24, Batch 173, loss_ca: 2.3720, adv_loss: 0.5954\n",
      "Epoch 24, Batch 174, loss_ca: 2.1591, adv_loss: 0.5084\n",
      "Epoch 24, Batch 175, loss_ca: 1.9434, adv_loss: 0.4609\n",
      "Epoch 24, Batch 176, loss_ca: 1.9413, adv_loss: 0.4218\n",
      "Epoch 24, Batch 177, loss_ca: 1.7829, adv_loss: 0.4291\n",
      "Epoch 24, Batch 178, loss_ca: 1.8221, adv_loss: 0.4673\n",
      "Epoch 24, Batch 179, loss_ca: 1.8089, adv_loss: 0.4660\n",
      "Epoch 24, Batch 180, loss_ca: 1.7345, adv_loss: 0.4583\n",
      "Epoch 24, Batch 181, loss_ca: 1.7077, adv_loss: 0.4899\n",
      "Epoch 24, Batch 182, loss_ca: 1.6927, adv_loss: 0.4962\n",
      "Epoch 24, Batch 183, loss_ca: 1.9999, adv_loss: 0.5366\n",
      "Epoch 24, Batch 184, loss_ca: 1.7955, adv_loss: 0.5084\n",
      "Epoch 24, Batch 185, loss_ca: 1.7989, adv_loss: 0.5131\n",
      "Epoch 24, Batch 186, loss_ca: 1.7529, adv_loss: 0.5120\n",
      "Epoch 24, Batch 187, loss_ca: 1.7518, adv_loss: 0.4865\n",
      "Epoch 24, Batch 188, loss_ca: 1.7234, adv_loss: 0.4828\n",
      "Epoch 24, Batch 189, loss_ca: 1.8451, adv_loss: 0.4957\n",
      "Epoch 24, Batch 190, loss_ca: 2.3394, adv_loss: 0.5115\n",
      "Epoch 24, Batch 191, loss_ca: 2.1330, adv_loss: 0.4974\n",
      "Epoch 24, Batch 192, loss_ca: 2.2773, adv_loss: 0.5398\n",
      "Epoch 24, Batch 193, loss_ca: 1.9188, adv_loss: 0.5400\n",
      "Epoch 24, Batch 194, loss_ca: 1.7657, adv_loss: 0.5516\n",
      "Epoch 24, Batch 195, loss_ca: 1.7876, adv_loss: 0.5657\n",
      "Epoch 24, Batch 196, loss_ca: 2.2648, adv_loss: 0.5708\n",
      "Epoch 24, Batch 197, loss_ca: 1.8210, adv_loss: 0.5227\n",
      "Epoch 24, Batch 198, loss_ca: 1.9462, adv_loss: 0.6182\n",
      "Epoch 24, Batch 199, loss_ca: 1.8684, adv_loss: 0.6041\n",
      "Epoch 24, Batch 200, loss_ca: 1.7874, adv_loss: 0.5546\n",
      "Epoch 24, Batch 201, loss_ca: 2.0199, adv_loss: 0.5309\n",
      "Epoch 24, Batch 202, loss_ca: 2.0267, adv_loss: 0.5303\n",
      "Epoch 24, Batch 203, loss_ca: 2.4513, adv_loss: 0.5699\n",
      "Epoch 24, Batch 204, loss_ca: 1.9636, adv_loss: 0.5027\n",
      "Epoch 24, Batch 205, loss_ca: 2.1227, adv_loss: 0.5165\n",
      "Epoch 24, Batch 206, loss_ca: 2.0311, adv_loss: 0.5139\n",
      "Epoch 24, Batch 207, loss_ca: 2.1807, adv_loss: 0.5450\n",
      "Epoch 24, Batch 208, loss_ca: 1.9677, adv_loss: 0.5291\n",
      "Epoch 24, Batch 209, loss_ca: 2.1517, adv_loss: 0.5910\n",
      "Epoch 24, Batch 210, loss_ca: 1.8919, adv_loss: 0.5711\n",
      "Epoch 24, Batch 211, loss_ca: 1.8574, adv_loss: 0.5876\n",
      "Epoch 24, Batch 212, loss_ca: 1.6500, adv_loss: 0.5952\n",
      "Epoch 24, Batch 213, loss_ca: 2.0604, adv_loss: 0.5947\n",
      "Epoch 24, Batch 214, loss_ca: 2.1021, adv_loss: 0.6225\n",
      "Epoch 24, Batch 215, loss_ca: 1.8060, adv_loss: 0.5770\n",
      "Epoch 24, Batch 216, loss_ca: 1.8059, adv_loss: 0.5985\n",
      "Epoch 24, Batch 217, loss_ca: 1.7564, adv_loss: 0.6200\n",
      "Epoch 24, Batch 218, loss_ca: 1.7951, adv_loss: 0.6430\n",
      "Epoch 24, Batch 219, loss_ca: 2.1476, adv_loss: 0.5710\n",
      "Epoch 24, Batch 220, loss_ca: 2.0414, adv_loss: 0.6114\n",
      "Epoch 24, Batch 221, loss_ca: 1.8418, adv_loss: 0.5503\n",
      "Epoch 24, Batch 222, loss_ca: 1.8053, adv_loss: 0.5304\n",
      "Epoch 24, Batch 223, loss_ca: 1.7624, adv_loss: 0.4999\n",
      "Epoch 24, Batch 224, loss_ca: 1.7205, adv_loss: 0.4770\n",
      "Epoch 24, Batch 225, loss_ca: 1.6760, adv_loss: 0.4568\n",
      "Epoch 24, Batch 226, loss_ca: 1.7120, adv_loss: 0.4707\n",
      "Epoch 24, Batch 227, loss_ca: 1.6840, adv_loss: 0.4948\n",
      "Epoch 24, Batch 228, loss_ca: 1.6745, adv_loss: 0.5137\n",
      "Epoch 25, Batch 25, loss_ca: 2.0820, adv_loss: 0.5347\n",
      "Epoch 25, Batch 26, loss_ca: 2.2183, adv_loss: 0.5539\n",
      "Epoch 25, Batch 27, loss_ca: 2.0745, adv_loss: 0.5577\n",
      "Epoch 25, Batch 28, loss_ca: 2.0160, adv_loss: 0.5498\n",
      "Epoch 25, Batch 29, loss_ca: 2.0288, adv_loss: 0.5613\n",
      "Epoch 25, Batch 30, loss_ca: 2.1008, adv_loss: 0.5647\n",
      "Epoch 25, Batch 31, loss_ca: 2.0143, adv_loss: 0.5496\n",
      "Epoch 25, Batch 32, loss_ca: 1.9313, adv_loss: 0.5408\n",
      "Epoch 25, Batch 33, loss_ca: 1.8818, adv_loss: 0.5550\n",
      "Epoch 25, Batch 34, loss_ca: 2.1220, adv_loss: 0.5503\n",
      "Epoch 25, Batch 35, loss_ca: 1.9590, adv_loss: 0.5610\n",
      "Epoch 25, Batch 36, loss_ca: 1.9580, adv_loss: 0.5686\n",
      "Epoch 25, Batch 37, loss_ca: 1.9550, adv_loss: 0.5650\n",
      "Epoch 25, Batch 38, loss_ca: 1.9707, adv_loss: 0.5859\n",
      "Epoch 25, Batch 39, loss_ca: 1.9318, adv_loss: 0.6113\n",
      "Epoch 25, Batch 40, loss_ca: 1.9902, adv_loss: 0.6084\n",
      "Epoch 25, Batch 41, loss_ca: 2.0411, adv_loss: 0.5951\n",
      "Epoch 25, Batch 42, loss_ca: 2.0209, adv_loss: 0.6070\n",
      "Epoch 25, Batch 43, loss_ca: 1.9639, adv_loss: 0.6034\n",
      "Epoch 25, Batch 44, loss_ca: 1.8624, adv_loss: 0.6006\n",
      "Epoch 25, Batch 45, loss_ca: 1.7882, adv_loss: 0.5967\n",
      "Epoch 25, Batch 46, loss_ca: 1.7880, adv_loss: 0.5830\n",
      "Epoch 25, Batch 47, loss_ca: 1.8329, adv_loss: 0.5661\n",
      "Epoch 25, Batch 48, loss_ca: 1.8316, adv_loss: 0.5601\n",
      "Epoch 25, Batch 49, loss_ca: 1.7668, adv_loss: 0.5426\n",
      "Epoch 25, Batch 50, loss_ca: 1.7568, adv_loss: 0.5234\n",
      "Epoch 25, Batch 51, loss_ca: 1.7808, adv_loss: 0.5231\n",
      "Epoch 25, Batch 52, loss_ca: 1.7987, adv_loss: 0.5352\n",
      "Epoch 25, Batch 53, loss_ca: 1.8224, adv_loss: 0.5438\n",
      "Epoch 25, Batch 54, loss_ca: 1.7942, adv_loss: 0.5385\n",
      "Epoch 25, Batch 55, loss_ca: 1.7501, adv_loss: 0.5364\n",
      "Epoch 25, Batch 56, loss_ca: 1.7158, adv_loss: 0.5327\n",
      "Epoch 25, Batch 57, loss_ca: 1.7100, adv_loss: 0.5265\n",
      "Epoch 25, Batch 58, loss_ca: 1.8540, adv_loss: 0.5523\n",
      "Epoch 25, Batch 59, loss_ca: 1.7813, adv_loss: 0.5281\n",
      "Epoch 25, Batch 60, loss_ca: 1.7999, adv_loss: 0.5369\n",
      "Epoch 25, Batch 61, loss_ca: 1.7789, adv_loss: 0.5445\n",
      "Epoch 25, Batch 62, loss_ca: 1.9100, adv_loss: 0.5560\n",
      "Epoch 25, Batch 63, loss_ca: 1.8450, adv_loss: 0.5341\n",
      "Epoch 25, Batch 64, loss_ca: 1.8894, adv_loss: 0.5424\n",
      "Epoch 25, Batch 65, loss_ca: 1.7867, adv_loss: 0.5189\n",
      "Epoch 25, Batch 66, loss_ca: 1.7660, adv_loss: 0.5426\n",
      "Epoch 25, Batch 67, loss_ca: 1.7987, adv_loss: 0.5338\n",
      "Epoch 25, Batch 68, loss_ca: 1.7428, adv_loss: 0.5223\n",
      "Epoch 25, Batch 69, loss_ca: 1.7828, adv_loss: 0.5214\n",
      "Epoch 25, Batch 70, loss_ca: 1.9067, adv_loss: 0.5255\n",
      "Epoch 25, Batch 71, loss_ca: 1.8137, adv_loss: 0.4962\n",
      "Epoch 25, Batch 72, loss_ca: 1.8196, adv_loss: 0.5025\n",
      "Epoch 25, Batch 73, loss_ca: 1.7499, adv_loss: 0.5078\n",
      "Epoch 25, Batch 74, loss_ca: 1.6970, adv_loss: 0.5108\n",
      "Epoch 25, Batch 75, loss_ca: 1.6503, adv_loss: 0.5186\n",
      "Epoch 25, Batch 76, loss_ca: 1.6108, adv_loss: 0.4968\n",
      "Epoch 25, Batch 77, loss_ca: 1.6146, adv_loss: 0.4970\n",
      "Epoch 25, Batch 78, loss_ca: 1.6592, adv_loss: 0.4985\n",
      "Epoch 25, Batch 79, loss_ca: 1.6834, adv_loss: 0.5060\n",
      "Epoch 25, Batch 80, loss_ca: 1.6424, adv_loss: 0.5098\n",
      "Epoch 25, Batch 81, loss_ca: 1.6655, adv_loss: 0.5232\n",
      "Epoch 25, Batch 82, loss_ca: 1.6823, adv_loss: 0.5243\n",
      "Epoch 25, Batch 83, loss_ca: 1.7033, adv_loss: 0.5212\n",
      "Epoch 25, Batch 84, loss_ca: 1.7236, adv_loss: 0.5219\n",
      "Epoch 25, Batch 85, loss_ca: 1.7284, adv_loss: 0.5245\n",
      "Epoch 25, Batch 86, loss_ca: 1.8060, adv_loss: 0.5295\n",
      "Epoch 25, Batch 87, loss_ca: 1.7878, adv_loss: 0.5292\n",
      "Epoch 25, Batch 88, loss_ca: 1.7892, adv_loss: 0.5164\n",
      "Epoch 25, Batch 89, loss_ca: 1.8230, adv_loss: 0.5220\n",
      "Epoch 25, Batch 90, loss_ca: 1.7324, adv_loss: 0.4970\n",
      "Epoch 25, Batch 91, loss_ca: 1.8655, adv_loss: 0.5264\n",
      "Epoch 25, Batch 92, loss_ca: 1.8057, adv_loss: 0.4992\n",
      "Epoch 25, Batch 93, loss_ca: 1.9045, adv_loss: 0.5144\n",
      "Epoch 25, Batch 94, loss_ca: 1.7958, adv_loss: 0.5053\n",
      "Epoch 25, Batch 95, loss_ca: 1.8084, adv_loss: 0.5079\n",
      "Epoch 25, Batch 96, loss_ca: 1.8914, adv_loss: 0.5339\n",
      "Epoch 25, Batch 97, loss_ca: 1.8808, adv_loss: 0.5365\n",
      "Epoch 25, Batch 98, loss_ca: 1.8849, adv_loss: 0.5473\n",
      "Epoch 25, Batch 99, loss_ca: 1.7270, adv_loss: 0.5146\n",
      "Epoch 25, Batch 100, loss_ca: 1.8916, adv_loss: 0.5035\n",
      "Epoch 25, Batch 101, loss_ca: 1.9600, adv_loss: 0.5246\n",
      "Epoch 25, Batch 102, loss_ca: 1.8852, adv_loss: 0.5334\n",
      "Epoch 25, Batch 103, loss_ca: 1.9099, adv_loss: 0.5373\n",
      "Epoch 25, Batch 104, loss_ca: 1.8501, adv_loss: 0.5213\n",
      "Epoch 25, Batch 105, loss_ca: 1.9318, adv_loss: 0.5603\n",
      "Epoch 25, Batch 106, loss_ca: 1.9158, adv_loss: 0.5710\n",
      "Epoch 25, Batch 107, loss_ca: 2.0381, adv_loss: 0.5578\n",
      "Epoch 25, Batch 108, loss_ca: 1.9127, adv_loss: 0.5514\n",
      "Epoch 25, Batch 109, loss_ca: 1.9804, adv_loss: 0.5842\n",
      "Epoch 25, Batch 110, loss_ca: 1.8500, adv_loss: 0.5826\n",
      "Epoch 25, Batch 111, loss_ca: 1.8438, adv_loss: 0.5820\n",
      "Epoch 25, Batch 112, loss_ca: 1.8736, adv_loss: 0.5716\n",
      "Epoch 25, Batch 113, loss_ca: 1.8337, adv_loss: 0.5598\n",
      "Epoch 25, Batch 114, loss_ca: 1.8207, adv_loss: 0.5460\n",
      "Epoch 25, Batch 115, loss_ca: 1.7508, adv_loss: 0.5324\n",
      "Epoch 25, Batch 116, loss_ca: 1.8855, adv_loss: 0.5383\n",
      "Epoch 25, Batch 117, loss_ca: 1.8830, adv_loss: 0.5544\n",
      "Epoch 25, Batch 118, loss_ca: 1.8545, adv_loss: 0.5479\n",
      "Epoch 25, Batch 119, loss_ca: 1.7538, adv_loss: 0.5367\n",
      "Epoch 25, Batch 120, loss_ca: 1.7255, adv_loss: 0.5316\n",
      "Epoch 25, Batch 121, loss_ca: 1.8659, adv_loss: 0.5280\n",
      "Epoch 25, Batch 122, loss_ca: 1.9354, adv_loss: 0.5778\n",
      "Epoch 25, Batch 123, loss_ca: 1.9393, adv_loss: 0.5781\n",
      "Epoch 25, Batch 124, loss_ca: 1.9252, adv_loss: 0.5792\n",
      "Epoch 25, Batch 125, loss_ca: 1.8318, adv_loss: 0.5494\n",
      "Epoch 25, Batch 126, loss_ca: 2.0060, adv_loss: 0.5331\n",
      "Epoch 25, Batch 127, loss_ca: 1.9782, adv_loss: 0.5410\n",
      "Epoch 25, Batch 128, loss_ca: 1.9807, adv_loss: 0.5284\n",
      "Epoch 25, Batch 129, loss_ca: 1.9666, adv_loss: 0.5446\n",
      "Epoch 25, Batch 130, loss_ca: 1.9908, adv_loss: 0.5663\n",
      "Epoch 25, Batch 131, loss_ca: 1.9416, adv_loss: 0.4897\n",
      "Epoch 25, Batch 132, loss_ca: 2.0093, adv_loss: 0.5797\n",
      "Epoch 25, Batch 133, loss_ca: 1.8960, adv_loss: 0.5918\n",
      "Epoch 25, Batch 134, loss_ca: 1.8966, adv_loss: 0.5789\n",
      "Epoch 25, Batch 135, loss_ca: 1.8792, adv_loss: 0.5456\n",
      "Epoch 25, Batch 136, loss_ca: 1.8184, adv_loss: 0.4679\n",
      "Epoch 25, Batch 137, loss_ca: 1.7608, adv_loss: 0.4465\n",
      "Epoch 25, Batch 138, loss_ca: 1.8095, adv_loss: 0.4676\n",
      "Epoch 25, Batch 139, loss_ca: 1.8133, adv_loss: 0.4933\n",
      "Epoch 25, Batch 140, loss_ca: 1.7949, adv_loss: 0.5080\n",
      "Epoch 25, Batch 141, loss_ca: 1.7560, adv_loss: 0.5059\n",
      "Epoch 25, Batch 142, loss_ca: 1.9616, adv_loss: 0.5851\n",
      "Epoch 25, Batch 143, loss_ca: 1.8306, adv_loss: 0.5568\n",
      "Epoch 25, Batch 144, loss_ca: 1.7870, adv_loss: 0.5185\n",
      "Epoch 25, Batch 145, loss_ca: 1.7710, adv_loss: 0.4736\n",
      "Epoch 25, Batch 146, loss_ca: 1.7498, adv_loss: 0.4452\n",
      "Epoch 25, Batch 147, loss_ca: 1.8592, adv_loss: 0.4434\n",
      "Epoch 25, Batch 148, loss_ca: 1.8896, adv_loss: 0.4662\n",
      "Epoch 25, Batch 149, loss_ca: 1.7809, adv_loss: 0.4786\n",
      "Epoch 25, Batch 150, loss_ca: 1.6994, adv_loss: 0.4892\n",
      "Epoch 25, Batch 151, loss_ca: 1.6632, adv_loss: 0.5155\n",
      "Epoch 25, Batch 152, loss_ca: 1.6411, adv_loss: 0.5248\n",
      "Epoch 25, Batch 153, loss_ca: 1.8082, adv_loss: 0.5342\n",
      "Epoch 25, Batch 154, loss_ca: 1.8275, adv_loss: 0.5414\n",
      "Epoch 25, Batch 155, loss_ca: 1.8261, adv_loss: 0.5450\n",
      "Epoch 25, Batch 156, loss_ca: 1.8754, adv_loss: 0.5539\n",
      "Epoch 25, Batch 157, loss_ca: 1.9153, adv_loss: 0.5414\n",
      "Epoch 25, Batch 158, loss_ca: 1.9701, adv_loss: 0.5241\n",
      "Epoch 25, Batch 159, loss_ca: 2.2765, adv_loss: 0.5780\n",
      "Epoch 25, Batch 160, loss_ca: 2.1611, adv_loss: 0.5717\n",
      "Epoch 25, Batch 161, loss_ca: 2.0296, adv_loss: 0.5436\n",
      "Epoch 25, Batch 162, loss_ca: 2.2519, adv_loss: 0.5884\n",
      "Epoch 25, Batch 163, loss_ca: 2.1124, adv_loss: 0.5592\n",
      "Epoch 25, Batch 164, loss_ca: 2.2650, adv_loss: 0.5762\n",
      "Epoch 25, Batch 165, loss_ca: 2.2303, adv_loss: 0.5385\n",
      "Epoch 25, Batch 166, loss_ca: 2.1825, adv_loss: 0.5517\n",
      "Epoch 25, Batch 167, loss_ca: 2.1920, adv_loss: 0.5640\n",
      "Epoch 25, Batch 168, loss_ca: 2.2356, adv_loss: 0.5762\n",
      "Epoch 25, Batch 169, loss_ca: 2.1708, adv_loss: 0.5635\n",
      "Epoch 25, Batch 170, loss_ca: 2.1742, adv_loss: 0.5343\n",
      "Epoch 25, Batch 171, loss_ca: 2.1786, adv_loss: 0.5404\n",
      "Epoch 25, Batch 172, loss_ca: 2.2241, adv_loss: 0.5341\n",
      "Epoch 25, Batch 173, loss_ca: 2.0596, adv_loss: 0.5292\n",
      "Epoch 25, Batch 174, loss_ca: 1.9654, adv_loss: 0.4958\n",
      "Epoch 25, Batch 175, loss_ca: 1.9582, adv_loss: 0.5049\n",
      "Epoch 25, Batch 176, loss_ca: 1.8867, adv_loss: 0.5080\n",
      "Epoch 25, Batch 177, loss_ca: 1.7795, adv_loss: 0.5092\n",
      "Epoch 25, Batch 178, loss_ca: 1.8443, adv_loss: 0.4818\n",
      "Epoch 25, Batch 179, loss_ca: 1.7998, adv_loss: 0.4906\n",
      "Epoch 25, Batch 180, loss_ca: 1.6758, adv_loss: 0.4882\n",
      "Epoch 25, Batch 181, loss_ca: 1.7715, adv_loss: 0.4936\n",
      "Epoch 25, Batch 182, loss_ca: 1.7499, adv_loss: 0.4909\n",
      "Epoch 25, Batch 183, loss_ca: 1.8831, adv_loss: 0.4846\n",
      "Epoch 25, Batch 184, loss_ca: 1.7547, adv_loss: 0.4855\n",
      "Epoch 25, Batch 185, loss_ca: 1.7394, adv_loss: 0.4864\n",
      "Epoch 25, Batch 186, loss_ca: 1.7309, adv_loss: 0.4884\n",
      "Epoch 25, Batch 187, loss_ca: 1.7495, adv_loss: 0.4748\n",
      "Epoch 25, Batch 188, loss_ca: 1.7657, adv_loss: 0.4711\n",
      "Epoch 25, Batch 189, loss_ca: 1.8153, adv_loss: 0.4674\n",
      "Epoch 25, Batch 190, loss_ca: 1.9932, adv_loss: 0.4801\n",
      "Epoch 25, Batch 191, loss_ca: 1.9443, adv_loss: 0.4555\n",
      "Epoch 25, Batch 192, loss_ca: 2.1129, adv_loss: 0.4994\n",
      "Epoch 25, Batch 193, loss_ca: 2.3414, adv_loss: 0.5546\n",
      "Epoch 25, Batch 194, loss_ca: 2.2462, adv_loss: 0.5129\n",
      "Epoch 25, Batch 195, loss_ca: 1.9893, adv_loss: 0.5064\n",
      "Epoch 25, Batch 196, loss_ca: 2.3058, adv_loss: 0.5392\n",
      "Epoch 25, Batch 197, loss_ca: 2.1106, adv_loss: 0.5283\n",
      "Epoch 25, Batch 198, loss_ca: 2.4079, adv_loss: 0.5157\n",
      "Epoch 25, Batch 199, loss_ca: 2.1463, adv_loss: 0.5236\n",
      "Epoch 25, Batch 200, loss_ca: 1.8738, adv_loss: 0.5641\n",
      "Epoch 25, Batch 201, loss_ca: 1.9824, adv_loss: 0.5097\n",
      "Epoch 25, Batch 202, loss_ca: 1.9622, adv_loss: 0.4705\n",
      "Epoch 25, Batch 203, loss_ca: 2.2388, adv_loss: 0.4963\n",
      "Epoch 25, Batch 204, loss_ca: 1.8804, adv_loss: 0.5080\n",
      "Epoch 25, Batch 205, loss_ca: 2.1497, adv_loss: 0.5262\n",
      "Epoch 25, Batch 206, loss_ca: 1.9556, adv_loss: 0.4995\n",
      "Epoch 25, Batch 207, loss_ca: 2.2041, adv_loss: 0.5347\n",
      "Epoch 25, Batch 208, loss_ca: 2.0474, adv_loss: 0.5377\n",
      "Epoch 25, Batch 209, loss_ca: 1.9213, adv_loss: 0.4983\n",
      "Epoch 25, Batch 210, loss_ca: 1.9093, adv_loss: 0.5201\n",
      "Epoch 25, Batch 211, loss_ca: 1.8775, adv_loss: 0.5583\n",
      "Epoch 25, Batch 212, loss_ca: 1.5698, adv_loss: 0.5418\n",
      "Epoch 25, Batch 213, loss_ca: 1.9222, adv_loss: 0.5529\n",
      "Epoch 25, Batch 214, loss_ca: 1.8310, adv_loss: 0.5756\n",
      "Epoch 25, Batch 215, loss_ca: 1.6293, adv_loss: 0.5950\n",
      "Epoch 25, Batch 216, loss_ca: 1.7763, adv_loss: 0.6101\n",
      "Epoch 25, Batch 217, loss_ca: 1.7492, adv_loss: 0.6199\n",
      "Epoch 25, Batch 218, loss_ca: 1.7057, adv_loss: 0.6266\n",
      "Epoch 25, Batch 219, loss_ca: 2.2291, adv_loss: 0.6232\n",
      "Epoch 25, Batch 220, loss_ca: 1.9088, adv_loss: 0.6337\n",
      "Epoch 25, Batch 221, loss_ca: 1.8363, adv_loss: 0.6227\n",
      "Epoch 25, Batch 222, loss_ca: 1.8059, adv_loss: 0.6021\n",
      "Epoch 25, Batch 223, loss_ca: 1.7862, adv_loss: 0.5998\n",
      "Epoch 25, Batch 224, loss_ca: 1.8700, adv_loss: 0.5725\n",
      "Epoch 25, Batch 225, loss_ca: 1.8206, adv_loss: 0.5397\n",
      "Epoch 25, Batch 226, loss_ca: 1.7906, adv_loss: 0.5045\n",
      "Epoch 25, Batch 227, loss_ca: 1.7522, adv_loss: 0.5097\n",
      "Epoch 25, Batch 228, loss_ca: 1.7972, adv_loss: 0.5098\n",
      "Epoch 26, Batch 26, loss_ca: 2.1535, adv_loss: 0.5497\n",
      "Epoch 26, Batch 27, loss_ca: 2.0581, adv_loss: 0.5725\n",
      "Epoch 26, Batch 28, loss_ca: 1.9646, adv_loss: 0.5584\n",
      "Epoch 26, Batch 29, loss_ca: 1.9746, adv_loss: 0.5571\n",
      "Epoch 26, Batch 30, loss_ca: 1.9803, adv_loss: 0.5475\n",
      "Epoch 26, Batch 31, loss_ca: 1.9247, adv_loss: 0.5476\n",
      "Epoch 26, Batch 32, loss_ca: 1.8082, adv_loss: 0.5298\n",
      "Epoch 26, Batch 33, loss_ca: 1.8507, adv_loss: 0.4849\n",
      "Epoch 26, Batch 34, loss_ca: 2.0479, adv_loss: 0.5180\n",
      "Epoch 26, Batch 35, loss_ca: 2.0070, adv_loss: 0.5465\n",
      "Epoch 26, Batch 36, loss_ca: 1.9873, adv_loss: 0.5442\n",
      "Epoch 26, Batch 37, loss_ca: 2.0128, adv_loss: 0.5589\n",
      "Epoch 26, Batch 38, loss_ca: 2.0735, adv_loss: 0.5925\n",
      "Epoch 26, Batch 39, loss_ca: 2.0318, adv_loss: 0.6120\n",
      "Epoch 26, Batch 40, loss_ca: 1.9592, adv_loss: 0.5995\n",
      "Epoch 26, Batch 41, loss_ca: 2.0600, adv_loss: 0.5527\n",
      "Epoch 26, Batch 42, loss_ca: 2.0702, adv_loss: 0.5379\n",
      "Epoch 26, Batch 43, loss_ca: 1.9453, adv_loss: 0.5284\n",
      "Epoch 26, Batch 44, loss_ca: 1.8057, adv_loss: 0.5185\n",
      "Epoch 26, Batch 45, loss_ca: 1.7510, adv_loss: 0.5151\n",
      "Epoch 26, Batch 46, loss_ca: 1.7695, adv_loss: 0.5185\n",
      "Epoch 26, Batch 47, loss_ca: 1.7790, adv_loss: 0.5396\n",
      "Epoch 26, Batch 48, loss_ca: 1.8002, adv_loss: 0.5688\n",
      "Epoch 26, Batch 49, loss_ca: 1.7896, adv_loss: 0.5678\n",
      "Epoch 26, Batch 50, loss_ca: 1.7882, adv_loss: 0.5597\n",
      "Epoch 26, Batch 51, loss_ca: 1.7861, adv_loss: 0.5524\n",
      "Epoch 26, Batch 52, loss_ca: 1.8156, adv_loss: 0.5526\n",
      "Epoch 26, Batch 53, loss_ca: 1.8226, adv_loss: 0.5429\n",
      "Epoch 26, Batch 54, loss_ca: 1.7781, adv_loss: 0.5425\n",
      "Epoch 26, Batch 55, loss_ca: 1.7504, adv_loss: 0.5390\n",
      "Epoch 26, Batch 56, loss_ca: 1.7337, adv_loss: 0.5409\n",
      "Epoch 26, Batch 57, loss_ca: 1.6937, adv_loss: 0.5234\n",
      "Epoch 26, Batch 58, loss_ca: 1.8244, adv_loss: 0.5165\n",
      "Epoch 26, Batch 59, loss_ca: 1.8114, adv_loss: 0.5489\n",
      "Epoch 26, Batch 60, loss_ca: 1.7658, adv_loss: 0.5210\n",
      "Epoch 26, Batch 61, loss_ca: 1.7536, adv_loss: 0.5173\n",
      "Epoch 26, Batch 62, loss_ca: 1.8034, adv_loss: 0.5134\n",
      "Epoch 26, Batch 63, loss_ca: 1.8958, adv_loss: 0.4844\n",
      "Epoch 26, Batch 64, loss_ca: 1.8927, adv_loss: 0.5107\n",
      "Epoch 26, Batch 65, loss_ca: 1.7960, adv_loss: 0.5032\n",
      "Epoch 26, Batch 66, loss_ca: 1.8023, adv_loss: 0.5586\n",
      "Epoch 26, Batch 67, loss_ca: 1.7924, adv_loss: 0.5397\n",
      "Epoch 26, Batch 68, loss_ca: 1.8565, adv_loss: 0.5440\n",
      "Epoch 26, Batch 69, loss_ca: 1.7893, adv_loss: 0.5235\n",
      "Epoch 26, Batch 70, loss_ca: 1.7861, adv_loss: 0.5267\n",
      "Epoch 26, Batch 71, loss_ca: 1.7115, adv_loss: 0.5194\n",
      "Epoch 26, Batch 72, loss_ca: 1.7478, adv_loss: 0.5003\n",
      "Epoch 26, Batch 73, loss_ca: 1.7656, adv_loss: 0.4927\n",
      "Epoch 26, Batch 74, loss_ca: 1.6968, adv_loss: 0.4894\n",
      "Epoch 26, Batch 75, loss_ca: 1.6855, adv_loss: 0.4877\n",
      "Epoch 26, Batch 76, loss_ca: 1.6389, adv_loss: 0.4942\n",
      "Epoch 26, Batch 77, loss_ca: 1.6451, adv_loss: 0.4935\n",
      "Epoch 26, Batch 78, loss_ca: 1.6936, adv_loss: 0.5002\n",
      "Epoch 26, Batch 79, loss_ca: 1.6884, adv_loss: 0.5003\n",
      "Epoch 26, Batch 80, loss_ca: 1.6572, adv_loss: 0.5022\n",
      "Epoch 26, Batch 81, loss_ca: 1.6488, adv_loss: 0.4973\n",
      "Epoch 26, Batch 82, loss_ca: 1.7018, adv_loss: 0.5006\n",
      "Epoch 26, Batch 83, loss_ca: 1.7418, adv_loss: 0.5042\n",
      "Epoch 26, Batch 84, loss_ca: 1.7638, adv_loss: 0.5093\n",
      "Epoch 26, Batch 85, loss_ca: 1.7556, adv_loss: 0.5069\n",
      "Epoch 26, Batch 86, loss_ca: 1.8123, adv_loss: 0.5150\n",
      "Epoch 26, Batch 87, loss_ca: 1.7607, adv_loss: 0.5222\n",
      "Epoch 26, Batch 88, loss_ca: 1.7654, adv_loss: 0.5074\n",
      "Epoch 26, Batch 89, loss_ca: 1.8268, adv_loss: 0.4993\n",
      "Epoch 26, Batch 90, loss_ca: 1.7170, adv_loss: 0.5008\n",
      "Epoch 26, Batch 91, loss_ca: 1.8385, adv_loss: 0.5207\n",
      "Epoch 26, Batch 92, loss_ca: 1.7975, adv_loss: 0.5040\n",
      "Epoch 26, Batch 93, loss_ca: 1.8749, adv_loss: 0.5157\n",
      "Epoch 26, Batch 94, loss_ca: 1.8305, adv_loss: 0.5054\n",
      "Epoch 26, Batch 95, loss_ca: 1.8870, adv_loss: 0.5081\n",
      "Epoch 26, Batch 96, loss_ca: 1.9363, adv_loss: 0.5240\n",
      "Epoch 26, Batch 97, loss_ca: 1.8739, adv_loss: 0.5272\n",
      "Epoch 26, Batch 98, loss_ca: 1.8515, adv_loss: 0.5428\n",
      "Epoch 26, Batch 99, loss_ca: 1.7611, adv_loss: 0.5358\n",
      "Epoch 26, Batch 100, loss_ca: 1.9606, adv_loss: 0.5677\n",
      "Epoch 26, Batch 101, loss_ca: 1.9927, adv_loss: 0.5904\n",
      "Epoch 26, Batch 102, loss_ca: 1.9075, adv_loss: 0.5545\n",
      "Epoch 26, Batch 103, loss_ca: 1.8771, adv_loss: 0.5512\n",
      "Epoch 26, Batch 104, loss_ca: 1.8636, adv_loss: 0.5111\n",
      "Epoch 26, Batch 105, loss_ca: 1.9354, adv_loss: 0.5690\n",
      "Epoch 26, Batch 106, loss_ca: 1.9069, adv_loss: 0.5725\n",
      "Epoch 26, Batch 107, loss_ca: 2.0553, adv_loss: 0.5738\n",
      "Epoch 26, Batch 108, loss_ca: 1.8805, adv_loss: 0.5551\n",
      "Epoch 26, Batch 109, loss_ca: 1.9143, adv_loss: 0.5299\n",
      "Epoch 26, Batch 110, loss_ca: 1.7917, adv_loss: 0.5337\n",
      "Epoch 26, Batch 111, loss_ca: 1.7727, adv_loss: 0.5501\n",
      "Epoch 26, Batch 112, loss_ca: 1.8449, adv_loss: 0.5479\n",
      "Epoch 26, Batch 113, loss_ca: 1.7629, adv_loss: 0.5464\n",
      "Epoch 26, Batch 114, loss_ca: 1.8239, adv_loss: 0.5494\n",
      "Epoch 26, Batch 115, loss_ca: 1.7579, adv_loss: 0.5455\n",
      "Epoch 26, Batch 116, loss_ca: 1.7431, adv_loss: 0.5188\n",
      "Epoch 26, Batch 117, loss_ca: 1.7882, adv_loss: 0.5321\n",
      "Epoch 26, Batch 118, loss_ca: 1.7742, adv_loss: 0.5185\n",
      "Epoch 26, Batch 119, loss_ca: 1.7242, adv_loss: 0.5021\n",
      "Epoch 26, Batch 120, loss_ca: 1.6986, adv_loss: 0.5034\n",
      "Epoch 26, Batch 121, loss_ca: 1.8877, adv_loss: 0.4946\n",
      "Epoch 26, Batch 122, loss_ca: 1.9515, adv_loss: 0.5004\n",
      "Epoch 26, Batch 123, loss_ca: 1.9372, adv_loss: 0.5122\n",
      "Epoch 26, Batch 124, loss_ca: 1.8972, adv_loss: 0.5168\n",
      "Epoch 26, Batch 125, loss_ca: 1.8871, adv_loss: 0.5382\n",
      "Epoch 26, Batch 126, loss_ca: 2.0477, adv_loss: 0.5592\n",
      "Epoch 26, Batch 127, loss_ca: 2.0400, adv_loss: 0.5585\n",
      "Epoch 26, Batch 128, loss_ca: 2.0096, adv_loss: 0.5817\n",
      "Epoch 26, Batch 129, loss_ca: 1.9355, adv_loss: 0.5645\n",
      "Epoch 26, Batch 130, loss_ca: 1.9126, adv_loss: 0.5506\n",
      "Epoch 26, Batch 131, loss_ca: 1.9321, adv_loss: 0.5208\n",
      "Epoch 26, Batch 132, loss_ca: 1.9045, adv_loss: 0.5235\n",
      "Epoch 26, Batch 133, loss_ca: 1.8906, adv_loss: 0.5182\n",
      "Epoch 26, Batch 134, loss_ca: 1.8549, adv_loss: 0.5254\n",
      "Epoch 26, Batch 135, loss_ca: 1.8677, adv_loss: 0.5320\n",
      "Epoch 26, Batch 136, loss_ca: 1.8966, adv_loss: 0.5402\n",
      "Epoch 26, Batch 137, loss_ca: 1.7704, adv_loss: 0.5194\n",
      "Epoch 26, Batch 138, loss_ca: 1.7448, adv_loss: 0.5126\n",
      "Epoch 26, Batch 139, loss_ca: 1.7297, adv_loss: 0.5195\n",
      "Epoch 26, Batch 140, loss_ca: 1.6886, adv_loss: 0.5180\n",
      "Epoch 26, Batch 141, loss_ca: 1.7204, adv_loss: 0.5136\n",
      "Epoch 26, Batch 142, loss_ca: 1.8646, adv_loss: 0.5297\n",
      "Epoch 26, Batch 143, loss_ca: 1.7641, adv_loss: 0.5522\n",
      "Epoch 26, Batch 144, loss_ca: 1.7220, adv_loss: 0.5620\n",
      "Epoch 26, Batch 145, loss_ca: 1.7143, adv_loss: 0.5616\n",
      "Epoch 26, Batch 146, loss_ca: 1.7078, adv_loss: 0.5435\n",
      "Epoch 26, Batch 147, loss_ca: 1.8118, adv_loss: 0.5467\n",
      "Epoch 26, Batch 148, loss_ca: 1.9003, adv_loss: 0.5463\n",
      "Epoch 26, Batch 149, loss_ca: 1.7286, adv_loss: 0.5375\n",
      "Epoch 26, Batch 150, loss_ca: 1.6937, adv_loss: 0.5364\n",
      "Epoch 26, Batch 151, loss_ca: 1.6347, adv_loss: 0.5322\n",
      "Epoch 26, Batch 152, loss_ca: 1.6073, adv_loss: 0.5298\n",
      "Epoch 26, Batch 153, loss_ca: 1.8120, adv_loss: 0.5210\n",
      "Epoch 26, Batch 154, loss_ca: 1.8296, adv_loss: 0.4888\n",
      "Epoch 26, Batch 155, loss_ca: 1.8571, adv_loss: 0.4971\n",
      "Epoch 26, Batch 156, loss_ca: 1.8291, adv_loss: 0.4998\n",
      "Epoch 26, Batch 157, loss_ca: 1.8423, adv_loss: 0.5016\n",
      "Epoch 26, Batch 158, loss_ca: 2.0373, adv_loss: 0.4742\n",
      "Epoch 26, Batch 159, loss_ca: 2.1623, adv_loss: 0.4612\n",
      "Epoch 26, Batch 160, loss_ca: 2.2288, adv_loss: 0.4896\n",
      "Epoch 26, Batch 161, loss_ca: 1.9794, adv_loss: 0.5601\n",
      "Epoch 26, Batch 162, loss_ca: 2.1478, adv_loss: 0.5547\n",
      "Epoch 26, Batch 163, loss_ca: 2.0492, adv_loss: 0.5554\n",
      "Epoch 26, Batch 164, loss_ca: 2.2364, adv_loss: 0.5668\n",
      "Epoch 26, Batch 165, loss_ca: 2.3231, adv_loss: 0.5675\n",
      "Epoch 26, Batch 166, loss_ca: 2.1586, adv_loss: 0.5644\n",
      "Epoch 26, Batch 167, loss_ca: 2.2420, adv_loss: 0.5638\n",
      "Epoch 26, Batch 168, loss_ca: 2.3953, adv_loss: 0.5751\n",
      "Epoch 26, Batch 169, loss_ca: 2.2687, adv_loss: 0.5881\n",
      "Epoch 26, Batch 170, loss_ca: 2.1311, adv_loss: 0.5579\n",
      "Epoch 26, Batch 171, loss_ca: 2.1384, adv_loss: 0.5767\n",
      "Epoch 26, Batch 172, loss_ca: 2.2539, adv_loss: 0.5879\n",
      "Epoch 26, Batch 173, loss_ca: 1.9422, adv_loss: 0.5303\n",
      "Epoch 26, Batch 174, loss_ca: 1.8847, adv_loss: 0.5067\n",
      "Epoch 26, Batch 175, loss_ca: 1.8183, adv_loss: 0.5079\n",
      "Epoch 26, Batch 176, loss_ca: 1.7790, adv_loss: 0.5074\n",
      "Epoch 26, Batch 177, loss_ca: 1.7316, adv_loss: 0.5163\n",
      "Epoch 26, Batch 178, loss_ca: 1.8322, adv_loss: 0.5078\n",
      "Epoch 26, Batch 179, loss_ca: 1.8176, adv_loss: 0.5103\n",
      "Epoch 26, Batch 180, loss_ca: 1.7315, adv_loss: 0.5155\n",
      "Epoch 26, Batch 181, loss_ca: 1.7430, adv_loss: 0.5272\n",
      "Epoch 26, Batch 182, loss_ca: 1.6997, adv_loss: 0.5103\n",
      "Epoch 26, Batch 183, loss_ca: 1.8822, adv_loss: 0.5160\n",
      "Epoch 26, Batch 184, loss_ca: 1.8387, adv_loss: 0.4940\n",
      "Epoch 26, Batch 185, loss_ca: 1.7536, adv_loss: 0.4981\n",
      "Epoch 26, Batch 186, loss_ca: 1.6819, adv_loss: 0.5152\n",
      "Epoch 26, Batch 187, loss_ca: 1.7009, adv_loss: 0.5026\n",
      "Epoch 26, Batch 188, loss_ca: 1.6654, adv_loss: 0.5039\n",
      "Epoch 26, Batch 189, loss_ca: 1.7308, adv_loss: 0.5004\n",
      "Epoch 26, Batch 190, loss_ca: 1.9442, adv_loss: 0.5039\n",
      "Epoch 26, Batch 191, loss_ca: 1.8758, adv_loss: 0.4980\n",
      "Epoch 26, Batch 192, loss_ca: 2.0643, adv_loss: 0.5095\n",
      "Epoch 26, Batch 193, loss_ca: 1.9670, adv_loss: 0.5015\n",
      "Epoch 26, Batch 194, loss_ca: 1.8663, adv_loss: 0.5356\n",
      "Epoch 26, Batch 195, loss_ca: 1.9279, adv_loss: 0.5796\n",
      "Epoch 26, Batch 196, loss_ca: 2.3309, adv_loss: 0.6037\n",
      "Epoch 26, Batch 197, loss_ca: 2.0380, adv_loss: 0.5700\n",
      "Epoch 26, Batch 198, loss_ca: 2.2967, adv_loss: 0.5659\n",
      "Epoch 26, Batch 199, loss_ca: 2.1016, adv_loss: 0.5854\n",
      "Epoch 26, Batch 200, loss_ca: 1.8303, adv_loss: 0.5619\n",
      "Epoch 26, Batch 201, loss_ca: 2.0219, adv_loss: 0.5345\n",
      "Epoch 26, Batch 202, loss_ca: 1.9552, adv_loss: 0.5465\n",
      "Epoch 26, Batch 203, loss_ca: 2.2156, adv_loss: 0.5252\n",
      "Epoch 26, Batch 204, loss_ca: 1.9666, adv_loss: 0.5065\n",
      "Epoch 26, Batch 205, loss_ca: 2.1572, adv_loss: 0.5232\n",
      "Epoch 26, Batch 206, loss_ca: 1.8859, adv_loss: 0.5237\n",
      "Epoch 26, Batch 207, loss_ca: 2.1154, adv_loss: 0.5557\n",
      "Epoch 26, Batch 208, loss_ca: 1.9401, adv_loss: 0.5480\n",
      "Epoch 26, Batch 209, loss_ca: 1.7407, adv_loss: 0.5533\n",
      "Epoch 26, Batch 210, loss_ca: 1.8247, adv_loss: 0.5607\n",
      "Epoch 26, Batch 211, loss_ca: 2.0196, adv_loss: 0.5934\n",
      "Epoch 26, Batch 212, loss_ca: 1.7237, adv_loss: 0.6179\n",
      "Epoch 26, Batch 213, loss_ca: 1.9609, adv_loss: 0.6076\n",
      "Epoch 26, Batch 214, loss_ca: 1.6490, adv_loss: 0.5879\n",
      "Epoch 26, Batch 215, loss_ca: 1.5755, adv_loss: 0.6124\n",
      "Epoch 26, Batch 216, loss_ca: 1.6831, adv_loss: 0.6107\n",
      "Epoch 26, Batch 217, loss_ca: 1.7286, adv_loss: 0.6402\n",
      "Epoch 26, Batch 218, loss_ca: 1.7448, adv_loss: 0.6083\n",
      "Epoch 26, Batch 219, loss_ca: 2.1862, adv_loss: 0.5507\n",
      "Epoch 26, Batch 220, loss_ca: 1.8669, adv_loss: 0.5678\n",
      "Epoch 26, Batch 221, loss_ca: 1.8326, adv_loss: 0.5502\n",
      "Epoch 26, Batch 222, loss_ca: 1.7650, adv_loss: 0.5340\n",
      "Epoch 26, Batch 223, loss_ca: 1.6965, adv_loss: 0.5213\n",
      "Epoch 26, Batch 224, loss_ca: 1.6812, adv_loss: 0.5142\n",
      "Epoch 26, Batch 225, loss_ca: 1.6608, adv_loss: 0.4998\n",
      "Epoch 26, Batch 226, loss_ca: 1.6786, adv_loss: 0.4784\n",
      "Epoch 26, Batch 227, loss_ca: 1.6696, adv_loss: 0.4885\n",
      "Epoch 26, Batch 228, loss_ca: 1.8056, adv_loss: 0.4887\n",
      "Epoch 27, Batch 27, loss_ca: 2.1100, adv_loss: 0.6109\n",
      "Epoch 27, Batch 28, loss_ca: 2.0003, adv_loss: 0.5912\n",
      "Epoch 27, Batch 29, loss_ca: 2.0301, adv_loss: 0.5757\n",
      "Epoch 27, Batch 30, loss_ca: 2.0700, adv_loss: 0.5444\n",
      "Epoch 27, Batch 31, loss_ca: 1.9931, adv_loss: 0.5446\n",
      "Epoch 27, Batch 32, loss_ca: 1.9507, adv_loss: 0.5347\n",
      "Epoch 27, Batch 33, loss_ca: 1.8902, adv_loss: 0.5130\n",
      "Epoch 27, Batch 34, loss_ca: 2.1221, adv_loss: 0.5397\n",
      "Epoch 27, Batch 35, loss_ca: 2.0260, adv_loss: 0.5578\n",
      "Epoch 27, Batch 36, loss_ca: 1.9960, adv_loss: 0.5637\n",
      "Epoch 27, Batch 37, loss_ca: 1.9851, adv_loss: 0.5563\n",
      "Epoch 27, Batch 38, loss_ca: 2.0497, adv_loss: 0.5699\n",
      "Epoch 27, Batch 39, loss_ca: 2.0330, adv_loss: 0.6032\n",
      "Epoch 27, Batch 40, loss_ca: 2.0124, adv_loss: 0.5990\n",
      "Epoch 27, Batch 41, loss_ca: 1.9576, adv_loss: 0.5671\n",
      "Epoch 27, Batch 42, loss_ca: 1.9503, adv_loss: 0.5648\n",
      "Epoch 27, Batch 43, loss_ca: 1.9415, adv_loss: 0.5705\n",
      "Epoch 27, Batch 44, loss_ca: 1.8615, adv_loss: 0.5672\n",
      "Epoch 27, Batch 45, loss_ca: 1.8263, adv_loss: 0.5634\n",
      "Epoch 27, Batch 46, loss_ca: 1.8196, adv_loss: 0.5540\n",
      "Epoch 27, Batch 47, loss_ca: 1.7857, adv_loss: 0.5549\n",
      "Epoch 27, Batch 48, loss_ca: 1.7467, adv_loss: 0.5493\n",
      "Epoch 27, Batch 49, loss_ca: 1.7383, adv_loss: 0.5468\n",
      "Epoch 27, Batch 50, loss_ca: 1.7638, adv_loss: 0.5530\n",
      "Epoch 27, Batch 51, loss_ca: 1.8029, adv_loss: 0.5543\n",
      "Epoch 27, Batch 52, loss_ca: 1.7791, adv_loss: 0.5488\n",
      "Epoch 27, Batch 53, loss_ca: 1.8008, adv_loss: 0.5689\n",
      "Epoch 27, Batch 54, loss_ca: 1.7561, adv_loss: 0.5674\n",
      "Epoch 27, Batch 55, loss_ca: 1.7324, adv_loss: 0.5570\n",
      "Epoch 27, Batch 56, loss_ca: 1.7022, adv_loss: 0.5475\n",
      "Epoch 27, Batch 57, loss_ca: 1.6934, adv_loss: 0.5292\n",
      "Epoch 27, Batch 58, loss_ca: 1.7893, adv_loss: 0.5372\n",
      "Epoch 27, Batch 59, loss_ca: 1.7612, adv_loss: 0.5246\n",
      "Epoch 27, Batch 60, loss_ca: 1.7350, adv_loss: 0.5235\n",
      "Epoch 27, Batch 61, loss_ca: 1.6952, adv_loss: 0.5316\n",
      "Epoch 27, Batch 62, loss_ca: 1.7679, adv_loss: 0.5435\n",
      "Epoch 27, Batch 63, loss_ca: 1.8425, adv_loss: 0.5466\n",
      "Epoch 27, Batch 64, loss_ca: 1.8779, adv_loss: 0.5534\n",
      "Epoch 27, Batch 65, loss_ca: 1.7530, adv_loss: 0.5376\n",
      "Epoch 27, Batch 66, loss_ca: 1.7237, adv_loss: 0.5347\n",
      "Epoch 27, Batch 67, loss_ca: 1.7555, adv_loss: 0.5538\n",
      "Epoch 27, Batch 68, loss_ca: 1.7814, adv_loss: 0.5078\n",
      "Epoch 27, Batch 69, loss_ca: 1.8514, adv_loss: 0.5055\n",
      "Epoch 27, Batch 70, loss_ca: 1.8144, adv_loss: 0.5074\n",
      "Epoch 27, Batch 71, loss_ca: 1.7770, adv_loss: 0.4909\n",
      "Epoch 27, Batch 72, loss_ca: 1.7875, adv_loss: 0.4855\n",
      "Epoch 27, Batch 73, loss_ca: 1.7045, adv_loss: 0.4897\n",
      "Epoch 27, Batch 74, loss_ca: 1.6784, adv_loss: 0.4806\n",
      "Epoch 27, Batch 75, loss_ca: 1.6369, adv_loss: 0.4831\n",
      "Epoch 27, Batch 76, loss_ca: 1.6213, adv_loss: 0.4619\n",
      "Epoch 27, Batch 77, loss_ca: 1.6149, adv_loss: 0.4639\n",
      "Epoch 27, Batch 78, loss_ca: 1.6539, adv_loss: 0.4729\n",
      "Epoch 27, Batch 79, loss_ca: 1.6633, adv_loss: 0.4733\n",
      "Epoch 27, Batch 80, loss_ca: 1.6527, adv_loss: 0.4858\n",
      "Epoch 27, Batch 81, loss_ca: 1.6687, adv_loss: 0.4931\n",
      "Epoch 27, Batch 82, loss_ca: 1.7047, adv_loss: 0.5105\n",
      "Epoch 27, Batch 83, loss_ca: 1.7137, adv_loss: 0.5135\n",
      "Epoch 27, Batch 84, loss_ca: 1.7343, adv_loss: 0.5168\n",
      "Epoch 27, Batch 85, loss_ca: 1.8452, adv_loss: 0.5361\n",
      "Epoch 27, Batch 86, loss_ca: 1.8001, adv_loss: 0.5321\n",
      "Epoch 27, Batch 87, loss_ca: 1.8021, adv_loss: 0.5086\n",
      "Epoch 27, Batch 88, loss_ca: 1.7770, adv_loss: 0.4944\n",
      "Epoch 27, Batch 89, loss_ca: 1.8227, adv_loss: 0.4864\n",
      "Epoch 27, Batch 90, loss_ca: 1.7206, adv_loss: 0.4928\n",
      "Epoch 27, Batch 91, loss_ca: 1.8517, adv_loss: 0.5214\n",
      "Epoch 27, Batch 92, loss_ca: 1.7662, adv_loss: 0.5161\n",
      "Epoch 27, Batch 93, loss_ca: 1.8600, adv_loss: 0.5321\n",
      "Epoch 27, Batch 94, loss_ca: 1.8144, adv_loss: 0.5221\n",
      "Epoch 27, Batch 95, loss_ca: 1.8149, adv_loss: 0.4872\n",
      "Epoch 27, Batch 96, loss_ca: 1.8766, adv_loss: 0.5229\n",
      "Epoch 27, Batch 97, loss_ca: 1.8451, adv_loss: 0.5273\n",
      "Epoch 27, Batch 98, loss_ca: 1.8280, adv_loss: 0.5327\n",
      "Epoch 27, Batch 99, loss_ca: 1.7598, adv_loss: 0.5103\n",
      "Epoch 27, Batch 100, loss_ca: 1.8692, adv_loss: 0.4720\n",
      "Epoch 27, Batch 101, loss_ca: 2.0008, adv_loss: 0.4944\n",
      "Epoch 27, Batch 102, loss_ca: 1.9022, adv_loss: 0.4994\n",
      "Epoch 27, Batch 103, loss_ca: 1.8939, adv_loss: 0.5241\n",
      "Epoch 27, Batch 104, loss_ca: 1.8811, adv_loss: 0.5219\n",
      "Epoch 27, Batch 105, loss_ca: 2.0557, adv_loss: 0.5716\n",
      "Epoch 27, Batch 106, loss_ca: 1.9744, adv_loss: 0.5916\n",
      "Epoch 27, Batch 107, loss_ca: 2.0021, adv_loss: 0.5890\n",
      "Epoch 27, Batch 108, loss_ca: 1.8916, adv_loss: 0.5961\n",
      "Epoch 27, Batch 109, loss_ca: 1.8705, adv_loss: 0.5665\n",
      "Epoch 27, Batch 110, loss_ca: 1.8116, adv_loss: 0.5809\n",
      "Epoch 27, Batch 111, loss_ca: 1.7925, adv_loss: 0.5766\n",
      "Epoch 27, Batch 112, loss_ca: 1.8372, adv_loss: 0.5809\n",
      "Epoch 27, Batch 113, loss_ca: 1.7835, adv_loss: 0.5766\n",
      "Epoch 27, Batch 114, loss_ca: 1.7994, adv_loss: 0.5666\n",
      "Epoch 27, Batch 115, loss_ca: 1.7587, adv_loss: 0.5599\n",
      "Epoch 27, Batch 116, loss_ca: 1.7614, adv_loss: 0.5543\n",
      "Epoch 27, Batch 117, loss_ca: 1.7572, adv_loss: 0.5480\n",
      "Epoch 27, Batch 118, loss_ca: 1.7592, adv_loss: 0.5345\n",
      "Epoch 27, Batch 119, loss_ca: 1.7455, adv_loss: 0.5286\n",
      "Epoch 27, Batch 120, loss_ca: 1.7719, adv_loss: 0.5292\n",
      "Epoch 27, Batch 121, loss_ca: 1.9175, adv_loss: 0.5221\n",
      "Epoch 27, Batch 122, loss_ca: 1.8830, adv_loss: 0.5421\n",
      "Epoch 27, Batch 123, loss_ca: 1.8379, adv_loss: 0.5540\n",
      "Epoch 27, Batch 124, loss_ca: 1.8589, adv_loss: 0.5689\n",
      "Epoch 27, Batch 125, loss_ca: 1.8702, adv_loss: 0.5764\n",
      "Epoch 27, Batch 126, loss_ca: 1.9491, adv_loss: 0.5389\n",
      "Epoch 27, Batch 127, loss_ca: 2.0195, adv_loss: 0.5354\n",
      "Epoch 27, Batch 128, loss_ca: 2.0508, adv_loss: 0.5506\n",
      "Epoch 27, Batch 129, loss_ca: 1.9669, adv_loss: 0.5441\n",
      "Epoch 27, Batch 130, loss_ca: 1.9231, adv_loss: 0.5463\n",
      "Epoch 27, Batch 131, loss_ca: 1.8974, adv_loss: 0.5484\n",
      "Epoch 27, Batch 132, loss_ca: 1.9100, adv_loss: 0.5657\n",
      "Epoch 27, Batch 133, loss_ca: 1.9443, adv_loss: 0.5678\n",
      "Epoch 27, Batch 134, loss_ca: 1.8857, adv_loss: 0.5851\n",
      "Epoch 27, Batch 135, loss_ca: 1.8744, adv_loss: 0.5620\n",
      "Epoch 27, Batch 136, loss_ca: 1.8872, adv_loss: 0.5180\n",
      "Epoch 27, Batch 137, loss_ca: 1.7765, adv_loss: 0.4930\n",
      "Epoch 27, Batch 138, loss_ca: 1.7246, adv_loss: 0.5015\n",
      "Epoch 27, Batch 139, loss_ca: 1.6766, adv_loss: 0.4921\n",
      "Epoch 27, Batch 140, loss_ca: 1.6827, adv_loss: 0.4990\n",
      "Epoch 27, Batch 141, loss_ca: 1.7169, adv_loss: 0.5014\n",
      "Epoch 27, Batch 142, loss_ca: 1.7867, adv_loss: 0.5395\n",
      "Epoch 27, Batch 143, loss_ca: 1.7472, adv_loss: 0.5407\n",
      "Epoch 27, Batch 144, loss_ca: 1.7485, adv_loss: 0.5488\n",
      "Epoch 27, Batch 145, loss_ca: 1.7460, adv_loss: 0.5450\n",
      "Epoch 27, Batch 146, loss_ca: 1.7168, adv_loss: 0.5116\n",
      "Epoch 27, Batch 147, loss_ca: 1.8430, adv_loss: 0.5018\n",
      "Epoch 27, Batch 148, loss_ca: 1.9417, adv_loss: 0.4913\n",
      "Epoch 27, Batch 149, loss_ca: 1.7773, adv_loss: 0.5033\n",
      "Epoch 27, Batch 150, loss_ca: 1.7108, adv_loss: 0.5139\n",
      "Epoch 27, Batch 151, loss_ca: 1.6340, adv_loss: 0.5231\n",
      "Epoch 27, Batch 152, loss_ca: 1.5961, adv_loss: 0.5203\n",
      "Epoch 27, Batch 153, loss_ca: 1.7760, adv_loss: 0.5210\n",
      "Epoch 27, Batch 154, loss_ca: 1.8045, adv_loss: 0.5124\n",
      "Epoch 27, Batch 155, loss_ca: 1.8013, adv_loss: 0.5463\n",
      "Epoch 27, Batch 156, loss_ca: 1.8353, adv_loss: 0.5420\n",
      "Epoch 27, Batch 157, loss_ca: 1.7968, adv_loss: 0.4970\n",
      "Epoch 27, Batch 158, loss_ca: 1.9330, adv_loss: 0.5404\n",
      "Epoch 27, Batch 159, loss_ca: 2.1833, adv_loss: 0.5515\n",
      "Epoch 27, Batch 160, loss_ca: 2.1591, adv_loss: 0.5953\n",
      "Epoch 27, Batch 161, loss_ca: 1.8736, adv_loss: 0.5045\n",
      "Epoch 27, Batch 162, loss_ca: 2.6524, adv_loss: 0.5360\n",
      "Epoch 27, Batch 163, loss_ca: 2.3490, adv_loss: 0.5271\n",
      "Epoch 27, Batch 164, loss_ca: 2.4198, adv_loss: 0.5105\n",
      "Epoch 27, Batch 165, loss_ca: 2.3105, adv_loss: 0.5397\n",
      "Epoch 27, Batch 166, loss_ca: 2.1587, adv_loss: 0.5667\n",
      "Epoch 27, Batch 167, loss_ca: 2.0536, adv_loss: 0.5175\n",
      "Epoch 27, Batch 168, loss_ca: 2.3302, adv_loss: 0.5842\n",
      "Epoch 27, Batch 169, loss_ca: 2.3012, adv_loss: 0.5978\n",
      "Epoch 27, Batch 170, loss_ca: 2.1119, adv_loss: 0.5528\n",
      "Epoch 27, Batch 171, loss_ca: 2.2261, adv_loss: 0.5957\n",
      "Epoch 27, Batch 172, loss_ca: 2.5368, adv_loss: 0.6371\n",
      "Epoch 27, Batch 173, loss_ca: 2.2214, adv_loss: 0.5975\n",
      "Epoch 27, Batch 174, loss_ca: 1.9751, adv_loss: 0.5039\n",
      "Epoch 27, Batch 175, loss_ca: 2.0168, adv_loss: 0.4897\n",
      "Epoch 27, Batch 176, loss_ca: 1.9165, adv_loss: 0.4828\n",
      "Epoch 27, Batch 177, loss_ca: 1.7465, adv_loss: 0.4789\n",
      "Epoch 27, Batch 178, loss_ca: 1.7866, adv_loss: 0.4348\n",
      "Epoch 27, Batch 179, loss_ca: 1.7546, adv_loss: 0.4518\n",
      "Epoch 27, Batch 180, loss_ca: 1.6632, adv_loss: 0.4688\n",
      "Epoch 27, Batch 181, loss_ca: 1.7000, adv_loss: 0.4836\n",
      "Epoch 27, Batch 182, loss_ca: 1.6554, adv_loss: 0.4931\n",
      "Epoch 27, Batch 183, loss_ca: 1.8519, adv_loss: 0.5326\n",
      "Epoch 27, Batch 184, loss_ca: 1.7659, adv_loss: 0.4873\n",
      "Epoch 27, Batch 185, loss_ca: 1.8052, adv_loss: 0.4990\n",
      "Epoch 27, Batch 186, loss_ca: 1.8253, adv_loss: 0.4979\n",
      "Epoch 27, Batch 187, loss_ca: 1.8386, adv_loss: 0.5023\n",
      "Epoch 27, Batch 188, loss_ca: 1.7726, adv_loss: 0.5057\n",
      "Epoch 27, Batch 189, loss_ca: 1.8241, adv_loss: 0.5255\n",
      "Epoch 27, Batch 190, loss_ca: 1.9311, adv_loss: 0.5400\n",
      "Epoch 27, Batch 191, loss_ca: 1.9329, adv_loss: 0.5401\n",
      "Epoch 27, Batch 192, loss_ca: 2.1538, adv_loss: 0.5768\n",
      "Epoch 27, Batch 193, loss_ca: 2.0045, adv_loss: 0.5924\n",
      "Epoch 27, Batch 194, loss_ca: 1.6525, adv_loss: 0.5490\n",
      "Epoch 27, Batch 195, loss_ca: 1.7607, adv_loss: 0.5231\n",
      "Epoch 27, Batch 196, loss_ca: 2.1881, adv_loss: 0.5582\n",
      "Epoch 27, Batch 197, loss_ca: 1.8394, adv_loss: 0.5419\n",
      "Epoch 27, Batch 198, loss_ca: 1.6111, adv_loss: 0.4599\n",
      "Epoch 27, Batch 199, loss_ca: 1.5961, adv_loss: 0.4595\n",
      "Epoch 27, Batch 200, loss_ca: 1.6720, adv_loss: 0.4335\n",
      "Epoch 27, Batch 201, loss_ca: 2.0818, adv_loss: 0.5390\n",
      "Epoch 27, Batch 202, loss_ca: 2.0575, adv_loss: 0.6006\n",
      "Epoch 27, Batch 203, loss_ca: 2.5613, adv_loss: 0.5997\n",
      "Epoch 27, Batch 204, loss_ca: 1.9880, adv_loss: 0.5727\n",
      "Epoch 27, Batch 205, loss_ca: 2.1502, adv_loss: 0.5694\n",
      "Epoch 27, Batch 206, loss_ca: 2.0064, adv_loss: 0.5747\n",
      "Epoch 27, Batch 207, loss_ca: 2.0419, adv_loss: 0.5632\n",
      "Epoch 27, Batch 208, loss_ca: 1.9522, adv_loss: 0.5616\n",
      "Epoch 27, Batch 209, loss_ca: 1.7911, adv_loss: 0.5377\n",
      "Epoch 27, Batch 210, loss_ca: 1.8460, adv_loss: 0.5436\n",
      "Epoch 27, Batch 211, loss_ca: 1.8737, adv_loss: 0.5818\n",
      "Epoch 27, Batch 212, loss_ca: 1.7111, adv_loss: 0.5835\n",
      "Epoch 27, Batch 213, loss_ca: 2.0991, adv_loss: 0.6273\n",
      "Epoch 27, Batch 214, loss_ca: 1.9992, adv_loss: 0.6388\n",
      "Epoch 27, Batch 215, loss_ca: 1.6740, adv_loss: 0.6011\n",
      "Epoch 27, Batch 216, loss_ca: 1.8236, adv_loss: 0.6246\n",
      "Epoch 27, Batch 217, loss_ca: 1.6870, adv_loss: 0.6442\n",
      "Epoch 27, Batch 218, loss_ca: 1.6748, adv_loss: 0.6122\n",
      "Epoch 27, Batch 219, loss_ca: 2.0062, adv_loss: 0.4894\n",
      "Epoch 27, Batch 220, loss_ca: 1.8321, adv_loss: 0.5309\n",
      "Epoch 27, Batch 221, loss_ca: 1.6905, adv_loss: 0.5229\n",
      "Epoch 27, Batch 222, loss_ca: 1.6528, adv_loss: 0.5174\n",
      "Epoch 27, Batch 223, loss_ca: 1.6881, adv_loss: 0.5178\n",
      "Epoch 27, Batch 224, loss_ca: 1.6918, adv_loss: 0.5520\n",
      "Epoch 27, Batch 225, loss_ca: 1.6692, adv_loss: 0.5338\n",
      "Epoch 27, Batch 226, loss_ca: 1.7317, adv_loss: 0.5157\n",
      "Epoch 27, Batch 227, loss_ca: 1.7661, adv_loss: 0.5345\n",
      "Epoch 27, Batch 228, loss_ca: 1.7090, adv_loss: 0.5294\n",
      "Epoch 28, Batch 28, loss_ca: 1.8470, adv_loss: 0.5092\n",
      "Epoch 28, Batch 29, loss_ca: 1.9182, adv_loss: 0.5193\n",
      "Epoch 28, Batch 30, loss_ca: 1.9259, adv_loss: 0.5323\n",
      "Epoch 28, Batch 31, loss_ca: 1.8942, adv_loss: 0.5275\n",
      "Epoch 28, Batch 32, loss_ca: 1.8817, adv_loss: 0.5348\n",
      "Epoch 28, Batch 33, loss_ca: 1.9085, adv_loss: 0.5319\n",
      "Epoch 28, Batch 34, loss_ca: 2.0657, adv_loss: 0.5922\n",
      "Epoch 28, Batch 35, loss_ca: 1.9431, adv_loss: 0.6074\n",
      "Epoch 28, Batch 36, loss_ca: 2.0254, adv_loss: 0.6187\n",
      "Epoch 28, Batch 37, loss_ca: 1.9951, adv_loss: 0.6224\n",
      "Epoch 28, Batch 38, loss_ca: 2.0079, adv_loss: 0.6194\n",
      "Epoch 28, Batch 39, loss_ca: 2.0172, adv_loss: 0.6336\n",
      "Epoch 28, Batch 40, loss_ca: 2.0111, adv_loss: 0.6083\n",
      "Epoch 28, Batch 41, loss_ca: 1.9242, adv_loss: 0.5697\n",
      "Epoch 28, Batch 42, loss_ca: 1.8548, adv_loss: 0.5595\n",
      "Epoch 28, Batch 43, loss_ca: 1.8176, adv_loss: 0.5386\n",
      "Epoch 28, Batch 44, loss_ca: 1.7768, adv_loss: 0.5328\n",
      "Epoch 28, Batch 45, loss_ca: 1.7766, adv_loss: 0.5168\n",
      "Epoch 28, Batch 46, loss_ca: 1.7781, adv_loss: 0.5308\n",
      "Epoch 28, Batch 47, loss_ca: 1.7746, adv_loss: 0.5283\n",
      "Epoch 28, Batch 48, loss_ca: 1.8016, adv_loss: 0.5376\n",
      "Epoch 28, Batch 49, loss_ca: 1.7752, adv_loss: 0.5378\n",
      "Epoch 28, Batch 50, loss_ca: 1.7678, adv_loss: 0.5434\n",
      "Epoch 28, Batch 51, loss_ca: 1.7453, adv_loss: 0.5452\n",
      "Epoch 28, Batch 52, loss_ca: 1.8620, adv_loss: 0.5472\n",
      "Epoch 28, Batch 53, loss_ca: 1.7793, adv_loss: 0.5570\n",
      "Epoch 28, Batch 54, loss_ca: 1.7324, adv_loss: 0.5618\n",
      "Epoch 28, Batch 55, loss_ca: 1.7109, adv_loss: 0.5577\n",
      "Epoch 28, Batch 56, loss_ca: 1.7033, adv_loss: 0.5481\n",
      "Epoch 28, Batch 57, loss_ca: 1.7062, adv_loss: 0.5434\n",
      "Epoch 28, Batch 58, loss_ca: 1.9107, adv_loss: 0.5651\n",
      "Epoch 28, Batch 59, loss_ca: 1.8388, adv_loss: 0.5368\n",
      "Epoch 28, Batch 60, loss_ca: 1.7324, adv_loss: 0.5208\n",
      "Epoch 28, Batch 61, loss_ca: 1.7142, adv_loss: 0.5054\n",
      "Epoch 28, Batch 62, loss_ca: 1.7583, adv_loss: 0.4975\n",
      "Epoch 28, Batch 63, loss_ca: 1.8079, adv_loss: 0.4990\n",
      "Epoch 28, Batch 64, loss_ca: 1.8276, adv_loss: 0.5293\n",
      "Epoch 28, Batch 65, loss_ca: 1.7671, adv_loss: 0.5113\n",
      "Epoch 28, Batch 66, loss_ca: 1.7908, adv_loss: 0.5034\n",
      "Epoch 28, Batch 67, loss_ca: 1.7951, adv_loss: 0.5167\n",
      "Epoch 28, Batch 68, loss_ca: 1.8262, adv_loss: 0.5131\n",
      "Epoch 28, Batch 69, loss_ca: 1.8040, adv_loss: 0.4974\n",
      "Epoch 28, Batch 70, loss_ca: 1.7535, adv_loss: 0.5118\n",
      "Epoch 28, Batch 71, loss_ca: 1.7279, adv_loss: 0.4895\n",
      "Epoch 28, Batch 72, loss_ca: 1.7575, adv_loss: 0.5056\n",
      "Epoch 28, Batch 73, loss_ca: 1.6890, adv_loss: 0.4928\n",
      "Epoch 28, Batch 74, loss_ca: 1.6924, adv_loss: 0.4808\n",
      "Epoch 28, Batch 75, loss_ca: 1.6423, adv_loss: 0.4893\n",
      "Epoch 28, Batch 76, loss_ca: 1.6353, adv_loss: 0.4977\n",
      "Epoch 28, Batch 77, loss_ca: 1.6641, adv_loss: 0.5027\n",
      "Epoch 28, Batch 78, loss_ca: 1.6940, adv_loss: 0.4998\n",
      "Epoch 28, Batch 79, loss_ca: 1.6665, adv_loss: 0.4915\n",
      "Epoch 28, Batch 80, loss_ca: 1.6490, adv_loss: 0.4921\n",
      "Epoch 28, Batch 81, loss_ca: 1.6294, adv_loss: 0.4874\n",
      "Epoch 28, Batch 82, loss_ca: 1.6994, adv_loss: 0.5023\n",
      "Epoch 28, Batch 83, loss_ca: 1.6855, adv_loss: 0.4776\n",
      "Epoch 28, Batch 84, loss_ca: 1.6746, adv_loss: 0.4814\n",
      "Epoch 28, Batch 85, loss_ca: 1.7475, adv_loss: 0.4791\n",
      "Epoch 28, Batch 86, loss_ca: 1.7631, adv_loss: 0.4937\n",
      "Epoch 28, Batch 87, loss_ca: 1.7585, adv_loss: 0.4983\n",
      "Epoch 28, Batch 88, loss_ca: 1.7646, adv_loss: 0.5034\n",
      "Epoch 28, Batch 89, loss_ca: 1.8126, adv_loss: 0.5057\n",
      "Epoch 28, Batch 90, loss_ca: 1.7467, adv_loss: 0.4984\n",
      "Epoch 28, Batch 91, loss_ca: 1.8242, adv_loss: 0.5360\n",
      "Epoch 28, Batch 92, loss_ca: 1.7728, adv_loss: 0.5368\n",
      "Epoch 28, Batch 93, loss_ca: 1.8646, adv_loss: 0.5588\n",
      "Epoch 28, Batch 94, loss_ca: 1.8019, adv_loss: 0.5430\n",
      "Epoch 28, Batch 95, loss_ca: 1.8859, adv_loss: 0.5332\n",
      "Epoch 28, Batch 96, loss_ca: 1.8608, adv_loss: 0.5694\n",
      "Epoch 28, Batch 97, loss_ca: 1.8300, adv_loss: 0.5634\n",
      "Epoch 28, Batch 98, loss_ca: 1.7704, adv_loss: 0.5419\n",
      "Epoch 28, Batch 99, loss_ca: 1.7768, adv_loss: 0.5002\n",
      "Epoch 28, Batch 100, loss_ca: 1.9021, adv_loss: 0.4948\n",
      "Epoch 28, Batch 101, loss_ca: 1.9650, adv_loss: 0.5049\n",
      "Epoch 28, Batch 102, loss_ca: 1.9445, adv_loss: 0.5150\n",
      "Epoch 28, Batch 103, loss_ca: 1.8907, adv_loss: 0.5314\n",
      "Epoch 28, Batch 104, loss_ca: 2.0056, adv_loss: 0.5438\n",
      "Epoch 28, Batch 105, loss_ca: 2.0305, adv_loss: 0.5868\n",
      "Epoch 28, Batch 106, loss_ca: 2.0113, adv_loss: 0.6132\n",
      "Epoch 28, Batch 107, loss_ca: 2.0691, adv_loss: 0.6171\n",
      "Epoch 28, Batch 108, loss_ca: 1.8897, adv_loss: 0.5916\n",
      "Epoch 28, Batch 109, loss_ca: 1.9650, adv_loss: 0.5170\n",
      "Epoch 28, Batch 110, loss_ca: 1.8342, adv_loss: 0.5097\n",
      "Epoch 28, Batch 111, loss_ca: 1.8049, adv_loss: 0.5195\n",
      "Epoch 28, Batch 112, loss_ca: 1.8599, adv_loss: 0.5141\n",
      "Epoch 28, Batch 113, loss_ca: 1.7769, adv_loss: 0.5091\n",
      "Epoch 28, Batch 114, loss_ca: 1.7934, adv_loss: 0.5030\n",
      "Epoch 28, Batch 115, loss_ca: 1.7532, adv_loss: 0.5101\n",
      "Epoch 28, Batch 116, loss_ca: 1.8593, adv_loss: 0.5255\n",
      "Epoch 28, Batch 117, loss_ca: 1.8549, adv_loss: 0.5388\n",
      "Epoch 28, Batch 118, loss_ca: 1.8456, adv_loss: 0.5317\n",
      "Epoch 28, Batch 119, loss_ca: 1.8170, adv_loss: 0.5267\n",
      "Epoch 28, Batch 120, loss_ca: 1.7745, adv_loss: 0.5093\n",
      "Epoch 28, Batch 121, loss_ca: 1.9194, adv_loss: 0.5110\n",
      "Epoch 28, Batch 122, loss_ca: 1.9286, adv_loss: 0.5065\n",
      "Epoch 28, Batch 123, loss_ca: 1.9204, adv_loss: 0.5097\n",
      "Epoch 28, Batch 124, loss_ca: 1.9012, adv_loss: 0.5141\n",
      "Epoch 28, Batch 125, loss_ca: 1.8220, adv_loss: 0.5229\n",
      "Epoch 28, Batch 126, loss_ca: 1.8822, adv_loss: 0.5354\n",
      "Epoch 28, Batch 127, loss_ca: 1.9238, adv_loss: 0.5719\n",
      "Epoch 28, Batch 128, loss_ca: 1.9387, adv_loss: 0.5759\n",
      "Epoch 28, Batch 129, loss_ca: 1.9236, adv_loss: 0.5736\n",
      "Epoch 28, Batch 130, loss_ca: 1.8990, adv_loss: 0.5796\n",
      "Epoch 28, Batch 131, loss_ca: 1.8798, adv_loss: 0.5639\n",
      "Epoch 28, Batch 132, loss_ca: 1.9282, adv_loss: 0.5583\n",
      "Epoch 28, Batch 133, loss_ca: 1.9068, adv_loss: 0.5721\n",
      "Epoch 28, Batch 134, loss_ca: 1.9668, adv_loss: 0.5995\n",
      "Epoch 28, Batch 135, loss_ca: 1.8937, adv_loss: 0.5916\n",
      "Epoch 28, Batch 136, loss_ca: 1.8787, adv_loss: 0.5656\n",
      "Epoch 28, Batch 137, loss_ca: 1.8130, adv_loss: 0.5383\n",
      "Epoch 28, Batch 138, loss_ca: 1.7514, adv_loss: 0.5198\n",
      "Epoch 28, Batch 139, loss_ca: 1.7014, adv_loss: 0.5071\n",
      "Epoch 28, Batch 140, loss_ca: 1.6571, adv_loss: 0.4827\n",
      "Epoch 28, Batch 141, loss_ca: 1.6775, adv_loss: 0.4879\n",
      "Epoch 28, Batch 142, loss_ca: 1.8803, adv_loss: 0.5421\n",
      "Epoch 28, Batch 143, loss_ca: 1.8424, adv_loss: 0.5335\n",
      "Epoch 28, Batch 144, loss_ca: 1.8542, adv_loss: 0.5549\n",
      "Epoch 28, Batch 145, loss_ca: 1.7813, adv_loss: 0.5411\n",
      "Epoch 28, Batch 146, loss_ca: 1.7275, adv_loss: 0.5151\n",
      "Epoch 28, Batch 147, loss_ca: 1.9035, adv_loss: 0.5076\n",
      "Epoch 28, Batch 148, loss_ca: 1.9163, adv_loss: 0.5110\n",
      "Epoch 28, Batch 149, loss_ca: 1.7810, adv_loss: 0.4910\n",
      "Epoch 28, Batch 150, loss_ca: 1.7502, adv_loss: 0.4994\n",
      "Epoch 28, Batch 151, loss_ca: 1.6751, adv_loss: 0.5004\n",
      "Epoch 28, Batch 152, loss_ca: 1.6152, adv_loss: 0.4927\n",
      "Epoch 28, Batch 153, loss_ca: 1.7761, adv_loss: 0.4854\n",
      "Epoch 28, Batch 154, loss_ca: 1.8721, adv_loss: 0.4899\n",
      "Epoch 28, Batch 155, loss_ca: 1.8468, adv_loss: 0.4711\n",
      "Epoch 28, Batch 156, loss_ca: 1.8107, adv_loss: 0.4613\n",
      "Epoch 28, Batch 157, loss_ca: 1.8252, adv_loss: 0.4937\n",
      "Epoch 28, Batch 158, loss_ca: 1.9315, adv_loss: 0.5082\n",
      "Epoch 28, Batch 159, loss_ca: 2.1447, adv_loss: 0.5015\n",
      "Epoch 28, Batch 160, loss_ca: 2.0478, adv_loss: 0.5194\n",
      "Epoch 28, Batch 161, loss_ca: 1.8356, adv_loss: 0.5334\n",
      "Epoch 28, Batch 162, loss_ca: 1.9296, adv_loss: 0.4745\n",
      "Epoch 28, Batch 163, loss_ca: 1.9561, adv_loss: 0.5098\n",
      "Epoch 28, Batch 164, loss_ca: 2.1068, adv_loss: 0.4639\n",
      "Epoch 28, Batch 165, loss_ca: 2.2442, adv_loss: 0.4904\n",
      "Epoch 28, Batch 166, loss_ca: 2.0704, adv_loss: 0.5780\n",
      "Epoch 28, Batch 167, loss_ca: 1.9456, adv_loss: 0.5106\n",
      "Epoch 28, Batch 168, loss_ca: 2.0224, adv_loss: 0.5694\n",
      "Epoch 28, Batch 169, loss_ca: 2.0711, adv_loss: 0.5790\n",
      "Epoch 28, Batch 170, loss_ca: 2.1342, adv_loss: 0.5464\n",
      "Epoch 28, Batch 171, loss_ca: 2.1902, adv_loss: 0.5745\n",
      "Epoch 28, Batch 172, loss_ca: 2.2375, adv_loss: 0.5820\n",
      "Epoch 28, Batch 173, loss_ca: 2.0999, adv_loss: 0.5607\n",
      "Epoch 28, Batch 174, loss_ca: 1.9880, adv_loss: 0.5217\n",
      "Epoch 28, Batch 175, loss_ca: 1.9970, adv_loss: 0.5206\n",
      "Epoch 28, Batch 176, loss_ca: 1.8751, adv_loss: 0.5116\n",
      "Epoch 28, Batch 177, loss_ca: 1.7788, adv_loss: 0.5258\n",
      "Epoch 28, Batch 178, loss_ca: 1.8614, adv_loss: 0.5137\n",
      "Epoch 28, Batch 179, loss_ca: 1.7803, adv_loss: 0.5130\n",
      "Epoch 28, Batch 180, loss_ca: 1.7318, adv_loss: 0.5086\n",
      "Epoch 28, Batch 181, loss_ca: 1.6573, adv_loss: 0.4990\n",
      "Epoch 28, Batch 182, loss_ca: 1.7008, adv_loss: 0.4951\n",
      "Epoch 28, Batch 183, loss_ca: 1.8336, adv_loss: 0.4995\n",
      "Epoch 28, Batch 184, loss_ca: 1.7048, adv_loss: 0.4926\n",
      "Epoch 28, Batch 185, loss_ca: 1.7148, adv_loss: 0.4861\n",
      "Epoch 28, Batch 186, loss_ca: 1.6912, adv_loss: 0.4922\n",
      "Epoch 28, Batch 187, loss_ca: 1.7566, adv_loss: 0.4957\n",
      "Epoch 28, Batch 188, loss_ca: 1.7358, adv_loss: 0.4872\n",
      "Epoch 28, Batch 189, loss_ca: 1.7906, adv_loss: 0.5102\n",
      "Epoch 28, Batch 190, loss_ca: 2.0368, adv_loss: 0.5851\n",
      "Epoch 28, Batch 191, loss_ca: 2.0680, adv_loss: 0.6047\n",
      "Epoch 28, Batch 192, loss_ca: 2.3093, adv_loss: 0.5934\n",
      "Epoch 28, Batch 193, loss_ca: 1.9190, adv_loss: 0.5939\n",
      "Epoch 28, Batch 194, loss_ca: 1.8952, adv_loss: 0.5548\n",
      "Epoch 28, Batch 195, loss_ca: 1.7952, adv_loss: 0.5408\n",
      "Epoch 28, Batch 196, loss_ca: 2.2769, adv_loss: 0.5291\n",
      "Epoch 28, Batch 197, loss_ca: 1.8549, adv_loss: 0.5101\n",
      "Epoch 28, Batch 198, loss_ca: 2.2101, adv_loss: 0.5321\n",
      "Epoch 28, Batch 199, loss_ca: 2.1602, adv_loss: 0.5426\n",
      "Epoch 28, Batch 200, loss_ca: 1.7725, adv_loss: 0.5263\n",
      "Epoch 28, Batch 201, loss_ca: 1.8276, adv_loss: 0.4718\n",
      "Epoch 28, Batch 202, loss_ca: 1.9701, adv_loss: 0.4957\n",
      "Epoch 28, Batch 203, loss_ca: 2.1948, adv_loss: 0.5013\n",
      "Epoch 28, Batch 204, loss_ca: 1.9133, adv_loss: 0.4818\n",
      "Epoch 28, Batch 205, loss_ca: 2.1193, adv_loss: 0.5158\n",
      "Epoch 28, Batch 206, loss_ca: 1.8968, adv_loss: 0.5038\n",
      "Epoch 28, Batch 207, loss_ca: 2.0581, adv_loss: 0.5216\n",
      "Epoch 28, Batch 208, loss_ca: 2.0081, adv_loss: 0.5504\n",
      "Epoch 28, Batch 209, loss_ca: 1.7864, adv_loss: 0.5329\n",
      "Epoch 28, Batch 210, loss_ca: 1.8411, adv_loss: 0.5421\n",
      "Epoch 28, Batch 211, loss_ca: 1.8365, adv_loss: 0.6074\n",
      "Epoch 28, Batch 212, loss_ca: 1.5837, adv_loss: 0.6013\n",
      "Epoch 28, Batch 213, loss_ca: 1.8882, adv_loss: 0.5785\n",
      "Epoch 28, Batch 214, loss_ca: 1.7033, adv_loss: 0.5720\n",
      "Epoch 28, Batch 215, loss_ca: 1.5123, adv_loss: 0.6051\n",
      "Epoch 28, Batch 216, loss_ca: 1.6494, adv_loss: 0.6289\n",
      "Epoch 28, Batch 217, loss_ca: 1.8180, adv_loss: 0.6373\n",
      "Epoch 28, Batch 218, loss_ca: 1.9350, adv_loss: 0.6156\n",
      "Epoch 28, Batch 219, loss_ca: 2.1079, adv_loss: 0.5790\n",
      "Epoch 28, Batch 220, loss_ca: 1.9394, adv_loss: 0.5443\n",
      "Epoch 28, Batch 221, loss_ca: 1.7437, adv_loss: 0.5135\n",
      "Epoch 28, Batch 222, loss_ca: 1.6853, adv_loss: 0.5148\n",
      "Epoch 28, Batch 223, loss_ca: 1.6596, adv_loss: 0.5218\n",
      "Epoch 28, Batch 224, loss_ca: 1.6655, adv_loss: 0.5375\n",
      "Epoch 28, Batch 225, loss_ca: 1.6414, adv_loss: 0.5253\n",
      "Epoch 28, Batch 226, loss_ca: 1.6697, adv_loss: 0.5128\n",
      "Epoch 28, Batch 227, loss_ca: 1.7086, adv_loss: 0.5315\n",
      "Epoch 28, Batch 228, loss_ca: 1.7160, adv_loss: 0.5125\n",
      "Epoch 29, Batch 29, loss_ca: 1.9769, adv_loss: 0.6007\n",
      "Epoch 29, Batch 30, loss_ca: 2.0395, adv_loss: 0.5995\n",
      "Epoch 29, Batch 31, loss_ca: 1.9594, adv_loss: 0.5958\n",
      "Epoch 29, Batch 32, loss_ca: 1.8523, adv_loss: 0.5873\n",
      "Epoch 29, Batch 33, loss_ca: 1.8175, adv_loss: 0.5193\n",
      "Epoch 29, Batch 34, loss_ca: 1.9925, adv_loss: 0.5733\n",
      "Epoch 29, Batch 35, loss_ca: 1.9557, adv_loss: 0.5899\n",
      "Epoch 29, Batch 36, loss_ca: 1.9486, adv_loss: 0.5932\n",
      "Epoch 29, Batch 37, loss_ca: 1.9248, adv_loss: 0.6110\n",
      "Epoch 29, Batch 38, loss_ca: 1.9524, adv_loss: 0.5950\n",
      "Epoch 29, Batch 39, loss_ca: 1.9213, adv_loss: 0.5879\n",
      "Epoch 29, Batch 40, loss_ca: 2.0103, adv_loss: 0.6101\n",
      "Epoch 29, Batch 41, loss_ca: 1.8820, adv_loss: 0.5307\n",
      "Epoch 29, Batch 42, loss_ca: 1.8861, adv_loss: 0.5507\n",
      "Epoch 29, Batch 43, loss_ca: 1.8391, adv_loss: 0.5366\n",
      "Epoch 29, Batch 44, loss_ca: 1.7596, adv_loss: 0.5400\n",
      "Epoch 29, Batch 45, loss_ca: 1.7298, adv_loss: 0.5384\n",
      "Epoch 29, Batch 46, loss_ca: 1.7498, adv_loss: 0.5344\n",
      "Epoch 29, Batch 47, loss_ca: 1.8285, adv_loss: 0.5584\n",
      "Epoch 29, Batch 48, loss_ca: 1.7807, adv_loss: 0.5562\n",
      "Epoch 29, Batch 49, loss_ca: 1.7357, adv_loss: 0.5438\n",
      "Epoch 29, Batch 50, loss_ca: 1.7324, adv_loss: 0.5499\n",
      "Epoch 29, Batch 51, loss_ca: 1.7812, adv_loss: 0.5496\n",
      "Epoch 29, Batch 52, loss_ca: 1.9083, adv_loss: 0.5385\n",
      "Epoch 29, Batch 53, loss_ca: 1.8604, adv_loss: 0.5492\n",
      "Epoch 29, Batch 54, loss_ca: 1.8305, adv_loss: 0.5465\n",
      "Epoch 29, Batch 55, loss_ca: 1.7392, adv_loss: 0.5526\n",
      "Epoch 29, Batch 56, loss_ca: 1.7102, adv_loss: 0.5513\n",
      "Epoch 29, Batch 57, loss_ca: 1.7110, adv_loss: 0.5537\n",
      "Epoch 29, Batch 58, loss_ca: 1.8579, adv_loss: 0.5922\n",
      "Epoch 29, Batch 59, loss_ca: 1.8023, adv_loss: 0.5481\n",
      "Epoch 29, Batch 60, loss_ca: 1.8042, adv_loss: 0.5476\n",
      "Epoch 29, Batch 61, loss_ca: 1.7810, adv_loss: 0.5401\n",
      "Epoch 29, Batch 62, loss_ca: 1.7906, adv_loss: 0.5540\n",
      "Epoch 29, Batch 63, loss_ca: 1.8186, adv_loss: 0.5237\n",
      "Epoch 29, Batch 64, loss_ca: 1.8647, adv_loss: 0.5283\n",
      "Epoch 29, Batch 65, loss_ca: 1.8158, adv_loss: 0.5047\n",
      "Epoch 29, Batch 66, loss_ca: 1.8104, adv_loss: 0.5416\n",
      "Epoch 29, Batch 67, loss_ca: 1.7680, adv_loss: 0.5287\n",
      "Epoch 29, Batch 68, loss_ca: 1.8003, adv_loss: 0.4945\n",
      "Epoch 29, Batch 69, loss_ca: 1.8234, adv_loss: 0.4983\n",
      "Epoch 29, Batch 70, loss_ca: 1.8287, adv_loss: 0.5073\n",
      "Epoch 29, Batch 71, loss_ca: 1.7858, adv_loss: 0.5124\n",
      "Epoch 29, Batch 72, loss_ca: 1.7901, adv_loss: 0.5230\n",
      "Epoch 29, Batch 73, loss_ca: 1.7070, adv_loss: 0.5058\n",
      "Epoch 29, Batch 74, loss_ca: 1.6601, adv_loss: 0.5014\n",
      "Epoch 29, Batch 75, loss_ca: 1.6107, adv_loss: 0.4926\n",
      "Epoch 29, Batch 76, loss_ca: 1.6307, adv_loss: 0.4782\n",
      "Epoch 29, Batch 77, loss_ca: 1.6234, adv_loss: 0.4814\n",
      "Epoch 29, Batch 78, loss_ca: 1.6557, adv_loss: 0.4898\n",
      "Epoch 29, Batch 79, loss_ca: 1.6854, adv_loss: 0.4912\n",
      "Epoch 29, Batch 80, loss_ca: 1.6609, adv_loss: 0.5043\n",
      "Epoch 29, Batch 81, loss_ca: 1.6992, adv_loss: 0.5172\n",
      "Epoch 29, Batch 82, loss_ca: 1.7685, adv_loss: 0.5300\n",
      "Epoch 29, Batch 83, loss_ca: 1.7238, adv_loss: 0.5203\n",
      "Epoch 29, Batch 84, loss_ca: 1.7210, adv_loss: 0.5167\n",
      "Epoch 29, Batch 85, loss_ca: 1.8566, adv_loss: 0.5108\n",
      "Epoch 29, Batch 86, loss_ca: 1.8014, adv_loss: 0.5379\n",
      "Epoch 29, Batch 87, loss_ca: 1.8256, adv_loss: 0.5318\n",
      "Epoch 29, Batch 88, loss_ca: 1.7879, adv_loss: 0.4995\n",
      "Epoch 29, Batch 89, loss_ca: 1.8461, adv_loss: 0.4975\n",
      "Epoch 29, Batch 90, loss_ca: 1.7540, adv_loss: 0.4744\n",
      "Epoch 29, Batch 91, loss_ca: 1.8103, adv_loss: 0.5088\n",
      "Epoch 29, Batch 92, loss_ca: 1.7290, adv_loss: 0.5004\n",
      "Epoch 29, Batch 93, loss_ca: 1.8204, adv_loss: 0.5120\n",
      "Epoch 29, Batch 94, loss_ca: 1.7468, adv_loss: 0.5045\n",
      "Epoch 29, Batch 95, loss_ca: 1.7333, adv_loss: 0.4578\n",
      "Epoch 29, Batch 96, loss_ca: 1.8268, adv_loss: 0.5008\n",
      "Epoch 29, Batch 97, loss_ca: 1.8000, adv_loss: 0.4931\n",
      "Epoch 29, Batch 98, loss_ca: 1.8006, adv_loss: 0.5105\n",
      "Epoch 29, Batch 99, loss_ca: 1.6730, adv_loss: 0.4992\n",
      "Epoch 29, Batch 100, loss_ca: 1.8434, adv_loss: 0.5192\n",
      "Epoch 29, Batch 101, loss_ca: 1.9236, adv_loss: 0.5366\n",
      "Epoch 29, Batch 102, loss_ca: 1.8853, adv_loss: 0.5386\n",
      "Epoch 29, Batch 103, loss_ca: 1.8814, adv_loss: 0.5392\n",
      "Epoch 29, Batch 104, loss_ca: 1.9621, adv_loss: 0.5301\n",
      "Epoch 29, Batch 105, loss_ca: 2.0082, adv_loss: 0.6164\n",
      "Epoch 29, Batch 106, loss_ca: 2.0092, adv_loss: 0.6422\n",
      "Epoch 29, Batch 107, loss_ca: 2.0388, adv_loss: 0.6488\n",
      "Epoch 29, Batch 108, loss_ca: 1.9655, adv_loss: 0.6017\n",
      "Epoch 29, Batch 109, loss_ca: 1.8911, adv_loss: 0.5944\n",
      "Epoch 29, Batch 110, loss_ca: 1.8086, adv_loss: 0.5955\n",
      "Epoch 29, Batch 111, loss_ca: 1.7744, adv_loss: 0.5886\n",
      "Epoch 29, Batch 112, loss_ca: 1.8270, adv_loss: 0.5686\n",
      "Epoch 29, Batch 113, loss_ca: 1.7503, adv_loss: 0.5551\n",
      "Epoch 29, Batch 114, loss_ca: 1.7592, adv_loss: 0.5433\n",
      "Epoch 29, Batch 115, loss_ca: 1.6973, adv_loss: 0.5347\n",
      "Epoch 29, Batch 116, loss_ca: 1.7623, adv_loss: 0.5358\n",
      "Epoch 29, Batch 117, loss_ca: 1.7532, adv_loss: 0.5303\n",
      "Epoch 29, Batch 118, loss_ca: 1.7269, adv_loss: 0.5168\n",
      "Epoch 29, Batch 119, loss_ca: 1.7125, adv_loss: 0.4864\n",
      "Epoch 29, Batch 120, loss_ca: 1.6905, adv_loss: 0.4814\n",
      "Epoch 29, Batch 121, loss_ca: 1.8839, adv_loss: 0.5022\n",
      "Epoch 29, Batch 122, loss_ca: 1.9424, adv_loss: 0.4968\n",
      "Epoch 29, Batch 123, loss_ca: 1.9529, adv_loss: 0.5138\n",
      "Epoch 29, Batch 124, loss_ca: 1.9121, adv_loss: 0.5156\n",
      "Epoch 29, Batch 125, loss_ca: 1.8688, adv_loss: 0.5260\n",
      "Epoch 29, Batch 126, loss_ca: 1.9430, adv_loss: 0.5270\n",
      "Epoch 29, Batch 127, loss_ca: 1.9507, adv_loss: 0.5409\n",
      "Epoch 29, Batch 128, loss_ca: 1.9238, adv_loss: 0.5516\n",
      "Epoch 29, Batch 129, loss_ca: 1.9098, adv_loss: 0.5463\n",
      "Epoch 29, Batch 130, loss_ca: 1.8834, adv_loss: 0.5459\n",
      "Epoch 29, Batch 131, loss_ca: 1.8703, adv_loss: 0.5295\n",
      "Epoch 29, Batch 132, loss_ca: 1.8581, adv_loss: 0.5711\n",
      "Epoch 29, Batch 133, loss_ca: 1.8906, adv_loss: 0.5743\n",
      "Epoch 29, Batch 134, loss_ca: 1.9179, adv_loss: 0.5714\n",
      "Epoch 29, Batch 135, loss_ca: 1.9294, adv_loss: 0.5671\n",
      "Epoch 29, Batch 136, loss_ca: 1.9284, adv_loss: 0.5153\n",
      "Epoch 29, Batch 137, loss_ca: 1.7802, adv_loss: 0.4872\n",
      "Epoch 29, Batch 138, loss_ca: 1.7476, adv_loss: 0.5048\n",
      "Epoch 29, Batch 139, loss_ca: 1.6698, adv_loss: 0.4993\n",
      "Epoch 29, Batch 140, loss_ca: 1.6258, adv_loss: 0.4829\n",
      "Epoch 29, Batch 141, loss_ca: 1.6352, adv_loss: 0.4716\n",
      "Epoch 29, Batch 142, loss_ca: 1.7793, adv_loss: 0.4864\n",
      "Epoch 29, Batch 143, loss_ca: 1.7509, adv_loss: 0.5083\n",
      "Epoch 29, Batch 144, loss_ca: 1.7242, adv_loss: 0.5112\n",
      "Epoch 29, Batch 145, loss_ca: 1.6959, adv_loss: 0.5316\n",
      "Epoch 29, Batch 146, loss_ca: 1.6848, adv_loss: 0.4958\n",
      "Epoch 29, Batch 147, loss_ca: 1.7684, adv_loss: 0.5179\n",
      "Epoch 29, Batch 148, loss_ca: 1.8217, adv_loss: 0.5141\n",
      "Epoch 29, Batch 149, loss_ca: 1.7517, adv_loss: 0.5218\n",
      "Epoch 29, Batch 150, loss_ca: 1.6988, adv_loss: 0.5235\n",
      "Epoch 29, Batch 151, loss_ca: 1.6477, adv_loss: 0.5297\n",
      "Epoch 29, Batch 152, loss_ca: 1.6458, adv_loss: 0.5314\n",
      "Epoch 29, Batch 153, loss_ca: 1.8387, adv_loss: 0.5334\n",
      "Epoch 29, Batch 154, loss_ca: 1.8455, adv_loss: 0.5153\n",
      "Epoch 29, Batch 155, loss_ca: 1.8155, adv_loss: 0.5178\n",
      "Epoch 29, Batch 156, loss_ca: 1.7840, adv_loss: 0.5257\n",
      "Epoch 29, Batch 157, loss_ca: 1.8487, adv_loss: 0.5263\n",
      "Epoch 29, Batch 158, loss_ca: 2.0861, adv_loss: 0.5407\n",
      "Epoch 29, Batch 159, loss_ca: 2.2455, adv_loss: 0.5291\n",
      "Epoch 29, Batch 160, loss_ca: 2.0810, adv_loss: 0.5432\n",
      "Epoch 29, Batch 161, loss_ca: 1.9886, adv_loss: 0.5589\n",
      "Epoch 29, Batch 162, loss_ca: 2.2005, adv_loss: 0.5689\n",
      "Epoch 29, Batch 163, loss_ca: 2.1568, adv_loss: 0.5863\n",
      "Epoch 29, Batch 164, loss_ca: 2.3918, adv_loss: 0.6153\n",
      "Epoch 29, Batch 165, loss_ca: 2.3478, adv_loss: 0.5950\n",
      "Epoch 29, Batch 166, loss_ca: 2.0995, adv_loss: 0.5527\n",
      "Epoch 29, Batch 167, loss_ca: 1.9736, adv_loss: 0.5359\n",
      "Epoch 29, Batch 168, loss_ca: 2.0519, adv_loss: 0.5257\n",
      "Epoch 29, Batch 169, loss_ca: 2.0975, adv_loss: 0.4936\n",
      "Epoch 29, Batch 170, loss_ca: 2.0937, adv_loss: 0.5017\n",
      "Epoch 29, Batch 171, loss_ca: 2.0223, adv_loss: 0.5230\n",
      "Epoch 29, Batch 172, loss_ca: 2.1026, adv_loss: 0.5224\n",
      "Epoch 29, Batch 173, loss_ca: 1.9074, adv_loss: 0.5362\n",
      "Epoch 29, Batch 174, loss_ca: 1.9948, adv_loss: 0.5129\n",
      "Epoch 29, Batch 175, loss_ca: 1.9313, adv_loss: 0.4965\n",
      "Epoch 29, Batch 176, loss_ca: 1.8805, adv_loss: 0.5047\n",
      "Epoch 29, Batch 177, loss_ca: 1.7916, adv_loss: 0.4941\n",
      "Epoch 29, Batch 178, loss_ca: 1.8571, adv_loss: 0.4902\n",
      "Epoch 29, Batch 179, loss_ca: 1.8028, adv_loss: 0.4816\n",
      "Epoch 29, Batch 180, loss_ca: 1.6967, adv_loss: 0.4773\n",
      "Epoch 29, Batch 181, loss_ca: 1.7481, adv_loss: 0.4949\n",
      "Epoch 29, Batch 182, loss_ca: 1.7471, adv_loss: 0.4989\n",
      "Epoch 29, Batch 183, loss_ca: 1.8633, adv_loss: 0.4964\n",
      "Epoch 29, Batch 184, loss_ca: 1.7763, adv_loss: 0.4768\n",
      "Epoch 29, Batch 185, loss_ca: 1.7950, adv_loss: 0.4789\n",
      "Epoch 29, Batch 186, loss_ca: 1.7697, adv_loss: 0.4925\n",
      "Epoch 29, Batch 187, loss_ca: 1.7597, adv_loss: 0.4930\n",
      "Epoch 29, Batch 188, loss_ca: 1.6924, adv_loss: 0.4984\n",
      "Epoch 29, Batch 189, loss_ca: 1.8152, adv_loss: 0.5454\n",
      "Epoch 29, Batch 190, loss_ca: 1.9727, adv_loss: 0.5513\n",
      "Epoch 29, Batch 191, loss_ca: 1.9776, adv_loss: 0.5896\n",
      "Epoch 29, Batch 192, loss_ca: 2.1413, adv_loss: 0.5575\n",
      "Epoch 29, Batch 193, loss_ca: 1.9772, adv_loss: 0.6189\n",
      "Epoch 29, Batch 194, loss_ca: 1.9247, adv_loss: 0.6094\n",
      "Epoch 29, Batch 195, loss_ca: 2.0943, adv_loss: 0.6034\n",
      "Epoch 29, Batch 196, loss_ca: 2.9504, adv_loss: 0.5913\n",
      "Epoch 29, Batch 197, loss_ca: 2.2155, adv_loss: 0.5727\n",
      "Epoch 29, Batch 198, loss_ca: 2.4819, adv_loss: 0.6665\n",
      "Epoch 29, Batch 199, loss_ca: 2.1503, adv_loss: 0.6517\n",
      "Epoch 29, Batch 200, loss_ca: 1.9111, adv_loss: 0.5529\n",
      "Epoch 29, Batch 201, loss_ca: 1.8666, adv_loss: 0.5142\n",
      "Epoch 29, Batch 202, loss_ca: 1.9418, adv_loss: 0.5107\n",
      "Epoch 29, Batch 203, loss_ca: 2.2926, adv_loss: 0.5229\n",
      "Epoch 29, Batch 204, loss_ca: 1.7916, adv_loss: 0.5032\n",
      "Epoch 29, Batch 205, loss_ca: 2.1200, adv_loss: 0.5034\n",
      "Epoch 29, Batch 206, loss_ca: 1.8156, adv_loss: 0.5212\n",
      "Epoch 29, Batch 207, loss_ca: 2.0658, adv_loss: 0.5123\n",
      "Epoch 29, Batch 208, loss_ca: 1.9891, adv_loss: 0.5181\n",
      "Epoch 29, Batch 209, loss_ca: 1.8510, adv_loss: 0.5014\n",
      "Epoch 29, Batch 210, loss_ca: 1.9446, adv_loss: 0.5320\n",
      "Epoch 29, Batch 211, loss_ca: 1.8050, adv_loss: 0.5589\n",
      "Epoch 29, Batch 212, loss_ca: 1.6008, adv_loss: 0.5394\n",
      "Epoch 29, Batch 213, loss_ca: 1.9326, adv_loss: 0.5817\n",
      "Epoch 29, Batch 214, loss_ca: 1.8192, adv_loss: 0.6439\n",
      "Epoch 29, Batch 215, loss_ca: 1.6048, adv_loss: 0.6605\n",
      "Epoch 29, Batch 216, loss_ca: 1.7767, adv_loss: 0.6475\n",
      "Epoch 29, Batch 217, loss_ca: 1.7251, adv_loss: 0.6412\n",
      "Epoch 29, Batch 218, loss_ca: 1.7990, adv_loss: 0.5642\n",
      "Epoch 29, Batch 219, loss_ca: 2.5098, adv_loss: 0.4936\n",
      "Epoch 29, Batch 220, loss_ca: 2.0304, adv_loss: 0.5973\n",
      "Epoch 29, Batch 221, loss_ca: 1.8361, adv_loss: 0.5751\n",
      "Epoch 29, Batch 222, loss_ca: 1.7836, adv_loss: 0.5624\n",
      "Epoch 29, Batch 223, loss_ca: 1.7518, adv_loss: 0.5379\n",
      "Epoch 29, Batch 224, loss_ca: 1.6633, adv_loss: 0.5401\n",
      "Epoch 29, Batch 225, loss_ca: 1.6346, adv_loss: 0.5119\n",
      "Epoch 29, Batch 226, loss_ca: 1.6648, adv_loss: 0.4874\n",
      "Epoch 29, Batch 227, loss_ca: 1.6492, adv_loss: 0.4869\n",
      "Epoch 29, Batch 228, loss_ca: 1.7146, adv_loss: 0.4811\n",
      "Epoch 30, Batch 30, loss_ca: 2.1618, adv_loss: 0.5272\n",
      "Epoch 30, Batch 31, loss_ca: 2.0662, adv_loss: 0.5481\n",
      "Epoch 30, Batch 32, loss_ca: 1.9287, adv_loss: 0.5614\n",
      "Epoch 30, Batch 33, loss_ca: 1.8012, adv_loss: 0.5550\n",
      "Epoch 30, Batch 34, loss_ca: 1.9718, adv_loss: 0.5962\n",
      "Epoch 30, Batch 35, loss_ca: 1.9725, adv_loss: 0.6154\n",
      "Epoch 30, Batch 36, loss_ca: 1.9621, adv_loss: 0.6125\n",
      "Epoch 30, Batch 37, loss_ca: 1.9748, adv_loss: 0.6107\n",
      "Epoch 30, Batch 38, loss_ca: 1.9799, adv_loss: 0.5925\n",
      "Epoch 30, Batch 39, loss_ca: 1.9477, adv_loss: 0.5942\n",
      "Epoch 30, Batch 40, loss_ca: 1.9644, adv_loss: 0.5914\n",
      "Epoch 30, Batch 41, loss_ca: 1.8749, adv_loss: 0.5465\n",
      "Epoch 30, Batch 42, loss_ca: 1.8510, adv_loss: 0.5272\n",
      "Epoch 30, Batch 43, loss_ca: 1.8161, adv_loss: 0.5082\n",
      "Epoch 30, Batch 44, loss_ca: 1.7493, adv_loss: 0.5168\n",
      "Epoch 30, Batch 45, loss_ca: 1.7412, adv_loss: 0.5126\n",
      "Epoch 30, Batch 46, loss_ca: 1.7343, adv_loss: 0.5148\n",
      "Epoch 30, Batch 47, loss_ca: 1.7398, adv_loss: 0.5316\n",
      "Epoch 30, Batch 48, loss_ca: 1.8108, adv_loss: 0.5449\n",
      "Epoch 30, Batch 49, loss_ca: 1.7896, adv_loss: 0.5567\n",
      "Epoch 30, Batch 50, loss_ca: 1.8243, adv_loss: 0.5618\n",
      "Epoch 30, Batch 51, loss_ca: 1.8104, adv_loss: 0.5763\n",
      "Epoch 30, Batch 52, loss_ca: 1.8181, adv_loss: 0.5588\n",
      "Epoch 30, Batch 53, loss_ca: 1.8307, adv_loss: 0.5812\n",
      "Epoch 30, Batch 54, loss_ca: 1.7713, adv_loss: 0.5790\n",
      "Epoch 30, Batch 55, loss_ca: 1.7277, adv_loss: 0.5738\n",
      "Epoch 30, Batch 56, loss_ca: 1.7079, adv_loss: 0.5557\n",
      "Epoch 30, Batch 57, loss_ca: 1.6898, adv_loss: 0.5505\n",
      "Epoch 30, Batch 58, loss_ca: 1.7759, adv_loss: 0.5528\n",
      "Epoch 30, Batch 59, loss_ca: 1.8052, adv_loss: 0.5373\n",
      "Epoch 30, Batch 60, loss_ca: 1.7452, adv_loss: 0.5378\n",
      "Epoch 30, Batch 61, loss_ca: 1.7166, adv_loss: 0.5200\n",
      "Epoch 30, Batch 62, loss_ca: 1.7507, adv_loss: 0.5321\n",
      "Epoch 30, Batch 63, loss_ca: 1.8127, adv_loss: 0.5993\n",
      "Epoch 30, Batch 64, loss_ca: 1.9023, adv_loss: 0.5955\n",
      "Epoch 30, Batch 65, loss_ca: 1.7624, adv_loss: 0.5306\n",
      "Epoch 30, Batch 66, loss_ca: 1.7433, adv_loss: 0.5262\n",
      "Epoch 30, Batch 67, loss_ca: 1.8088, adv_loss: 0.5467\n",
      "Epoch 30, Batch 68, loss_ca: 1.7935, adv_loss: 0.4890\n",
      "Epoch 30, Batch 69, loss_ca: 1.7259, adv_loss: 0.4902\n",
      "Epoch 30, Batch 70, loss_ca: 1.7370, adv_loss: 0.5064\n",
      "Epoch 30, Batch 71, loss_ca: 1.7606, adv_loss: 0.4814\n",
      "Epoch 30, Batch 72, loss_ca: 1.7279, adv_loss: 0.4895\n",
      "Epoch 30, Batch 73, loss_ca: 1.6797, adv_loss: 0.5015\n",
      "Epoch 30, Batch 74, loss_ca: 1.7149, adv_loss: 0.5042\n",
      "Epoch 30, Batch 75, loss_ca: 1.6850, adv_loss: 0.5114\n",
      "Epoch 30, Batch 76, loss_ca: 1.6329, adv_loss: 0.5100\n",
      "Epoch 30, Batch 77, loss_ca: 1.6060, adv_loss: 0.5006\n",
      "Epoch 30, Batch 78, loss_ca: 1.6349, adv_loss: 0.4863\n",
      "Epoch 30, Batch 79, loss_ca: 1.6421, adv_loss: 0.4847\n",
      "Epoch 30, Batch 80, loss_ca: 1.6212, adv_loss: 0.4803\n",
      "Epoch 30, Batch 81, loss_ca: 1.6402, adv_loss: 0.4796\n",
      "Epoch 30, Batch 82, loss_ca: 1.7271, adv_loss: 0.4821\n",
      "Epoch 30, Batch 83, loss_ca: 1.6900, adv_loss: 0.5025\n",
      "Epoch 30, Batch 84, loss_ca: 1.6984, adv_loss: 0.5064\n",
      "Epoch 30, Batch 85, loss_ca: 1.8076, adv_loss: 0.5210\n",
      "Epoch 30, Batch 86, loss_ca: 1.8878, adv_loss: 0.5490\n",
      "Epoch 30, Batch 87, loss_ca: 1.8729, adv_loss: 0.5314\n",
      "Epoch 30, Batch 88, loss_ca: 1.7995, adv_loss: 0.5249\n",
      "Epoch 30, Batch 89, loss_ca: 1.7989, adv_loss: 0.5234\n",
      "Epoch 30, Batch 90, loss_ca: 1.7512, adv_loss: 0.5255\n",
      "Epoch 30, Batch 91, loss_ca: 1.9069, adv_loss: 0.5170\n",
      "Epoch 30, Batch 92, loss_ca: 1.8286, adv_loss: 0.5108\n",
      "Epoch 30, Batch 93, loss_ca: 1.9189, adv_loss: 0.5154\n",
      "Epoch 30, Batch 94, loss_ca: 1.8029, adv_loss: 0.5233\n",
      "Epoch 30, Batch 95, loss_ca: 1.8424, adv_loss: 0.5094\n",
      "Epoch 30, Batch 96, loss_ca: 1.8694, adv_loss: 0.5172\n",
      "Epoch 30, Batch 97, loss_ca: 1.8187, adv_loss: 0.5202\n",
      "Epoch 30, Batch 98, loss_ca: 1.7886, adv_loss: 0.5191\n",
      "Epoch 30, Batch 99, loss_ca: 1.7561, adv_loss: 0.5264\n",
      "Epoch 30, Batch 100, loss_ca: 1.9031, adv_loss: 0.5064\n",
      "Epoch 30, Batch 101, loss_ca: 1.9371, adv_loss: 0.5235\n",
      "Epoch 30, Batch 102, loss_ca: 1.8750, adv_loss: 0.5228\n",
      "Epoch 30, Batch 103, loss_ca: 1.8806, adv_loss: 0.5337\n",
      "Epoch 30, Batch 104, loss_ca: 1.8133, adv_loss: 0.5427\n",
      "Epoch 30, Batch 105, loss_ca: 1.9304, adv_loss: 0.5978\n",
      "Epoch 30, Batch 106, loss_ca: 1.9600, adv_loss: 0.6137\n",
      "Epoch 30, Batch 107, loss_ca: 1.9763, adv_loss: 0.6080\n",
      "Epoch 30, Batch 108, loss_ca: 1.9369, adv_loss: 0.5659\n",
      "Epoch 30, Batch 109, loss_ca: 1.9355, adv_loss: 0.5267\n",
      "Epoch 30, Batch 110, loss_ca: 1.8144, adv_loss: 0.5129\n",
      "Epoch 30, Batch 111, loss_ca: 1.7414, adv_loss: 0.5265\n",
      "Epoch 30, Batch 112, loss_ca: 1.7478, adv_loss: 0.5442\n",
      "Epoch 30, Batch 113, loss_ca: 1.7159, adv_loss: 0.5304\n",
      "Epoch 30, Batch 114, loss_ca: 1.7474, adv_loss: 0.5133\n",
      "Epoch 30, Batch 115, loss_ca: 1.7299, adv_loss: 0.5181\n",
      "Epoch 30, Batch 116, loss_ca: 1.7180, adv_loss: 0.5207\n",
      "Epoch 30, Batch 117, loss_ca: 1.7163, adv_loss: 0.5152\n",
      "Epoch 30, Batch 118, loss_ca: 1.7369, adv_loss: 0.5218\n",
      "Epoch 30, Batch 119, loss_ca: 1.7075, adv_loss: 0.5250\n",
      "Epoch 30, Batch 120, loss_ca: 1.7119, adv_loss: 0.5295\n",
      "Epoch 30, Batch 121, loss_ca: 1.9556, adv_loss: 0.5440\n",
      "Epoch 30, Batch 122, loss_ca: 1.8840, adv_loss: 0.5395\n",
      "Epoch 30, Batch 123, loss_ca: 1.8672, adv_loss: 0.5401\n",
      "Epoch 30, Batch 124, loss_ca: 1.8516, adv_loss: 0.5344\n",
      "Epoch 30, Batch 125, loss_ca: 1.8438, adv_loss: 0.5358\n",
      "Epoch 30, Batch 126, loss_ca: 1.9125, adv_loss: 0.5376\n",
      "Epoch 30, Batch 127, loss_ca: 1.9412, adv_loss: 0.5866\n",
      "Epoch 30, Batch 128, loss_ca: 1.9678, adv_loss: 0.6011\n",
      "Epoch 30, Batch 129, loss_ca: 1.9427, adv_loss: 0.5989\n",
      "Epoch 30, Batch 130, loss_ca: 1.9162, adv_loss: 0.6082\n",
      "Epoch 30, Batch 131, loss_ca: 1.8704, adv_loss: 0.5774\n",
      "Epoch 30, Batch 132, loss_ca: 1.8668, adv_loss: 0.5864\n",
      "Epoch 30, Batch 133, loss_ca: 1.8745, adv_loss: 0.5803\n",
      "Epoch 30, Batch 134, loss_ca: 1.8867, adv_loss: 0.5758\n",
      "Epoch 30, Batch 135, loss_ca: 1.8792, adv_loss: 0.5704\n",
      "Epoch 30, Batch 136, loss_ca: 1.8477, adv_loss: 0.5408\n",
      "Epoch 30, Batch 137, loss_ca: 1.8093, adv_loss: 0.4975\n",
      "Epoch 30, Batch 138, loss_ca: 1.8029, adv_loss: 0.5072\n",
      "Epoch 30, Batch 139, loss_ca: 1.7632, adv_loss: 0.5042\n",
      "Epoch 30, Batch 140, loss_ca: 1.7381, adv_loss: 0.4922\n",
      "Epoch 30, Batch 141, loss_ca: 1.6927, adv_loss: 0.4948\n",
      "Epoch 30, Batch 142, loss_ca: 1.7492, adv_loss: 0.5351\n",
      "Epoch 30, Batch 143, loss_ca: 1.6830, adv_loss: 0.5305\n",
      "Epoch 30, Batch 144, loss_ca: 1.6831, adv_loss: 0.5431\n",
      "Epoch 30, Batch 145, loss_ca: 1.7058, adv_loss: 0.5317\n",
      "Epoch 30, Batch 146, loss_ca: 1.7264, adv_loss: 0.4955\n",
      "Epoch 30, Batch 147, loss_ca: 1.8034, adv_loss: 0.5090\n",
      "Epoch 30, Batch 148, loss_ca: 1.8006, adv_loss: 0.5163\n",
      "Epoch 30, Batch 149, loss_ca: 1.6918, adv_loss: 0.4962\n",
      "Epoch 30, Batch 150, loss_ca: 1.6658, adv_loss: 0.5151\n",
      "Epoch 30, Batch 151, loss_ca: 1.5986, adv_loss: 0.5350\n",
      "Epoch 30, Batch 152, loss_ca: 1.6168, adv_loss: 0.5255\n",
      "Epoch 30, Batch 153, loss_ca: 1.8152, adv_loss: 0.5178\n",
      "Epoch 30, Batch 154, loss_ca: 1.8229, adv_loss: 0.4909\n",
      "Epoch 30, Batch 155, loss_ca: 1.8248, adv_loss: 0.4949\n",
      "Epoch 30, Batch 156, loss_ca: 1.7780, adv_loss: 0.4961\n",
      "Epoch 30, Batch 157, loss_ca: 1.8058, adv_loss: 0.5337\n",
      "Epoch 30, Batch 158, loss_ca: 1.9999, adv_loss: 0.5416\n",
      "Epoch 30, Batch 159, loss_ca: 2.2484, adv_loss: 0.4786\n",
      "Epoch 30, Batch 160, loss_ca: 2.1469, adv_loss: 0.4913\n",
      "Epoch 30, Batch 161, loss_ca: 1.8802, adv_loss: 0.5401\n",
      "Epoch 30, Batch 162, loss_ca: 2.2004, adv_loss: 0.5135\n",
      "Epoch 30, Batch 163, loss_ca: 2.1061, adv_loss: 0.6049\n",
      "Epoch 30, Batch 164, loss_ca: 2.3050, adv_loss: 0.5680\n",
      "Epoch 30, Batch 165, loss_ca: 2.3006, adv_loss: 0.5437\n",
      "Epoch 30, Batch 166, loss_ca: 2.2317, adv_loss: 0.6086\n",
      "Epoch 30, Batch 167, loss_ca: 1.9852, adv_loss: 0.6016\n",
      "Epoch 30, Batch 168, loss_ca: 2.1533, adv_loss: 0.5923\n",
      "Epoch 30, Batch 169, loss_ca: 2.1058, adv_loss: 0.6021\n",
      "Epoch 30, Batch 170, loss_ca: 2.0219, adv_loss: 0.5747\n",
      "Epoch 30, Batch 171, loss_ca: 2.0282, adv_loss: 0.5696\n",
      "Epoch 30, Batch 172, loss_ca: 2.1657, adv_loss: 0.5683\n",
      "Epoch 30, Batch 173, loss_ca: 2.0030, adv_loss: 0.5559\n",
      "Epoch 30, Batch 174, loss_ca: 1.9392, adv_loss: 0.5580\n",
      "Epoch 30, Batch 175, loss_ca: 1.9190, adv_loss: 0.5423\n",
      "Epoch 30, Batch 176, loss_ca: 1.8703, adv_loss: 0.5296\n",
      "Epoch 30, Batch 177, loss_ca: 1.7656, adv_loss: 0.5299\n",
      "Epoch 30, Batch 178, loss_ca: 1.8253, adv_loss: 0.5227\n",
      "Epoch 30, Batch 179, loss_ca: 1.8131, adv_loss: 0.4989\n",
      "Epoch 30, Batch 180, loss_ca: 1.7118, adv_loss: 0.4944\n",
      "Epoch 30, Batch 181, loss_ca: 1.6650, adv_loss: 0.4952\n",
      "Epoch 30, Batch 182, loss_ca: 1.6634, adv_loss: 0.4853\n",
      "Epoch 30, Batch 183, loss_ca: 1.7885, adv_loss: 0.4873\n",
      "Epoch 30, Batch 184, loss_ca: 1.6862, adv_loss: 0.4544\n",
      "Epoch 30, Batch 185, loss_ca: 1.7616, adv_loss: 0.4558\n",
      "Epoch 30, Batch 186, loss_ca: 1.7146, adv_loss: 0.4642\n",
      "Epoch 30, Batch 187, loss_ca: 1.7340, adv_loss: 0.4591\n",
      "Epoch 30, Batch 188, loss_ca: 1.7222, adv_loss: 0.4738\n",
      "Epoch 30, Batch 189, loss_ca: 1.8067, adv_loss: 0.4837\n",
      "Epoch 30, Batch 190, loss_ca: 1.8881, adv_loss: 0.4788\n",
      "Epoch 30, Batch 191, loss_ca: 1.8905, adv_loss: 0.4725\n",
      "Epoch 30, Batch 192, loss_ca: 2.0318, adv_loss: 0.5234\n",
      "Epoch 30, Batch 193, loss_ca: 1.9553, adv_loss: 0.5248\n",
      "Epoch 30, Batch 194, loss_ca: 1.7330, adv_loss: 0.5076\n",
      "Epoch 30, Batch 195, loss_ca: 1.8747, adv_loss: 0.5192\n",
      "Epoch 30, Batch 196, loss_ca: 2.3285, adv_loss: 0.5297\n",
      "Epoch 30, Batch 197, loss_ca: 1.9013, adv_loss: 0.5862\n",
      "Epoch 30, Batch 198, loss_ca: 2.0018, adv_loss: 0.5620\n",
      "Epoch 30, Batch 199, loss_ca: 1.8898, adv_loss: 0.5463\n",
      "Epoch 30, Batch 200, loss_ca: 1.8157, adv_loss: 0.5099\n",
      "Epoch 30, Batch 201, loss_ca: 2.1826, adv_loss: 0.5573\n",
      "Epoch 30, Batch 202, loss_ca: 2.1920, adv_loss: 0.5791\n",
      "Epoch 30, Batch 203, loss_ca: 2.5945, adv_loss: 0.5649\n",
      "Epoch 30, Batch 204, loss_ca: 1.9393, adv_loss: 0.5561\n",
      "Epoch 30, Batch 205, loss_ca: 2.0907, adv_loss: 0.5294\n",
      "Epoch 30, Batch 206, loss_ca: 1.8902, adv_loss: 0.5429\n",
      "Epoch 30, Batch 207, loss_ca: 2.0120, adv_loss: 0.5565\n",
      "Epoch 30, Batch 208, loss_ca: 1.9137, adv_loss: 0.5418\n",
      "Epoch 30, Batch 209, loss_ca: 1.8718, adv_loss: 0.5313\n",
      "Epoch 30, Batch 210, loss_ca: 1.8389, adv_loss: 0.5362\n",
      "Epoch 30, Batch 211, loss_ca: 1.9012, adv_loss: 0.5914\n",
      "Epoch 30, Batch 212, loss_ca: 1.9042, adv_loss: 0.5865\n",
      "Epoch 30, Batch 213, loss_ca: 2.1404, adv_loss: 0.6179\n",
      "Epoch 30, Batch 214, loss_ca: 1.8523, adv_loss: 0.5643\n",
      "Epoch 30, Batch 215, loss_ca: 1.6702, adv_loss: 0.5771\n",
      "Epoch 30, Batch 216, loss_ca: 1.6701, adv_loss: 0.5952\n",
      "Epoch 30, Batch 217, loss_ca: 1.6408, adv_loss: 0.6108\n",
      "Epoch 30, Batch 218, loss_ca: 1.7155, adv_loss: 0.6228\n",
      "Epoch 30, Batch 219, loss_ca: 2.1993, adv_loss: 0.5905\n",
      "Epoch 30, Batch 220, loss_ca: 1.8647, adv_loss: 0.5992\n",
      "Epoch 30, Batch 221, loss_ca: 1.7967, adv_loss: 0.5513\n",
      "Epoch 30, Batch 222, loss_ca: 1.7543, adv_loss: 0.5289\n",
      "Epoch 30, Batch 223, loss_ca: 1.7312, adv_loss: 0.5428\n",
      "Epoch 30, Batch 224, loss_ca: 1.6846, adv_loss: 0.5291\n",
      "Epoch 30, Batch 225, loss_ca: 1.6421, adv_loss: 0.5192\n",
      "Epoch 30, Batch 226, loss_ca: 1.6713, adv_loss: 0.5069\n",
      "Epoch 30, Batch 227, loss_ca: 1.6321, adv_loss: 0.5118\n",
      "Epoch 30, Batch 228, loss_ca: 1.6532, adv_loss: 0.5173\n",
      "Epoch 31, Batch 31, loss_ca: 2.0783, adv_loss: 0.5401\n",
      "Epoch 31, Batch 32, loss_ca: 1.8734, adv_loss: 0.5273\n",
      "Epoch 31, Batch 33, loss_ca: 1.8383, adv_loss: 0.5029\n",
      "Epoch 31, Batch 34, loss_ca: 2.0012, adv_loss: 0.5520\n",
      "Epoch 31, Batch 35, loss_ca: 2.0652, adv_loss: 0.5628\n",
      "Epoch 31, Batch 36, loss_ca: 2.0153, adv_loss: 0.5799\n",
      "Epoch 31, Batch 37, loss_ca: 1.9281, adv_loss: 0.5767\n",
      "Epoch 31, Batch 38, loss_ca: 1.9684, adv_loss: 0.5917\n",
      "Epoch 31, Batch 39, loss_ca: 2.0083, adv_loss: 0.5956\n",
      "Epoch 31, Batch 40, loss_ca: 1.9600, adv_loss: 0.5847\n",
      "Epoch 31, Batch 41, loss_ca: 1.9317, adv_loss: 0.5348\n",
      "Epoch 31, Batch 42, loss_ca: 1.9396, adv_loss: 0.5348\n",
      "Epoch 31, Batch 43, loss_ca: 1.8781, adv_loss: 0.5272\n",
      "Epoch 31, Batch 44, loss_ca: 1.8417, adv_loss: 0.5375\n",
      "Epoch 31, Batch 45, loss_ca: 1.8081, adv_loss: 0.5351\n",
      "Epoch 31, Batch 46, loss_ca: 1.7659, adv_loss: 0.5607\n",
      "Epoch 31, Batch 47, loss_ca: 1.7429, adv_loss: 0.5604\n",
      "Epoch 31, Batch 48, loss_ca: 1.7794, adv_loss: 0.5689\n",
      "Epoch 31, Batch 49, loss_ca: 1.7691, adv_loss: 0.5636\n",
      "Epoch 31, Batch 50, loss_ca: 1.7536, adv_loss: 0.5484\n",
      "Epoch 31, Batch 51, loss_ca: 1.7549, adv_loss: 0.5409\n",
      "Epoch 31, Batch 52, loss_ca: 1.8187, adv_loss: 0.5512\n",
      "Epoch 31, Batch 53, loss_ca: 1.8816, adv_loss: 0.5616\n",
      "Epoch 31, Batch 54, loss_ca: 1.8613, adv_loss: 0.5530\n",
      "Epoch 31, Batch 55, loss_ca: 1.8210, adv_loss: 0.5660\n",
      "Epoch 31, Batch 56, loss_ca: 1.7468, adv_loss: 0.5649\n",
      "Epoch 31, Batch 57, loss_ca: 1.6985, adv_loss: 0.5544\n",
      "Epoch 31, Batch 58, loss_ca: 1.8236, adv_loss: 0.5624\n",
      "Epoch 31, Batch 59, loss_ca: 1.7712, adv_loss: 0.5451\n",
      "Epoch 31, Batch 60, loss_ca: 1.7104, adv_loss: 0.5370\n",
      "Epoch 31, Batch 61, loss_ca: 1.7324, adv_loss: 0.5308\n",
      "Epoch 31, Batch 62, loss_ca: 1.7473, adv_loss: 0.5262\n",
      "Epoch 31, Batch 63, loss_ca: 1.8185, adv_loss: 0.5333\n",
      "Epoch 31, Batch 64, loss_ca: 1.8371, adv_loss: 0.5141\n",
      "Epoch 31, Batch 65, loss_ca: 1.7742, adv_loss: 0.5185\n",
      "Epoch 31, Batch 66, loss_ca: 1.7740, adv_loss: 0.5521\n",
      "Epoch 31, Batch 67, loss_ca: 1.8194, adv_loss: 0.5229\n",
      "Epoch 31, Batch 68, loss_ca: 1.7910, adv_loss: 0.5331\n",
      "Epoch 31, Batch 69, loss_ca: 1.7720, adv_loss: 0.4947\n",
      "Epoch 31, Batch 70, loss_ca: 1.7915, adv_loss: 0.4980\n",
      "Epoch 31, Batch 71, loss_ca: 1.7535, adv_loss: 0.4964\n",
      "Epoch 31, Batch 72, loss_ca: 1.7394, adv_loss: 0.4968\n",
      "Epoch 31, Batch 73, loss_ca: 1.7547, adv_loss: 0.5096\n",
      "Epoch 31, Batch 74, loss_ca: 1.7597, adv_loss: 0.5003\n",
      "Epoch 31, Batch 75, loss_ca: 1.7312, adv_loss: 0.4941\n",
      "Epoch 31, Batch 76, loss_ca: 1.6862, adv_loss: 0.4984\n",
      "Epoch 31, Batch 77, loss_ca: 1.6699, adv_loss: 0.4946\n",
      "Epoch 31, Batch 78, loss_ca: 1.6888, adv_loss: 0.4872\n",
      "Epoch 31, Batch 79, loss_ca: 1.6597, adv_loss: 0.4986\n",
      "Epoch 31, Batch 80, loss_ca: 1.6345, adv_loss: 0.5098\n",
      "Epoch 31, Batch 81, loss_ca: 1.6133, adv_loss: 0.5027\n",
      "Epoch 31, Batch 82, loss_ca: 1.6602, adv_loss: 0.4833\n",
      "Epoch 31, Batch 83, loss_ca: 1.7293, adv_loss: 0.4951\n",
      "Epoch 31, Batch 84, loss_ca: 1.7070, adv_loss: 0.4924\n",
      "Epoch 31, Batch 85, loss_ca: 1.7450, adv_loss: 0.4974\n",
      "Epoch 31, Batch 86, loss_ca: 1.8599, adv_loss: 0.5015\n",
      "Epoch 31, Batch 87, loss_ca: 1.8509, adv_loss: 0.4893\n",
      "Epoch 31, Batch 88, loss_ca: 1.8261, adv_loss: 0.4905\n",
      "Epoch 31, Batch 89, loss_ca: 1.7747, adv_loss: 0.4949\n",
      "Epoch 31, Batch 90, loss_ca: 1.7120, adv_loss: 0.4918\n",
      "Epoch 31, Batch 91, loss_ca: 1.8362, adv_loss: 0.5378\n",
      "Epoch 31, Batch 92, loss_ca: 1.7727, adv_loss: 0.5342\n",
      "Epoch 31, Batch 93, loss_ca: 1.8543, adv_loss: 0.5596\n",
      "Epoch 31, Batch 94, loss_ca: 1.7996, adv_loss: 0.5397\n",
      "Epoch 31, Batch 95, loss_ca: 1.8529, adv_loss: 0.5183\n",
      "Epoch 31, Batch 96, loss_ca: 1.8197, adv_loss: 0.5145\n",
      "Epoch 31, Batch 97, loss_ca: 1.8107, adv_loss: 0.5090\n",
      "Epoch 31, Batch 98, loss_ca: 1.7752, adv_loss: 0.5115\n",
      "Epoch 31, Batch 99, loss_ca: 1.7159, adv_loss: 0.5198\n",
      "Epoch 31, Batch 100, loss_ca: 1.8298, adv_loss: 0.5005\n",
      "Epoch 31, Batch 101, loss_ca: 1.9294, adv_loss: 0.5097\n",
      "Epoch 31, Batch 102, loss_ca: 1.9094, adv_loss: 0.5134\n",
      "Epoch 31, Batch 103, loss_ca: 1.9036, adv_loss: 0.5181\n",
      "Epoch 31, Batch 104, loss_ca: 1.9400, adv_loss: 0.5317\n",
      "Epoch 31, Batch 105, loss_ca: 2.0368, adv_loss: 0.5660\n",
      "Epoch 31, Batch 106, loss_ca: 1.9806, adv_loss: 0.5793\n",
      "Epoch 31, Batch 107, loss_ca: 2.0347, adv_loss: 0.5815\n",
      "Epoch 31, Batch 108, loss_ca: 1.9026, adv_loss: 0.5822\n",
      "Epoch 31, Batch 109, loss_ca: 1.8132, adv_loss: 0.5145\n",
      "Epoch 31, Batch 110, loss_ca: 1.7823, adv_loss: 0.5318\n",
      "Epoch 31, Batch 111, loss_ca: 1.6982, adv_loss: 0.5084\n",
      "Epoch 31, Batch 112, loss_ca: 1.7322, adv_loss: 0.5016\n",
      "Epoch 31, Batch 113, loss_ca: 1.7102, adv_loss: 0.4952\n",
      "Epoch 31, Batch 114, loss_ca: 1.7459, adv_loss: 0.5005\n",
      "Epoch 31, Batch 115, loss_ca: 1.7290, adv_loss: 0.5097\n",
      "Epoch 31, Batch 116, loss_ca: 1.7349, adv_loss: 0.5288\n",
      "Epoch 31, Batch 117, loss_ca: 1.7974, adv_loss: 0.5453\n",
      "Epoch 31, Batch 118, loss_ca: 1.7239, adv_loss: 0.5295\n",
      "Epoch 31, Batch 119, loss_ca: 1.7237, adv_loss: 0.5178\n",
      "Epoch 31, Batch 120, loss_ca: 1.7001, adv_loss: 0.5076\n",
      "Epoch 31, Batch 121, loss_ca: 1.8762, adv_loss: 0.5296\n",
      "Epoch 31, Batch 122, loss_ca: 1.9477, adv_loss: 0.5526\n",
      "Epoch 31, Batch 123, loss_ca: 1.9512, adv_loss: 0.5607\n",
      "Epoch 31, Batch 124, loss_ca: 1.9206, adv_loss: 0.5732\n",
      "Epoch 31, Batch 125, loss_ca: 1.8801, adv_loss: 0.5726\n",
      "Epoch 31, Batch 126, loss_ca: 1.9512, adv_loss: 0.5672\n",
      "Epoch 31, Batch 127, loss_ca: 1.9074, adv_loss: 0.5458\n",
      "Epoch 31, Batch 128, loss_ca: 1.9210, adv_loss: 0.5591\n",
      "Epoch 31, Batch 129, loss_ca: 1.9080, adv_loss: 0.5609\n",
      "Epoch 31, Batch 130, loss_ca: 1.8382, adv_loss: 0.5409\n",
      "Epoch 31, Batch 131, loss_ca: 1.8232, adv_loss: 0.5441\n",
      "Epoch 31, Batch 132, loss_ca: 1.8448, adv_loss: 0.5937\n",
      "Epoch 31, Batch 133, loss_ca: 1.8868, adv_loss: 0.6123\n",
      "Epoch 31, Batch 134, loss_ca: 1.9103, adv_loss: 0.6015\n",
      "Epoch 31, Batch 135, loss_ca: 1.9356, adv_loss: 0.6180\n",
      "Epoch 31, Batch 136, loss_ca: 1.9547, adv_loss: 0.5871\n",
      "Epoch 31, Batch 137, loss_ca: 1.7869, adv_loss: 0.5495\n",
      "Epoch 31, Batch 138, loss_ca: 1.7375, adv_loss: 0.5356\n",
      "Epoch 31, Batch 139, loss_ca: 1.7003, adv_loss: 0.5159\n",
      "Epoch 31, Batch 140, loss_ca: 1.6611, adv_loss: 0.5065\n",
      "Epoch 31, Batch 141, loss_ca: 1.6751, adv_loss: 0.4906\n",
      "Epoch 31, Batch 142, loss_ca: 1.7823, adv_loss: 0.5387\n",
      "Epoch 31, Batch 143, loss_ca: 1.7440, adv_loss: 0.5291\n",
      "Epoch 31, Batch 144, loss_ca: 1.7180, adv_loss: 0.5287\n",
      "Epoch 31, Batch 145, loss_ca: 1.6901, adv_loss: 0.5359\n",
      "Epoch 31, Batch 146, loss_ca: 1.6880, adv_loss: 0.5318\n",
      "Epoch 31, Batch 147, loss_ca: 1.7419, adv_loss: 0.5353\n",
      "Epoch 31, Batch 148, loss_ca: 1.7786, adv_loss: 0.5175\n",
      "Epoch 31, Batch 149, loss_ca: 1.7408, adv_loss: 0.5019\n",
      "Epoch 31, Batch 150, loss_ca: 1.6628, adv_loss: 0.5255\n",
      "Epoch 31, Batch 151, loss_ca: 1.5817, adv_loss: 0.5343\n",
      "Epoch 31, Batch 152, loss_ca: 1.5828, adv_loss: 0.5398\n",
      "Epoch 31, Batch 153, loss_ca: 1.8268, adv_loss: 0.5148\n",
      "Epoch 31, Batch 154, loss_ca: 1.7994, adv_loss: 0.5227\n",
      "Epoch 31, Batch 155, loss_ca: 1.7658, adv_loss: 0.5351\n",
      "Epoch 31, Batch 156, loss_ca: 1.7548, adv_loss: 0.5336\n",
      "Epoch 31, Batch 157, loss_ca: 1.8369, adv_loss: 0.5536\n",
      "Epoch 31, Batch 158, loss_ca: 1.9652, adv_loss: 0.5722\n",
      "Epoch 31, Batch 159, loss_ca: 2.1924, adv_loss: 0.5487\n",
      "Epoch 31, Batch 160, loss_ca: 2.0737, adv_loss: 0.5743\n",
      "Epoch 31, Batch 161, loss_ca: 1.9369, adv_loss: 0.5471\n",
      "Epoch 31, Batch 162, loss_ca: 2.1812, adv_loss: 0.5450\n",
      "Epoch 31, Batch 163, loss_ca: 2.0157, adv_loss: 0.5261\n",
      "Epoch 31, Batch 164, loss_ca: 2.0998, adv_loss: 0.5203\n",
      "Epoch 31, Batch 165, loss_ca: 2.3231, adv_loss: 0.5038\n",
      "Epoch 31, Batch 166, loss_ca: 2.0677, adv_loss: 0.5378\n",
      "Epoch 31, Batch 167, loss_ca: 2.1401, adv_loss: 0.5821\n",
      "Epoch 31, Batch 168, loss_ca: 2.2947, adv_loss: 0.5393\n",
      "Epoch 31, Batch 169, loss_ca: 2.2174, adv_loss: 0.5374\n",
      "Epoch 31, Batch 170, loss_ca: 2.1179, adv_loss: 0.5655\n",
      "Epoch 31, Batch 171, loss_ca: 2.1176, adv_loss: 0.5805\n",
      "Epoch 31, Batch 172, loss_ca: 2.2216, adv_loss: 0.5700\n",
      "Epoch 31, Batch 173, loss_ca: 1.9725, adv_loss: 0.5670\n",
      "Epoch 31, Batch 174, loss_ca: 1.9898, adv_loss: 0.5260\n",
      "Epoch 31, Batch 175, loss_ca: 1.9644, adv_loss: 0.5167\n",
      "Epoch 31, Batch 176, loss_ca: 1.8616, adv_loss: 0.5141\n",
      "Epoch 31, Batch 177, loss_ca: 1.7649, adv_loss: 0.5110\n",
      "Epoch 31, Batch 178, loss_ca: 1.8644, adv_loss: 0.5024\n",
      "Epoch 31, Batch 179, loss_ca: 1.8493, adv_loss: 0.4942\n",
      "Epoch 31, Batch 180, loss_ca: 1.7505, adv_loss: 0.5020\n",
      "Epoch 31, Batch 181, loss_ca: 1.7232, adv_loss: 0.5112\n",
      "Epoch 31, Batch 182, loss_ca: 1.6913, adv_loss: 0.4901\n",
      "Epoch 31, Batch 183, loss_ca: 1.7857, adv_loss: 0.4679\n",
      "Epoch 31, Batch 184, loss_ca: 1.6829, adv_loss: 0.4667\n",
      "Epoch 31, Batch 185, loss_ca: 1.6759, adv_loss: 0.4799\n",
      "Epoch 31, Batch 186, loss_ca: 1.6859, adv_loss: 0.4757\n",
      "Epoch 31, Batch 187, loss_ca: 1.7718, adv_loss: 0.4798\n",
      "Epoch 31, Batch 188, loss_ca: 1.7457, adv_loss: 0.4913\n",
      "Epoch 31, Batch 189, loss_ca: 1.8856, adv_loss: 0.5148\n",
      "Epoch 31, Batch 190, loss_ca: 2.0024, adv_loss: 0.5543\n",
      "Epoch 31, Batch 191, loss_ca: 2.1038, adv_loss: 0.5710\n",
      "Epoch 31, Batch 192, loss_ca: 2.2036, adv_loss: 0.5514\n",
      "Epoch 31, Batch 193, loss_ca: 1.8945, adv_loss: 0.5294\n",
      "Epoch 31, Batch 194, loss_ca: 1.6427, adv_loss: 0.5285\n",
      "Epoch 31, Batch 195, loss_ca: 1.7668, adv_loss: 0.5455\n",
      "Epoch 31, Batch 196, loss_ca: 2.2685, adv_loss: 0.5350\n",
      "Epoch 31, Batch 197, loss_ca: 1.8070, adv_loss: 0.4927\n",
      "Epoch 31, Batch 198, loss_ca: 1.9455, adv_loss: 0.5734\n",
      "Epoch 31, Batch 199, loss_ca: 1.8121, adv_loss: 0.5565\n",
      "Epoch 31, Batch 200, loss_ca: 1.7620, adv_loss: 0.5527\n",
      "Epoch 31, Batch 201, loss_ca: 1.9286, adv_loss: 0.5073\n",
      "Epoch 31, Batch 202, loss_ca: 1.9997, adv_loss: 0.5450\n",
      "Epoch 31, Batch 203, loss_ca: 2.2135, adv_loss: 0.5439\n",
      "Epoch 31, Batch 204, loss_ca: 2.0176, adv_loss: 0.5461\n",
      "Epoch 31, Batch 205, loss_ca: 2.1943, adv_loss: 0.5414\n",
      "Epoch 31, Batch 206, loss_ca: 2.0768, adv_loss: 0.5778\n",
      "Epoch 31, Batch 207, loss_ca: 2.1952, adv_loss: 0.5892\n",
      "Epoch 31, Batch 208, loss_ca: 1.9636, adv_loss: 0.5446\n",
      "Epoch 31, Batch 209, loss_ca: 1.7624, adv_loss: 0.5240\n",
      "Epoch 31, Batch 210, loss_ca: 1.8433, adv_loss: 0.5570\n",
      "Epoch 31, Batch 211, loss_ca: 1.8416, adv_loss: 0.5338\n",
      "Epoch 31, Batch 212, loss_ca: 1.6170, adv_loss: 0.5917\n",
      "Epoch 31, Batch 213, loss_ca: 1.9171, adv_loss: 0.5609\n",
      "Epoch 31, Batch 214, loss_ca: 1.7089, adv_loss: 0.6381\n",
      "Epoch 31, Batch 215, loss_ca: 1.5219, adv_loss: 0.6068\n",
      "Epoch 31, Batch 216, loss_ca: 1.7358, adv_loss: 0.6277\n",
      "Epoch 31, Batch 217, loss_ca: 1.8330, adv_loss: 0.6083\n",
      "Epoch 31, Batch 218, loss_ca: 1.8203, adv_loss: 0.5754\n",
      "Epoch 31, Batch 219, loss_ca: 2.0865, adv_loss: 0.5448\n",
      "Epoch 31, Batch 220, loss_ca: 2.0054, adv_loss: 0.5308\n",
      "Epoch 31, Batch 221, loss_ca: 1.8081, adv_loss: 0.5165\n",
      "Epoch 31, Batch 222, loss_ca: 1.7108, adv_loss: 0.5139\n",
      "Epoch 31, Batch 223, loss_ca: 1.6857, adv_loss: 0.5232\n",
      "Epoch 31, Batch 224, loss_ca: 1.6375, adv_loss: 0.5160\n",
      "Epoch 31, Batch 225, loss_ca: 1.6217, adv_loss: 0.4994\n",
      "Epoch 31, Batch 226, loss_ca: 1.6836, adv_loss: 0.5015\n",
      "Epoch 31, Batch 227, loss_ca: 1.6234, adv_loss: 0.5027\n",
      "Epoch 31, Batch 228, loss_ca: 1.6971, adv_loss: 0.4979\n",
      "Epoch 32, Batch 32, loss_ca: 1.8819, adv_loss: 0.5515\n",
      "Epoch 32, Batch 33, loss_ca: 1.7912, adv_loss: 0.5366\n",
      "Epoch 32, Batch 34, loss_ca: 2.0931, adv_loss: 0.5948\n",
      "Epoch 32, Batch 35, loss_ca: 2.0210, adv_loss: 0.6280\n",
      "Epoch 32, Batch 36, loss_ca: 1.9985, adv_loss: 0.6148\n",
      "Epoch 32, Batch 37, loss_ca: 1.9620, adv_loss: 0.6146\n",
      "Epoch 32, Batch 38, loss_ca: 1.9733, adv_loss: 0.6137\n",
      "Epoch 32, Batch 39, loss_ca: 1.9668, adv_loss: 0.6242\n",
      "Epoch 32, Batch 40, loss_ca: 1.8771, adv_loss: 0.6141\n",
      "Epoch 32, Batch 41, loss_ca: 1.9346, adv_loss: 0.5062\n",
      "Epoch 32, Batch 42, loss_ca: 1.8174, adv_loss: 0.5283\n",
      "Epoch 32, Batch 43, loss_ca: 1.7940, adv_loss: 0.5018\n",
      "Epoch 32, Batch 44, loss_ca: 1.7716, adv_loss: 0.5039\n",
      "Epoch 32, Batch 45, loss_ca: 1.7653, adv_loss: 0.5103\n",
      "Epoch 32, Batch 46, loss_ca: 1.7612, adv_loss: 0.5040\n",
      "Epoch 32, Batch 47, loss_ca: 1.7549, adv_loss: 0.5126\n",
      "Epoch 32, Batch 48, loss_ca: 1.7460, adv_loss: 0.5483\n",
      "Epoch 32, Batch 49, loss_ca: 1.7320, adv_loss: 0.5561\n",
      "Epoch 32, Batch 50, loss_ca: 1.7212, adv_loss: 0.5596\n",
      "Epoch 32, Batch 51, loss_ca: 1.7274, adv_loss: 0.5621\n",
      "Epoch 32, Batch 52, loss_ca: 1.8238, adv_loss: 0.5823\n",
      "Epoch 32, Batch 53, loss_ca: 1.8375, adv_loss: 0.5851\n",
      "Epoch 32, Batch 54, loss_ca: 1.7831, adv_loss: 0.5736\n",
      "Epoch 32, Batch 55, loss_ca: 1.7391, adv_loss: 0.5593\n",
      "Epoch 32, Batch 56, loss_ca: 1.7107, adv_loss: 0.5405\n",
      "Epoch 32, Batch 57, loss_ca: 1.7229, adv_loss: 0.5384\n",
      "Epoch 32, Batch 58, loss_ca: 1.7752, adv_loss: 0.5549\n",
      "Epoch 32, Batch 59, loss_ca: 1.7371, adv_loss: 0.5609\n",
      "Epoch 32, Batch 60, loss_ca: 1.7322, adv_loss: 0.5496\n",
      "Epoch 32, Batch 61, loss_ca: 1.7187, adv_loss: 0.5598\n",
      "Epoch 32, Batch 62, loss_ca: 1.7490, adv_loss: 0.5444\n",
      "Epoch 32, Batch 63, loss_ca: 1.8914, adv_loss: 0.5603\n",
      "Epoch 32, Batch 64, loss_ca: 1.8992, adv_loss: 0.5532\n",
      "Epoch 32, Batch 65, loss_ca: 1.7774, adv_loss: 0.5446\n",
      "Epoch 32, Batch 66, loss_ca: 1.7164, adv_loss: 0.5517\n",
      "Epoch 32, Batch 67, loss_ca: 1.7131, adv_loss: 0.5298\n",
      "Epoch 32, Batch 68, loss_ca: 1.7419, adv_loss: 0.5211\n",
      "Epoch 32, Batch 69, loss_ca: 1.6750, adv_loss: 0.4945\n",
      "Epoch 32, Batch 70, loss_ca: 1.7168, adv_loss: 0.5153\n",
      "Epoch 32, Batch 71, loss_ca: 1.6691, adv_loss: 0.4708\n",
      "Epoch 32, Batch 72, loss_ca: 1.7254, adv_loss: 0.4607\n",
      "Epoch 32, Batch 73, loss_ca: 1.6942, adv_loss: 0.4582\n",
      "Epoch 32, Batch 74, loss_ca: 1.6964, adv_loss: 0.4556\n",
      "Epoch 32, Batch 75, loss_ca: 1.6603, adv_loss: 0.4679\n",
      "Epoch 32, Batch 76, loss_ca: 1.6073, adv_loss: 0.4782\n",
      "Epoch 32, Batch 77, loss_ca: 1.6134, adv_loss: 0.4883\n",
      "Epoch 32, Batch 78, loss_ca: 1.6875, adv_loss: 0.4919\n",
      "Epoch 32, Batch 79, loss_ca: 1.6573, adv_loss: 0.5229\n",
      "Epoch 32, Batch 80, loss_ca: 1.6409, adv_loss: 0.5187\n",
      "Epoch 32, Batch 81, loss_ca: 1.6388, adv_loss: 0.5308\n",
      "Epoch 32, Batch 82, loss_ca: 1.7118, adv_loss: 0.5246\n",
      "Epoch 32, Batch 83, loss_ca: 1.6929, adv_loss: 0.5098\n",
      "Epoch 32, Batch 84, loss_ca: 1.6914, adv_loss: 0.5151\n",
      "Epoch 32, Batch 85, loss_ca: 1.7441, adv_loss: 0.5130\n",
      "Epoch 32, Batch 86, loss_ca: 1.8080, adv_loss: 0.5117\n",
      "Epoch 32, Batch 87, loss_ca: 1.8873, adv_loss: 0.5091\n",
      "Epoch 32, Batch 88, loss_ca: 1.7959, adv_loss: 0.5071\n",
      "Epoch 32, Batch 89, loss_ca: 1.7856, adv_loss: 0.5262\n",
      "Epoch 32, Batch 90, loss_ca: 1.7071, adv_loss: 0.5004\n",
      "Epoch 32, Batch 91, loss_ca: 1.8508, adv_loss: 0.5487\n",
      "Epoch 32, Batch 92, loss_ca: 1.8062, adv_loss: 0.5254\n",
      "Epoch 32, Batch 93, loss_ca: 1.8479, adv_loss: 0.5569\n",
      "Epoch 32, Batch 94, loss_ca: 1.7704, adv_loss: 0.5352\n",
      "Epoch 32, Batch 95, loss_ca: 1.7945, adv_loss: 0.5203\n",
      "Epoch 32, Batch 96, loss_ca: 1.8005, adv_loss: 0.5291\n",
      "Epoch 32, Batch 97, loss_ca: 1.7622, adv_loss: 0.5288\n",
      "Epoch 32, Batch 98, loss_ca: 1.7521, adv_loss: 0.5239\n",
      "Epoch 32, Batch 99, loss_ca: 1.7189, adv_loss: 0.5088\n",
      "Epoch 32, Batch 100, loss_ca: 1.8088, adv_loss: 0.4938\n",
      "Epoch 32, Batch 101, loss_ca: 1.9440, adv_loss: 0.5351\n",
      "Epoch 32, Batch 102, loss_ca: 1.9245, adv_loss: 0.5357\n",
      "Epoch 32, Batch 103, loss_ca: 1.9213, adv_loss: 0.5415\n",
      "Epoch 32, Batch 104, loss_ca: 1.8607, adv_loss: 0.5376\n",
      "Epoch 32, Batch 105, loss_ca: 1.9945, adv_loss: 0.5936\n",
      "Epoch 32, Batch 106, loss_ca: 1.9620, adv_loss: 0.6094\n",
      "Epoch 32, Batch 107, loss_ca: 2.0115, adv_loss: 0.6299\n",
      "Epoch 32, Batch 108, loss_ca: 1.9116, adv_loss: 0.5944\n",
      "Epoch 32, Batch 109, loss_ca: 1.8268, adv_loss: 0.5525\n",
      "Epoch 32, Batch 110, loss_ca: 1.7327, adv_loss: 0.5475\n",
      "Epoch 32, Batch 111, loss_ca: 1.7149, adv_loss: 0.5382\n",
      "Epoch 32, Batch 112, loss_ca: 1.7409, adv_loss: 0.5301\n",
      "Epoch 32, Batch 113, loss_ca: 1.7194, adv_loss: 0.5307\n",
      "Epoch 32, Batch 114, loss_ca: 1.7042, adv_loss: 0.5104\n",
      "Epoch 32, Batch 115, loss_ca: 1.6549, adv_loss: 0.4966\n",
      "Epoch 32, Batch 116, loss_ca: 1.6937, adv_loss: 0.4905\n",
      "Epoch 32, Batch 117, loss_ca: 1.7088, adv_loss: 0.4973\n",
      "Epoch 32, Batch 118, loss_ca: 1.7091, adv_loss: 0.4965\n",
      "Epoch 32, Batch 119, loss_ca: 1.7028, adv_loss: 0.5043\n",
      "Epoch 32, Batch 120, loss_ca: 1.6902, adv_loss: 0.4982\n",
      "Epoch 32, Batch 121, loss_ca: 1.8813, adv_loss: 0.5030\n",
      "Epoch 32, Batch 122, loss_ca: 1.8928, adv_loss: 0.5428\n",
      "Epoch 32, Batch 123, loss_ca: 1.8217, adv_loss: 0.5388\n",
      "Epoch 32, Batch 124, loss_ca: 1.8513, adv_loss: 0.5614\n",
      "Epoch 32, Batch 125, loss_ca: 1.8134, adv_loss: 0.5459\n",
      "Epoch 32, Batch 126, loss_ca: 1.8771, adv_loss: 0.5096\n",
      "Epoch 32, Batch 127, loss_ca: 1.8975, adv_loss: 0.5242\n",
      "Epoch 32, Batch 128, loss_ca: 1.9114, adv_loss: 0.5503\n",
      "Epoch 32, Batch 129, loss_ca: 1.9070, adv_loss: 0.5575\n",
      "Epoch 32, Batch 130, loss_ca: 1.8624, adv_loss: 0.5725\n",
      "Epoch 32, Batch 131, loss_ca: 1.8142, adv_loss: 0.5543\n",
      "Epoch 32, Batch 132, loss_ca: 1.8596, adv_loss: 0.5715\n",
      "Epoch 32, Batch 133, loss_ca: 1.8860, adv_loss: 0.5729\n",
      "Epoch 32, Batch 134, loss_ca: 1.8840, adv_loss: 0.5793\n",
      "Epoch 32, Batch 135, loss_ca: 1.8741, adv_loss: 0.5757\n",
      "Epoch 32, Batch 136, loss_ca: 1.8578, adv_loss: 0.5025\n",
      "Epoch 32, Batch 137, loss_ca: 1.7447, adv_loss: 0.4776\n",
      "Epoch 32, Batch 138, loss_ca: 1.6776, adv_loss: 0.5156\n",
      "Epoch 32, Batch 139, loss_ca: 1.6482, adv_loss: 0.5143\n",
      "Epoch 32, Batch 140, loss_ca: 1.6430, adv_loss: 0.5083\n",
      "Epoch 32, Batch 141, loss_ca: 1.6570, adv_loss: 0.5138\n",
      "Epoch 32, Batch 142, loss_ca: 1.7360, adv_loss: 0.5208\n",
      "Epoch 32, Batch 143, loss_ca: 1.7023, adv_loss: 0.5265\n",
      "Epoch 32, Batch 144, loss_ca: 1.6769, adv_loss: 0.5522\n",
      "Epoch 32, Batch 145, loss_ca: 1.7020, adv_loss: 0.5400\n",
      "Epoch 32, Batch 146, loss_ca: 1.7274, adv_loss: 0.5396\n",
      "Epoch 32, Batch 147, loss_ca: 1.8579, adv_loss: 0.5943\n",
      "Epoch 32, Batch 148, loss_ca: 1.8481, adv_loss: 0.5975\n",
      "Epoch 32, Batch 149, loss_ca: 1.7870, adv_loss: 0.5530\n",
      "Epoch 32, Batch 150, loss_ca: 1.7536, adv_loss: 0.5513\n",
      "Epoch 32, Batch 151, loss_ca: 1.7116, adv_loss: 0.5552\n",
      "Epoch 32, Batch 152, loss_ca: 1.6674, adv_loss: 0.5419\n",
      "Epoch 32, Batch 153, loss_ca: 1.7955, adv_loss: 0.5171\n",
      "Epoch 32, Batch 154, loss_ca: 1.8354, adv_loss: 0.4805\n",
      "Epoch 32, Batch 155, loss_ca: 1.7831, adv_loss: 0.4931\n",
      "Epoch 32, Batch 156, loss_ca: 1.7696, adv_loss: 0.4987\n",
      "Epoch 32, Batch 157, loss_ca: 1.8098, adv_loss: 0.4671\n",
      "Epoch 32, Batch 158, loss_ca: 1.9121, adv_loss: 0.5349\n",
      "Epoch 32, Batch 159, loss_ca: 2.1391, adv_loss: 0.5125\n",
      "Epoch 32, Batch 160, loss_ca: 2.0314, adv_loss: 0.5078\n",
      "Epoch 32, Batch 161, loss_ca: 1.8902, adv_loss: 0.5012\n",
      "Epoch 32, Batch 162, loss_ca: 2.1800, adv_loss: 0.5346\n",
      "Epoch 32, Batch 163, loss_ca: 1.9530, adv_loss: 0.5368\n",
      "Epoch 32, Batch 164, loss_ca: 2.0952, adv_loss: 0.5491\n",
      "Epoch 32, Batch 165, loss_ca: 2.1481, adv_loss: 0.5244\n",
      "Epoch 32, Batch 166, loss_ca: 2.1280, adv_loss: 0.5869\n",
      "Epoch 32, Batch 167, loss_ca: 1.9543, adv_loss: 0.5420\n",
      "Epoch 32, Batch 168, loss_ca: 2.1556, adv_loss: 0.5729\n",
      "Epoch 32, Batch 169, loss_ca: 2.1575, adv_loss: 0.5846\n",
      "Epoch 32, Batch 170, loss_ca: 2.0322, adv_loss: 0.5383\n",
      "Epoch 32, Batch 171, loss_ca: 2.1741, adv_loss: 0.5667\n",
      "Epoch 32, Batch 172, loss_ca: 2.3228, adv_loss: 0.5872\n",
      "Epoch 32, Batch 173, loss_ca: 1.9316, adv_loss: 0.5406\n",
      "Epoch 32, Batch 174, loss_ca: 1.8751, adv_loss: 0.5353\n",
      "Epoch 32, Batch 175, loss_ca: 1.7755, adv_loss: 0.5083\n",
      "Epoch 32, Batch 176, loss_ca: 1.7515, adv_loss: 0.5067\n",
      "Epoch 32, Batch 177, loss_ca: 1.7158, adv_loss: 0.5169\n",
      "Epoch 32, Batch 178, loss_ca: 1.8876, adv_loss: 0.5099\n",
      "Epoch 32, Batch 179, loss_ca: 1.8796, adv_loss: 0.5028\n",
      "Epoch 32, Batch 180, loss_ca: 1.7117, adv_loss: 0.5175\n",
      "Epoch 32, Batch 181, loss_ca: 1.7532, adv_loss: 0.5251\n",
      "Epoch 32, Batch 182, loss_ca: 1.6578, adv_loss: 0.5148\n",
      "Epoch 32, Batch 183, loss_ca: 1.8209, adv_loss: 0.5182\n",
      "Epoch 32, Batch 184, loss_ca: 1.7662, adv_loss: 0.4998\n",
      "Epoch 32, Batch 185, loss_ca: 1.7308, adv_loss: 0.5129\n",
      "Epoch 32, Batch 186, loss_ca: 1.7256, adv_loss: 0.5095\n",
      "Epoch 32, Batch 187, loss_ca: 1.7757, adv_loss: 0.5077\n",
      "Epoch 32, Batch 188, loss_ca: 1.7238, adv_loss: 0.5071\n",
      "Epoch 32, Batch 189, loss_ca: 1.8389, adv_loss: 0.5257\n",
      "Epoch 32, Batch 190, loss_ca: 1.8879, adv_loss: 0.5289\n",
      "Epoch 32, Batch 191, loss_ca: 1.9514, adv_loss: 0.5119\n",
      "Epoch 32, Batch 192, loss_ca: 2.0880, adv_loss: 0.5645\n",
      "Epoch 32, Batch 193, loss_ca: 1.8729, adv_loss: 0.5788\n",
      "Epoch 32, Batch 194, loss_ca: 1.7661, adv_loss: 0.5880\n",
      "Epoch 32, Batch 195, loss_ca: 1.9004, adv_loss: 0.5748\n",
      "Epoch 32, Batch 196, loss_ca: 2.3125, adv_loss: 0.5893\n",
      "Epoch 32, Batch 197, loss_ca: 2.0741, adv_loss: 0.5339\n",
      "Epoch 32, Batch 198, loss_ca: 2.0355, adv_loss: 0.4952\n",
      "Epoch 32, Batch 199, loss_ca: 1.9512, adv_loss: 0.5038\n",
      "Epoch 32, Batch 200, loss_ca: 1.7811, adv_loss: 0.5068\n",
      "Epoch 32, Batch 201, loss_ca: 2.0725, adv_loss: 0.4952\n",
      "Epoch 32, Batch 202, loss_ca: 1.9721, adv_loss: 0.5419\n",
      "Epoch 32, Batch 203, loss_ca: 2.1298, adv_loss: 0.5554\n",
      "Epoch 32, Batch 204, loss_ca: 1.8823, adv_loss: 0.5000\n",
      "Epoch 32, Batch 205, loss_ca: 2.0021, adv_loss: 0.4917\n",
      "Epoch 32, Batch 206, loss_ca: 1.9574, adv_loss: 0.5527\n",
      "Epoch 32, Batch 207, loss_ca: 2.0555, adv_loss: 0.5259\n",
      "Epoch 32, Batch 208, loss_ca: 1.9323, adv_loss: 0.5355\n",
      "Epoch 32, Batch 209, loss_ca: 1.7782, adv_loss: 0.4989\n",
      "Epoch 32, Batch 210, loss_ca: 1.9398, adv_loss: 0.5409\n",
      "Epoch 32, Batch 211, loss_ca: 1.7924, adv_loss: 0.6259\n",
      "Epoch 32, Batch 212, loss_ca: 1.6776, adv_loss: 0.6012\n",
      "Epoch 32, Batch 213, loss_ca: 1.9424, adv_loss: 0.6451\n",
      "Epoch 32, Batch 214, loss_ca: 1.7120, adv_loss: 0.5716\n",
      "Epoch 32, Batch 215, loss_ca: 1.7164, adv_loss: 0.5913\n",
      "Epoch 32, Batch 216, loss_ca: 1.8019, adv_loss: 0.6043\n",
      "Epoch 32, Batch 217, loss_ca: 1.7118, adv_loss: 0.6446\n",
      "Epoch 32, Batch 218, loss_ca: 1.7443, adv_loss: 0.6274\n",
      "Epoch 32, Batch 219, loss_ca: 2.1971, adv_loss: 0.5842\n",
      "Epoch 32, Batch 220, loss_ca: 1.9319, adv_loss: 0.5589\n",
      "Epoch 32, Batch 221, loss_ca: 1.9021, adv_loss: 0.5218\n",
      "Epoch 32, Batch 222, loss_ca: 1.7845, adv_loss: 0.5303\n",
      "Epoch 32, Batch 223, loss_ca: 1.7151, adv_loss: 0.5302\n",
      "Epoch 32, Batch 224, loss_ca: 1.6368, adv_loss: 0.5262\n",
      "Epoch 32, Batch 225, loss_ca: 1.6441, adv_loss: 0.5237\n",
      "Epoch 32, Batch 226, loss_ca: 1.7163, adv_loss: 0.5161\n",
      "Epoch 32, Batch 227, loss_ca: 1.6651, adv_loss: 0.5247\n",
      "Epoch 32, Batch 228, loss_ca: 1.6761, adv_loss: 0.5184\n",
      "Epoch 33, Batch 33, loss_ca: 1.7976, adv_loss: 0.5045\n",
      "Epoch 33, Batch 34, loss_ca: 2.0351, adv_loss: 0.5962\n",
      "Epoch 33, Batch 35, loss_ca: 2.0019, adv_loss: 0.5973\n",
      "Epoch 33, Batch 36, loss_ca: 1.9818, adv_loss: 0.5818\n",
      "Epoch 33, Batch 37, loss_ca: 1.9909, adv_loss: 0.5624\n",
      "Epoch 33, Batch 38, loss_ca: 1.9817, adv_loss: 0.5691\n",
      "Epoch 33, Batch 39, loss_ca: 1.9675, adv_loss: 0.5737\n",
      "Epoch 33, Batch 40, loss_ca: 2.0500, adv_loss: 0.5651\n",
      "Epoch 33, Batch 41, loss_ca: 1.9122, adv_loss: 0.5012\n",
      "Epoch 33, Batch 42, loss_ca: 1.8906, adv_loss: 0.5533\n",
      "Epoch 33, Batch 43, loss_ca: 1.7909, adv_loss: 0.5525\n",
      "Epoch 33, Batch 44, loss_ca: 1.7480, adv_loss: 0.5510\n",
      "Epoch 33, Batch 45, loss_ca: 1.7542, adv_loss: 0.5343\n",
      "Epoch 33, Batch 46, loss_ca: 1.7845, adv_loss: 0.5235\n",
      "Epoch 33, Batch 47, loss_ca: 1.8470, adv_loss: 0.5391\n",
      "Epoch 33, Batch 48, loss_ca: 1.7673, adv_loss: 0.5796\n",
      "Epoch 33, Batch 49, loss_ca: 1.7726, adv_loss: 0.5653\n",
      "Epoch 33, Batch 50, loss_ca: 1.7609, adv_loss: 0.5784\n",
      "Epoch 33, Batch 51, loss_ca: 1.7768, adv_loss: 0.5677\n",
      "Epoch 33, Batch 52, loss_ca: 1.7192, adv_loss: 0.5437\n",
      "Epoch 33, Batch 53, loss_ca: 1.8257, adv_loss: 0.5842\n",
      "Epoch 33, Batch 54, loss_ca: 1.7760, adv_loss: 0.5704\n",
      "Epoch 33, Batch 55, loss_ca: 1.7634, adv_loss: 0.5601\n",
      "Epoch 33, Batch 56, loss_ca: 1.7509, adv_loss: 0.5440\n",
      "Epoch 33, Batch 57, loss_ca: 1.7251, adv_loss: 0.5261\n",
      "Epoch 33, Batch 58, loss_ca: 1.7945, adv_loss: 0.5267\n",
      "Epoch 33, Batch 59, loss_ca: 1.7401, adv_loss: 0.4895\n",
      "Epoch 33, Batch 60, loss_ca: 1.7128, adv_loss: 0.4953\n",
      "Epoch 33, Batch 61, loss_ca: 1.7217, adv_loss: 0.4970\n",
      "Epoch 33, Batch 62, loss_ca: 1.7718, adv_loss: 0.5216\n",
      "Epoch 33, Batch 63, loss_ca: 1.7734, adv_loss: 0.5925\n",
      "Epoch 33, Batch 64, loss_ca: 1.8016, adv_loss: 0.5908\n",
      "Epoch 33, Batch 65, loss_ca: 1.7219, adv_loss: 0.5381\n",
      "Epoch 33, Batch 66, loss_ca: 1.7518, adv_loss: 0.5524\n",
      "Epoch 33, Batch 67, loss_ca: 1.7351, adv_loss: 0.5258\n",
      "Epoch 33, Batch 68, loss_ca: 1.7613, adv_loss: 0.5382\n",
      "Epoch 33, Batch 69, loss_ca: 1.7757, adv_loss: 0.5048\n",
      "Epoch 33, Batch 70, loss_ca: 1.7514, adv_loss: 0.5057\n",
      "Epoch 33, Batch 71, loss_ca: 1.6421, adv_loss: 0.5043\n",
      "Epoch 33, Batch 72, loss_ca: 1.6749, adv_loss: 0.4901\n",
      "Epoch 33, Batch 73, loss_ca: 1.6066, adv_loss: 0.4945\n",
      "Epoch 33, Batch 74, loss_ca: 1.6197, adv_loss: 0.4966\n",
      "Epoch 33, Batch 75, loss_ca: 1.6047, adv_loss: 0.4903\n",
      "Epoch 33, Batch 76, loss_ca: 1.6237, adv_loss: 0.4830\n",
      "Epoch 33, Batch 77, loss_ca: 1.6596, adv_loss: 0.4852\n",
      "Epoch 33, Batch 78, loss_ca: 1.6979, adv_loss: 0.4762\n",
      "Epoch 33, Batch 79, loss_ca: 1.7190, adv_loss: 0.4838\n",
      "Epoch 33, Batch 80, loss_ca: 1.6520, adv_loss: 0.4914\n",
      "Epoch 33, Batch 81, loss_ca: 1.6304, adv_loss: 0.5018\n",
      "Epoch 33, Batch 82, loss_ca: 1.7322, adv_loss: 0.5219\n",
      "Epoch 33, Batch 83, loss_ca: 1.7078, adv_loss: 0.5013\n",
      "Epoch 33, Batch 84, loss_ca: 1.7172, adv_loss: 0.4931\n",
      "Epoch 33, Batch 85, loss_ca: 1.7289, adv_loss: 0.5092\n",
      "Epoch 33, Batch 86, loss_ca: 1.7650, adv_loss: 0.4840\n",
      "Epoch 33, Batch 87, loss_ca: 1.8357, adv_loss: 0.4952\n",
      "Epoch 33, Batch 88, loss_ca: 1.8217, adv_loss: 0.4915\n",
      "Epoch 33, Batch 89, loss_ca: 1.9081, adv_loss: 0.4994\n",
      "Epoch 33, Batch 90, loss_ca: 1.7664, adv_loss: 0.4940\n",
      "Epoch 33, Batch 91, loss_ca: 1.8545, adv_loss: 0.5258\n",
      "Epoch 33, Batch 92, loss_ca: 1.7869, adv_loss: 0.5452\n",
      "Epoch 33, Batch 93, loss_ca: 1.8746, adv_loss: 0.5525\n",
      "Epoch 33, Batch 94, loss_ca: 1.8143, adv_loss: 0.5522\n",
      "Epoch 33, Batch 95, loss_ca: 1.8155, adv_loss: 0.5029\n",
      "Epoch 33, Batch 96, loss_ca: 1.9157, adv_loss: 0.5514\n",
      "Epoch 33, Batch 97, loss_ca: 1.8185, adv_loss: 0.5353\n",
      "Epoch 33, Batch 98, loss_ca: 1.7637, adv_loss: 0.5224\n",
      "Epoch 33, Batch 99, loss_ca: 1.6767, adv_loss: 0.4995\n",
      "Epoch 33, Batch 100, loss_ca: 1.8080, adv_loss: 0.4913\n",
      "Epoch 33, Batch 101, loss_ca: 1.8511, adv_loss: 0.5118\n",
      "Epoch 33, Batch 102, loss_ca: 1.8387, adv_loss: 0.5149\n",
      "Epoch 33, Batch 103, loss_ca: 1.8459, adv_loss: 0.5189\n",
      "Epoch 33, Batch 104, loss_ca: 1.8481, adv_loss: 0.5387\n",
      "Epoch 33, Batch 105, loss_ca: 1.9244, adv_loss: 0.6012\n",
      "Epoch 33, Batch 106, loss_ca: 1.9400, adv_loss: 0.6221\n",
      "Epoch 33, Batch 107, loss_ca: 1.9926, adv_loss: 0.6407\n",
      "Epoch 33, Batch 108, loss_ca: 1.8432, adv_loss: 0.5450\n",
      "Epoch 33, Batch 109, loss_ca: 1.9015, adv_loss: 0.4910\n",
      "Epoch 33, Batch 110, loss_ca: 1.8004, adv_loss: 0.5059\n",
      "Epoch 33, Batch 111, loss_ca: 1.7706, adv_loss: 0.5236\n",
      "Epoch 33, Batch 112, loss_ca: 1.8354, adv_loss: 0.5378\n",
      "Epoch 33, Batch 113, loss_ca: 1.7696, adv_loss: 0.5470\n",
      "Epoch 33, Batch 114, loss_ca: 1.8140, adv_loss: 0.5671\n",
      "Epoch 33, Batch 115, loss_ca: 1.7413, adv_loss: 0.5699\n",
      "Epoch 33, Batch 116, loss_ca: 1.7175, adv_loss: 0.5374\n",
      "Epoch 33, Batch 117, loss_ca: 1.7344, adv_loss: 0.5468\n",
      "Epoch 33, Batch 118, loss_ca: 1.7468, adv_loss: 0.5392\n",
      "Epoch 33, Batch 119, loss_ca: 1.7562, adv_loss: 0.5294\n",
      "Epoch 33, Batch 120, loss_ca: 1.7418, adv_loss: 0.5276\n",
      "Epoch 33, Batch 121, loss_ca: 1.9201, adv_loss: 0.5154\n",
      "Epoch 33, Batch 122, loss_ca: 1.9227, adv_loss: 0.5241\n",
      "Epoch 33, Batch 123, loss_ca: 1.8889, adv_loss: 0.5202\n",
      "Epoch 33, Batch 124, loss_ca: 1.8699, adv_loss: 0.5147\n",
      "Epoch 33, Batch 125, loss_ca: 1.8588, adv_loss: 0.5241\n",
      "Epoch 33, Batch 126, loss_ca: 1.8717, adv_loss: 0.5372\n",
      "Epoch 33, Batch 127, loss_ca: 1.8900, adv_loss: 0.5912\n",
      "Epoch 33, Batch 128, loss_ca: 1.8784, adv_loss: 0.5802\n",
      "Epoch 33, Batch 129, loss_ca: 1.9363, adv_loss: 0.5988\n",
      "Epoch 33, Batch 130, loss_ca: 1.8858, adv_loss: 0.6276\n",
      "Epoch 33, Batch 131, loss_ca: 1.8211, adv_loss: 0.5518\n",
      "Epoch 33, Batch 132, loss_ca: 1.9335, adv_loss: 0.5918\n",
      "Epoch 33, Batch 133, loss_ca: 1.9175, adv_loss: 0.5984\n",
      "Epoch 33, Batch 134, loss_ca: 1.9135, adv_loss: 0.6057\n",
      "Epoch 33, Batch 135, loss_ca: 1.8698, adv_loss: 0.6055\n",
      "Epoch 33, Batch 136, loss_ca: 1.8373, adv_loss: 0.5813\n",
      "Epoch 33, Batch 137, loss_ca: 1.7196, adv_loss: 0.5098\n",
      "Epoch 33, Batch 138, loss_ca: 1.6937, adv_loss: 0.5039\n",
      "Epoch 33, Batch 139, loss_ca: 1.6947, adv_loss: 0.5012\n",
      "Epoch 33, Batch 140, loss_ca: 1.6774, adv_loss: 0.4904\n",
      "Epoch 33, Batch 141, loss_ca: 1.6797, adv_loss: 0.4951\n",
      "Epoch 33, Batch 142, loss_ca: 1.7903, adv_loss: 0.5270\n",
      "Epoch 33, Batch 143, loss_ca: 1.7063, adv_loss: 0.5284\n",
      "Epoch 33, Batch 144, loss_ca: 1.7047, adv_loss: 0.5718\n",
      "Epoch 33, Batch 145, loss_ca: 1.7125, adv_loss: 0.5542\n",
      "Epoch 33, Batch 146, loss_ca: 1.6777, adv_loss: 0.5178\n",
      "Epoch 33, Batch 147, loss_ca: 1.7378, adv_loss: 0.5313\n",
      "Epoch 33, Batch 148, loss_ca: 1.7455, adv_loss: 0.5357\n",
      "Epoch 33, Batch 149, loss_ca: 1.6650, adv_loss: 0.4869\n",
      "Epoch 33, Batch 150, loss_ca: 1.6716, adv_loss: 0.4817\n",
      "Epoch 33, Batch 151, loss_ca: 1.6060, adv_loss: 0.4900\n",
      "Epoch 33, Batch 152, loss_ca: 1.5909, adv_loss: 0.4899\n",
      "Epoch 33, Batch 153, loss_ca: 1.7823, adv_loss: 0.4942\n",
      "Epoch 33, Batch 154, loss_ca: 1.8047, adv_loss: 0.5020\n",
      "Epoch 33, Batch 155, loss_ca: 1.8260, adv_loss: 0.5088\n",
      "Epoch 33, Batch 156, loss_ca: 1.7960, adv_loss: 0.4987\n",
      "Epoch 33, Batch 157, loss_ca: 1.8482, adv_loss: 0.5240\n",
      "Epoch 33, Batch 158, loss_ca: 1.9078, adv_loss: 0.5519\n",
      "Epoch 33, Batch 159, loss_ca: 2.0880, adv_loss: 0.5257\n",
      "Epoch 33, Batch 160, loss_ca: 2.1351, adv_loss: 0.5758\n",
      "Epoch 33, Batch 161, loss_ca: 1.8218, adv_loss: 0.5638\n",
      "Epoch 33, Batch 162, loss_ca: 2.0575, adv_loss: 0.5123\n",
      "Epoch 33, Batch 163, loss_ca: 2.0644, adv_loss: 0.5441\n",
      "Epoch 33, Batch 164, loss_ca: 2.2102, adv_loss: 0.5025\n",
      "Epoch 33, Batch 165, loss_ca: 2.2338, adv_loss: 0.4899\n",
      "Epoch 33, Batch 166, loss_ca: 2.0763, adv_loss: 0.5683\n",
      "Epoch 33, Batch 167, loss_ca: 1.9620, adv_loss: 0.5305\n",
      "Epoch 33, Batch 168, loss_ca: 2.1594, adv_loss: 0.5454\n",
      "Epoch 33, Batch 169, loss_ca: 2.1826, adv_loss: 0.5644\n",
      "Epoch 33, Batch 170, loss_ca: 2.0588, adv_loss: 0.5496\n",
      "Epoch 33, Batch 171, loss_ca: 2.0931, adv_loss: 0.5385\n",
      "Epoch 33, Batch 172, loss_ca: 2.1672, adv_loss: 0.5670\n",
      "Epoch 33, Batch 173, loss_ca: 2.0126, adv_loss: 0.5518\n",
      "Epoch 33, Batch 174, loss_ca: 1.8555, adv_loss: 0.5199\n",
      "Epoch 33, Batch 175, loss_ca: 1.8832, adv_loss: 0.5256\n",
      "Epoch 33, Batch 176, loss_ca: 1.9364, adv_loss: 0.5338\n",
      "Epoch 33, Batch 177, loss_ca: 1.7762, adv_loss: 0.5251\n",
      "Epoch 33, Batch 178, loss_ca: 1.8444, adv_loss: 0.5225\n",
      "Epoch 33, Batch 179, loss_ca: 1.7804, adv_loss: 0.4926\n",
      "Epoch 33, Batch 180, loss_ca: 1.7112, adv_loss: 0.4866\n",
      "Epoch 33, Batch 181, loss_ca: 1.7044, adv_loss: 0.4825\n",
      "Epoch 33, Batch 182, loss_ca: 1.7086, adv_loss: 0.4662\n",
      "Epoch 33, Batch 183, loss_ca: 1.7869, adv_loss: 0.5150\n",
      "Epoch 33, Batch 184, loss_ca: 1.7212, adv_loss: 0.4942\n",
      "Epoch 33, Batch 185, loss_ca: 1.6948, adv_loss: 0.4933\n",
      "Epoch 33, Batch 186, loss_ca: 1.6938, adv_loss: 0.4874\n",
      "Epoch 33, Batch 187, loss_ca: 1.6910, adv_loss: 0.4900\n",
      "Epoch 33, Batch 188, loss_ca: 1.6612, adv_loss: 0.5061\n",
      "Epoch 33, Batch 189, loss_ca: 1.7586, adv_loss: 0.4813\n",
      "Epoch 33, Batch 190, loss_ca: 2.2468, adv_loss: 0.5431\n",
      "Epoch 33, Batch 191, loss_ca: 1.9206, adv_loss: 0.5194\n",
      "Epoch 33, Batch 192, loss_ca: 2.1948, adv_loss: 0.5702\n",
      "Epoch 33, Batch 193, loss_ca: 2.0168, adv_loss: 0.5887\n",
      "Epoch 33, Batch 194, loss_ca: 1.7271, adv_loss: 0.5908\n",
      "Epoch 33, Batch 195, loss_ca: 1.8170, adv_loss: 0.5612\n",
      "Epoch 33, Batch 196, loss_ca: 2.2192, adv_loss: 0.5740\n",
      "Epoch 33, Batch 197, loss_ca: 1.8254, adv_loss: 0.5132\n",
      "Epoch 33, Batch 198, loss_ca: 1.6359, adv_loss: 0.4840\n",
      "Epoch 33, Batch 199, loss_ca: 1.5774, adv_loss: 0.4401\n",
      "Epoch 33, Batch 200, loss_ca: 1.5863, adv_loss: 0.4718\n",
      "Epoch 33, Batch 201, loss_ca: 1.9343, adv_loss: 0.5201\n",
      "Epoch 33, Batch 202, loss_ca: 2.0524, adv_loss: 0.5575\n",
      "Epoch 33, Batch 203, loss_ca: 2.5187, adv_loss: 0.5599\n",
      "Epoch 33, Batch 204, loss_ca: 1.9986, adv_loss: 0.5313\n",
      "Epoch 33, Batch 205, loss_ca: 2.1702, adv_loss: 0.5493\n",
      "Epoch 33, Batch 206, loss_ca: 2.0559, adv_loss: 0.5280\n",
      "Epoch 33, Batch 207, loss_ca: 2.0617, adv_loss: 0.5771\n",
      "Epoch 33, Batch 208, loss_ca: 1.9573, adv_loss: 0.5864\n",
      "Epoch 33, Batch 209, loss_ca: 1.7685, adv_loss: 0.5131\n",
      "Epoch 33, Batch 210, loss_ca: 1.8001, adv_loss: 0.5254\n",
      "Epoch 33, Batch 211, loss_ca: 1.7736, adv_loss: 0.5997\n",
      "Epoch 33, Batch 212, loss_ca: 1.5384, adv_loss: 0.6004\n",
      "Epoch 33, Batch 213, loss_ca: 1.9441, adv_loss: 0.5759\n",
      "Epoch 33, Batch 214, loss_ca: 2.0441, adv_loss: 0.6370\n",
      "Epoch 33, Batch 215, loss_ca: 1.6391, adv_loss: 0.6417\n",
      "Epoch 33, Batch 216, loss_ca: 1.7142, adv_loss: 0.6699\n",
      "Epoch 33, Batch 217, loss_ca: 1.6520, adv_loss: 0.6243\n",
      "Epoch 33, Batch 218, loss_ca: 1.6393, adv_loss: 0.5920\n",
      "Epoch 33, Batch 219, loss_ca: 1.9602, adv_loss: 0.5217\n",
      "Epoch 33, Batch 220, loss_ca: 1.8507, adv_loss: 0.4994\n",
      "Epoch 33, Batch 221, loss_ca: 1.7041, adv_loss: 0.4774\n",
      "Epoch 33, Batch 222, loss_ca: 1.6811, adv_loss: 0.4812\n",
      "Epoch 33, Batch 223, loss_ca: 1.6999, adv_loss: 0.4924\n",
      "Epoch 33, Batch 224, loss_ca: 1.6397, adv_loss: 0.4930\n",
      "Epoch 33, Batch 225, loss_ca: 1.6433, adv_loss: 0.4848\n",
      "Epoch 33, Batch 226, loss_ca: 1.6668, adv_loss: 0.4959\n",
      "Epoch 33, Batch 227, loss_ca: 1.6276, adv_loss: 0.5146\n",
      "Epoch 33, Batch 228, loss_ca: 1.6763, adv_loss: 0.5082\n",
      "Epoch 34, Batch 34, loss_ca: 2.0070, adv_loss: 0.5947\n",
      "Epoch 34, Batch 35, loss_ca: 2.0284, adv_loss: 0.6461\n",
      "Epoch 34, Batch 36, loss_ca: 1.9951, adv_loss: 0.6325\n",
      "Epoch 34, Batch 37, loss_ca: 1.9034, adv_loss: 0.6143\n",
      "Epoch 34, Batch 38, loss_ca: 1.9094, adv_loss: 0.6065\n",
      "Epoch 34, Batch 39, loss_ca: 1.9397, adv_loss: 0.6316\n",
      "Epoch 34, Batch 40, loss_ca: 1.8545, adv_loss: 0.6111\n",
      "Epoch 34, Batch 41, loss_ca: 1.7706, adv_loss: 0.5224\n",
      "Epoch 34, Batch 42, loss_ca: 1.8606, adv_loss: 0.5367\n",
      "Epoch 34, Batch 43, loss_ca: 1.8361, adv_loss: 0.5326\n",
      "Epoch 34, Batch 44, loss_ca: 1.8498, adv_loss: 0.5469\n",
      "Epoch 34, Batch 45, loss_ca: 1.7880, adv_loss: 0.5555\n",
      "Epoch 34, Batch 46, loss_ca: 1.7297, adv_loss: 0.5430\n",
      "Epoch 34, Batch 47, loss_ca: 1.9196, adv_loss: 0.5639\n",
      "Epoch 34, Batch 48, loss_ca: 1.8113, adv_loss: 0.5647\n",
      "Epoch 34, Batch 49, loss_ca: 1.7589, adv_loss: 0.5532\n",
      "Epoch 34, Batch 50, loss_ca: 1.7620, adv_loss: 0.5394\n",
      "Epoch 34, Batch 51, loss_ca: 1.7705, adv_loss: 0.5600\n",
      "Epoch 34, Batch 52, loss_ca: 1.7538, adv_loss: 0.5355\n",
      "Epoch 34, Batch 53, loss_ca: 1.8305, adv_loss: 0.5558\n",
      "Epoch 34, Batch 54, loss_ca: 1.7777, adv_loss: 0.5467\n",
      "Epoch 34, Batch 55, loss_ca: 1.7358, adv_loss: 0.5507\n",
      "Epoch 34, Batch 56, loss_ca: 1.7035, adv_loss: 0.5572\n",
      "Epoch 34, Batch 57, loss_ca: 1.6930, adv_loss: 0.5522\n",
      "Epoch 34, Batch 58, loss_ca: 1.8914, adv_loss: 0.5681\n",
      "Epoch 34, Batch 59, loss_ca: 1.7022, adv_loss: 0.5496\n",
      "Epoch 34, Batch 60, loss_ca: 1.6607, adv_loss: 0.5440\n",
      "Epoch 34, Batch 61, loss_ca: 1.6566, adv_loss: 0.5226\n",
      "Epoch 34, Batch 62, loss_ca: 1.7356, adv_loss: 0.5362\n",
      "Epoch 34, Batch 63, loss_ca: 1.8482, adv_loss: 0.5420\n",
      "Epoch 34, Batch 64, loss_ca: 1.8598, adv_loss: 0.5641\n",
      "Epoch 34, Batch 65, loss_ca: 1.7569, adv_loss: 0.5479\n",
      "Epoch 34, Batch 66, loss_ca: 1.8085, adv_loss: 0.5675\n",
      "Epoch 34, Batch 67, loss_ca: 1.7718, adv_loss: 0.5037\n",
      "Epoch 34, Batch 68, loss_ca: 1.7826, adv_loss: 0.5140\n",
      "Epoch 34, Batch 69, loss_ca: 1.8125, adv_loss: 0.5061\n",
      "Epoch 34, Batch 70, loss_ca: 1.7752, adv_loss: 0.4942\n",
      "Epoch 34, Batch 71, loss_ca: 1.7433, adv_loss: 0.4824\n",
      "Epoch 34, Batch 72, loss_ca: 1.7899, adv_loss: 0.4892\n",
      "Epoch 34, Batch 73, loss_ca: 1.7155, adv_loss: 0.4879\n",
      "Epoch 34, Batch 74, loss_ca: 1.7059, adv_loss: 0.4966\n",
      "Epoch 34, Batch 75, loss_ca: 1.6613, adv_loss: 0.4956\n",
      "Epoch 34, Batch 76, loss_ca: 1.6467, adv_loss: 0.4888\n",
      "Epoch 34, Batch 77, loss_ca: 1.6010, adv_loss: 0.4780\n",
      "Epoch 34, Batch 78, loss_ca: 1.6602, adv_loss: 0.4823\n",
      "Epoch 34, Batch 79, loss_ca: 1.6707, adv_loss: 0.4726\n",
      "Epoch 34, Batch 80, loss_ca: 1.6672, adv_loss: 0.4837\n",
      "Epoch 34, Batch 81, loss_ca: 1.6966, adv_loss: 0.4807\n",
      "Epoch 34, Batch 82, loss_ca: 1.7296, adv_loss: 0.5191\n",
      "Epoch 34, Batch 83, loss_ca: 1.7073, adv_loss: 0.5128\n",
      "Epoch 34, Batch 84, loss_ca: 1.7273, adv_loss: 0.5286\n",
      "Epoch 34, Batch 85, loss_ca: 1.7515, adv_loss: 0.5240\n",
      "Epoch 34, Batch 86, loss_ca: 1.8451, adv_loss: 0.5526\n",
      "Epoch 34, Batch 87, loss_ca: 1.8156, adv_loss: 0.5390\n",
      "Epoch 34, Batch 88, loss_ca: 1.7818, adv_loss: 0.5240\n",
      "Epoch 34, Batch 89, loss_ca: 1.8202, adv_loss: 0.5100\n",
      "Epoch 34, Batch 90, loss_ca: 1.7638, adv_loss: 0.4665\n",
      "Epoch 34, Batch 91, loss_ca: 1.8942, adv_loss: 0.5214\n",
      "Epoch 34, Batch 92, loss_ca: 1.8584, adv_loss: 0.5148\n",
      "Epoch 34, Batch 93, loss_ca: 1.8636, adv_loss: 0.5344\n",
      "Epoch 34, Batch 94, loss_ca: 1.7637, adv_loss: 0.5080\n",
      "Epoch 34, Batch 95, loss_ca: 1.7374, adv_loss: 0.4527\n",
      "Epoch 34, Batch 96, loss_ca: 1.7722, adv_loss: 0.4879\n",
      "Epoch 34, Batch 97, loss_ca: 1.7536, adv_loss: 0.5002\n",
      "Epoch 34, Batch 98, loss_ca: 1.7313, adv_loss: 0.5071\n",
      "Epoch 34, Batch 99, loss_ca: 1.6697, adv_loss: 0.4983\n",
      "Epoch 34, Batch 100, loss_ca: 1.8412, adv_loss: 0.4970\n",
      "Epoch 34, Batch 101, loss_ca: 1.9595, adv_loss: 0.5290\n",
      "Epoch 34, Batch 102, loss_ca: 1.8526, adv_loss: 0.5255\n",
      "Epoch 34, Batch 103, loss_ca: 1.8437, adv_loss: 0.5236\n",
      "Epoch 34, Batch 104, loss_ca: 1.8698, adv_loss: 0.5159\n",
      "Epoch 34, Batch 105, loss_ca: 2.0083, adv_loss: 0.6066\n",
      "Epoch 34, Batch 106, loss_ca: 1.9942, adv_loss: 0.6083\n",
      "Epoch 34, Batch 107, loss_ca: 2.1174, adv_loss: 0.6165\n",
      "Epoch 34, Batch 108, loss_ca: 1.9864, adv_loss: 0.5804\n",
      "Epoch 34, Batch 109, loss_ca: 1.9270, adv_loss: 0.5197\n",
      "Epoch 34, Batch 110, loss_ca: 1.7869, adv_loss: 0.4989\n",
      "Epoch 34, Batch 111, loss_ca: 1.7270, adv_loss: 0.4947\n",
      "Epoch 34, Batch 112, loss_ca: 1.7706, adv_loss: 0.4939\n",
      "Epoch 34, Batch 113, loss_ca: 1.6914, adv_loss: 0.4898\n",
      "Epoch 34, Batch 114, loss_ca: 1.7101, adv_loss: 0.5007\n",
      "Epoch 34, Batch 115, loss_ca: 1.6653, adv_loss: 0.5017\n",
      "Epoch 34, Batch 116, loss_ca: 1.7025, adv_loss: 0.4953\n",
      "Epoch 34, Batch 117, loss_ca: 1.7238, adv_loss: 0.5113\n",
      "Epoch 34, Batch 118, loss_ca: 1.7203, adv_loss: 0.5187\n",
      "Epoch 34, Batch 119, loss_ca: 1.7229, adv_loss: 0.5112\n",
      "Epoch 34, Batch 120, loss_ca: 1.7150, adv_loss: 0.5114\n",
      "Epoch 34, Batch 121, loss_ca: 1.8827, adv_loss: 0.5412\n",
      "Epoch 34, Batch 122, loss_ca: 1.8874, adv_loss: 0.5515\n",
      "Epoch 34, Batch 123, loss_ca: 1.8675, adv_loss: 0.5601\n",
      "Epoch 34, Batch 124, loss_ca: 1.8531, adv_loss: 0.5686\n",
      "Epoch 34, Batch 125, loss_ca: 1.8484, adv_loss: 0.5679\n",
      "Epoch 34, Batch 126, loss_ca: 1.8326, adv_loss: 0.5670\n",
      "Epoch 34, Batch 127, loss_ca: 1.8591, adv_loss: 0.5815\n",
      "Epoch 34, Batch 128, loss_ca: 1.9073, adv_loss: 0.5997\n",
      "Epoch 34, Batch 129, loss_ca: 1.9807, adv_loss: 0.5955\n",
      "Epoch 34, Batch 130, loss_ca: 1.8920, adv_loss: 0.5759\n",
      "Epoch 34, Batch 131, loss_ca: 1.8617, adv_loss: 0.5359\n",
      "Epoch 34, Batch 132, loss_ca: 1.8655, adv_loss: 0.5887\n",
      "Epoch 34, Batch 133, loss_ca: 1.8881, adv_loss: 0.6041\n",
      "Epoch 34, Batch 134, loss_ca: 1.8880, adv_loss: 0.6231\n",
      "Epoch 34, Batch 135, loss_ca: 1.9013, adv_loss: 0.6204\n",
      "Epoch 34, Batch 136, loss_ca: 1.8821, adv_loss: 0.5805\n",
      "Epoch 34, Batch 137, loss_ca: 1.7934, adv_loss: 0.5382\n",
      "Epoch 34, Batch 138, loss_ca: 1.6959, adv_loss: 0.5310\n",
      "Epoch 34, Batch 139, loss_ca: 1.6516, adv_loss: 0.5194\n",
      "Epoch 34, Batch 140, loss_ca: 1.6259, adv_loss: 0.5064\n",
      "Epoch 34, Batch 141, loss_ca: 1.6177, adv_loss: 0.4953\n",
      "Epoch 34, Batch 142, loss_ca: 1.7240, adv_loss: 0.5270\n",
      "Epoch 34, Batch 143, loss_ca: 1.6990, adv_loss: 0.5551\n",
      "Epoch 34, Batch 144, loss_ca: 1.6978, adv_loss: 0.5363\n",
      "Epoch 34, Batch 145, loss_ca: 1.7383, adv_loss: 0.5104\n",
      "Epoch 34, Batch 146, loss_ca: 1.8067, adv_loss: 0.4764\n",
      "Epoch 34, Batch 147, loss_ca: 1.9250, adv_loss: 0.4845\n",
      "Epoch 34, Batch 148, loss_ca: 1.8970, adv_loss: 0.5198\n",
      "Epoch 34, Batch 149, loss_ca: 1.8063, adv_loss: 0.5256\n",
      "Epoch 34, Batch 150, loss_ca: 1.8129, adv_loss: 0.5504\n",
      "Epoch 34, Batch 151, loss_ca: 1.7070, adv_loss: 0.5537\n",
      "Epoch 34, Batch 152, loss_ca: 1.6918, adv_loss: 0.5505\n",
      "Epoch 34, Batch 153, loss_ca: 1.8094, adv_loss: 0.5376\n",
      "Epoch 34, Batch 154, loss_ca: 1.8575, adv_loss: 0.5119\n",
      "Epoch 34, Batch 155, loss_ca: 1.8156, adv_loss: 0.5167\n",
      "Epoch 34, Batch 156, loss_ca: 1.8180, adv_loss: 0.5116\n",
      "Epoch 34, Batch 157, loss_ca: 1.8042, adv_loss: 0.4948\n",
      "Epoch 34, Batch 158, loss_ca: 2.0463, adv_loss: 0.5697\n",
      "Epoch 34, Batch 159, loss_ca: 2.1437, adv_loss: 0.5450\n",
      "Epoch 34, Batch 160, loss_ca: 1.9260, adv_loss: 0.5152\n",
      "Epoch 34, Batch 161, loss_ca: 1.8384, adv_loss: 0.4815\n",
      "Epoch 34, Batch 162, loss_ca: 2.0676, adv_loss: 0.4894\n",
      "Epoch 34, Batch 163, loss_ca: 1.9028, adv_loss: 0.4899\n",
      "Epoch 34, Batch 164, loss_ca: 2.0966, adv_loss: 0.5135\n",
      "Epoch 34, Batch 165, loss_ca: 2.2170, adv_loss: 0.4782\n",
      "Epoch 34, Batch 166, loss_ca: 2.0410, adv_loss: 0.5653\n",
      "Epoch 34, Batch 167, loss_ca: 1.9261, adv_loss: 0.5497\n",
      "Epoch 34, Batch 168, loss_ca: 2.2564, adv_loss: 0.5873\n",
      "Epoch 34, Batch 169, loss_ca: 2.2876, adv_loss: 0.5969\n",
      "Epoch 34, Batch 170, loss_ca: 2.0107, adv_loss: 0.5280\n",
      "Epoch 34, Batch 171, loss_ca: 2.0242, adv_loss: 0.5542\n",
      "Epoch 34, Batch 172, loss_ca: 2.0713, adv_loss: 0.5534\n",
      "Epoch 34, Batch 173, loss_ca: 1.9202, adv_loss: 0.5408\n",
      "Epoch 34, Batch 174, loss_ca: 1.7991, adv_loss: 0.4972\n",
      "Epoch 34, Batch 175, loss_ca: 1.8016, adv_loss: 0.5002\n",
      "Epoch 34, Batch 176, loss_ca: 1.7520, adv_loss: 0.5024\n",
      "Epoch 34, Batch 177, loss_ca: 1.7184, adv_loss: 0.5059\n",
      "Epoch 34, Batch 178, loss_ca: 1.8651, adv_loss: 0.5272\n",
      "Epoch 34, Batch 179, loss_ca: 1.8107, adv_loss: 0.5083\n",
      "Epoch 34, Batch 180, loss_ca: 1.6404, adv_loss: 0.4919\n",
      "Epoch 34, Batch 181, loss_ca: 1.6528, adv_loss: 0.4860\n",
      "Epoch 34, Batch 182, loss_ca: 1.6169, adv_loss: 0.4708\n",
      "Epoch 34, Batch 183, loss_ca: 1.9634, adv_loss: 0.4891\n",
      "Epoch 34, Batch 184, loss_ca: 1.7643, adv_loss: 0.4666\n",
      "Epoch 34, Batch 185, loss_ca: 1.6989, adv_loss: 0.4871\n",
      "Epoch 34, Batch 186, loss_ca: 1.6439, adv_loss: 0.4881\n",
      "Epoch 34, Batch 187, loss_ca: 1.6979, adv_loss: 0.5160\n",
      "Epoch 34, Batch 188, loss_ca: 1.6898, adv_loss: 0.5175\n",
      "Epoch 34, Batch 189, loss_ca: 1.7480, adv_loss: 0.5189\n",
      "Epoch 34, Batch 190, loss_ca: 1.9476, adv_loss: 0.5594\n",
      "Epoch 34, Batch 191, loss_ca: 2.0217, adv_loss: 0.5515\n",
      "Epoch 34, Batch 192, loss_ca: 2.1180, adv_loss: 0.5684\n",
      "Epoch 34, Batch 193, loss_ca: 1.9741, adv_loss: 0.6010\n",
      "Epoch 34, Batch 194, loss_ca: 1.9336, adv_loss: 0.5826\n",
      "Epoch 34, Batch 195, loss_ca: 1.9445, adv_loss: 0.5755\n",
      "Epoch 34, Batch 196, loss_ca: 2.2202, adv_loss: 0.5827\n",
      "Epoch 34, Batch 197, loss_ca: 1.8041, adv_loss: 0.5293\n",
      "Epoch 34, Batch 198, loss_ca: 2.2049, adv_loss: 0.6288\n",
      "Epoch 34, Batch 199, loss_ca: 2.0882, adv_loss: 0.5938\n",
      "Epoch 34, Batch 200, loss_ca: 1.8066, adv_loss: 0.5435\n",
      "Epoch 34, Batch 201, loss_ca: 1.8320, adv_loss: 0.5081\n",
      "Epoch 34, Batch 202, loss_ca: 1.9142, adv_loss: 0.5459\n",
      "Epoch 34, Batch 203, loss_ca: 2.3850, adv_loss: 0.5352\n",
      "Epoch 34, Batch 204, loss_ca: 1.8668, adv_loss: 0.5029\n",
      "Epoch 34, Batch 205, loss_ca: 2.0440, adv_loss: 0.4843\n",
      "Epoch 34, Batch 206, loss_ca: 1.9225, adv_loss: 0.5488\n",
      "Epoch 34, Batch 207, loss_ca: 2.1657, adv_loss: 0.5420\n",
      "Epoch 34, Batch 208, loss_ca: 2.0101, adv_loss: 0.5277\n",
      "Epoch 34, Batch 209, loss_ca: 1.7465, adv_loss: 0.5059\n",
      "Epoch 34, Batch 210, loss_ca: 1.8009, adv_loss: 0.5292\n",
      "Epoch 34, Batch 211, loss_ca: 1.7391, adv_loss: 0.5932\n",
      "Epoch 34, Batch 212, loss_ca: 1.5233, adv_loss: 0.5821\n",
      "Epoch 34, Batch 213, loss_ca: 1.8026, adv_loss: 0.5554\n",
      "Epoch 34, Batch 214, loss_ca: 1.6674, adv_loss: 0.4397\n",
      "Epoch 34, Batch 215, loss_ca: 1.5418, adv_loss: 0.5110\n",
      "Epoch 34, Batch 216, loss_ca: 1.6191, adv_loss: 0.6014\n",
      "Epoch 34, Batch 217, loss_ca: 1.6093, adv_loss: 0.5993\n",
      "Epoch 34, Batch 218, loss_ca: 1.7517, adv_loss: 0.6105\n",
      "Epoch 34, Batch 219, loss_ca: 2.1694, adv_loss: 0.5866\n",
      "Epoch 34, Batch 220, loss_ca: 1.9913, adv_loss: 0.5882\n",
      "Epoch 34, Batch 221, loss_ca: 1.8572, adv_loss: 0.5456\n",
      "Epoch 34, Batch 222, loss_ca: 1.7854, adv_loss: 0.5294\n",
      "Epoch 34, Batch 223, loss_ca: 1.7149, adv_loss: 0.5245\n",
      "Epoch 34, Batch 224, loss_ca: 1.6490, adv_loss: 0.5204\n",
      "Epoch 34, Batch 225, loss_ca: 1.5994, adv_loss: 0.5065\n",
      "Epoch 34, Batch 226, loss_ca: 1.6516, adv_loss: 0.5045\n",
      "Epoch 34, Batch 227, loss_ca: 1.6250, adv_loss: 0.5175\n",
      "Epoch 34, Batch 228, loss_ca: 1.6371, adv_loss: 0.5133\n",
      "Epoch 35, Batch 35, loss_ca: 2.0853, adv_loss: 0.6525\n",
      "Epoch 35, Batch 36, loss_ca: 1.9538, adv_loss: 0.6225\n",
      "Epoch 35, Batch 37, loss_ca: 1.9625, adv_loss: 0.6096\n",
      "Epoch 35, Batch 38, loss_ca: 1.9206, adv_loss: 0.5915\n",
      "Epoch 35, Batch 39, loss_ca: 1.9176, adv_loss: 0.5894\n",
      "Epoch 35, Batch 40, loss_ca: 1.9065, adv_loss: 0.5864\n",
      "Epoch 35, Batch 41, loss_ca: 1.8858, adv_loss: 0.5066\n",
      "Epoch 35, Batch 42, loss_ca: 1.9022, adv_loss: 0.4851\n",
      "Epoch 35, Batch 43, loss_ca: 1.8774, adv_loss: 0.5011\n",
      "Epoch 35, Batch 44, loss_ca: 1.7980, adv_loss: 0.5084\n",
      "Epoch 35, Batch 45, loss_ca: 1.7395, adv_loss: 0.5296\n",
      "Epoch 35, Batch 46, loss_ca: 1.7587, adv_loss: 0.5094\n",
      "Epoch 35, Batch 47, loss_ca: 1.8313, adv_loss: 0.5134\n",
      "Epoch 35, Batch 48, loss_ca: 1.8379, adv_loss: 0.5382\n",
      "Epoch 35, Batch 49, loss_ca: 1.8012, adv_loss: 0.5703\n",
      "Epoch 35, Batch 50, loss_ca: 1.7529, adv_loss: 0.5514\n",
      "Epoch 35, Batch 51, loss_ca: 1.7363, adv_loss: 0.5702\n",
      "Epoch 35, Batch 52, loss_ca: 1.6892, adv_loss: 0.5139\n",
      "Epoch 35, Batch 53, loss_ca: 1.7238, adv_loss: 0.5754\n",
      "Epoch 35, Batch 54, loss_ca: 1.7218, adv_loss: 0.5351\n",
      "Epoch 35, Batch 55, loss_ca: 1.7248, adv_loss: 0.5331\n",
      "Epoch 35, Batch 56, loss_ca: 1.7579, adv_loss: 0.5373\n",
      "Epoch 35, Batch 57, loss_ca: 1.7531, adv_loss: 0.5289\n",
      "Epoch 35, Batch 58, loss_ca: 1.8625, adv_loss: 0.5164\n",
      "Epoch 35, Batch 59, loss_ca: 1.8660, adv_loss: 0.5013\n",
      "Epoch 35, Batch 60, loss_ca: 1.7797, adv_loss: 0.5041\n",
      "Epoch 35, Batch 61, loss_ca: 1.7599, adv_loss: 0.5046\n",
      "Epoch 35, Batch 62, loss_ca: 1.7955, adv_loss: 0.5226\n",
      "Epoch 35, Batch 63, loss_ca: 1.8156, adv_loss: 0.5570\n",
      "Epoch 35, Batch 64, loss_ca: 1.8064, adv_loss: 0.5668\n",
      "Epoch 35, Batch 65, loss_ca: 1.7394, adv_loss: 0.5485\n",
      "Epoch 35, Batch 66, loss_ca: 1.7789, adv_loss: 0.5771\n",
      "Epoch 35, Batch 67, loss_ca: 1.9075, adv_loss: 0.5557\n",
      "Epoch 35, Batch 68, loss_ca: 1.8311, adv_loss: 0.5450\n",
      "Epoch 35, Batch 69, loss_ca: 1.7447, adv_loss: 0.5183\n",
      "Epoch 35, Batch 70, loss_ca: 1.7064, adv_loss: 0.4953\n",
      "Epoch 35, Batch 71, loss_ca: 1.6567, adv_loss: 0.4972\n",
      "Epoch 35, Batch 72, loss_ca: 1.7103, adv_loss: 0.4950\n",
      "Epoch 35, Batch 73, loss_ca: 1.6645, adv_loss: 0.4702\n",
      "Epoch 35, Batch 74, loss_ca: 1.6883, adv_loss: 0.4612\n",
      "Epoch 35, Batch 75, loss_ca: 1.6416, adv_loss: 0.4692\n",
      "Epoch 35, Batch 76, loss_ca: 1.6194, adv_loss: 0.4733\n",
      "Epoch 35, Batch 77, loss_ca: 1.6091, adv_loss: 0.4725\n",
      "Epoch 35, Batch 78, loss_ca: 1.6450, adv_loss: 0.4697\n",
      "Epoch 35, Batch 79, loss_ca: 1.6363, adv_loss: 0.4797\n",
      "Epoch 35, Batch 80, loss_ca: 1.6257, adv_loss: 0.4908\n",
      "Epoch 35, Batch 81, loss_ca: 1.6197, adv_loss: 0.5007\n",
      "Epoch 35, Batch 82, loss_ca: 1.6995, adv_loss: 0.5128\n",
      "Epoch 35, Batch 83, loss_ca: 1.6969, adv_loss: 0.5184\n",
      "Epoch 35, Batch 84, loss_ca: 1.7337, adv_loss: 0.5276\n",
      "Epoch 35, Batch 85, loss_ca: 1.7250, adv_loss: 0.5154\n",
      "Epoch 35, Batch 86, loss_ca: 1.8244, adv_loss: 0.5000\n",
      "Epoch 35, Batch 87, loss_ca: 1.8564, adv_loss: 0.5182\n",
      "Epoch 35, Batch 88, loss_ca: 1.8438, adv_loss: 0.5193\n",
      "Epoch 35, Batch 89, loss_ca: 1.8797, adv_loss: 0.5762\n",
      "Epoch 35, Batch 90, loss_ca: 1.8618, adv_loss: 0.5493\n",
      "Epoch 35, Batch 91, loss_ca: 1.8419, adv_loss: 0.5270\n",
      "Epoch 35, Batch 92, loss_ca: 1.7743, adv_loss: 0.5147\n",
      "Epoch 35, Batch 93, loss_ca: 1.8564, adv_loss: 0.5198\n",
      "Epoch 35, Batch 94, loss_ca: 1.7796, adv_loss: 0.5167\n",
      "Epoch 35, Batch 95, loss_ca: 1.7879, adv_loss: 0.5065\n",
      "Epoch 35, Batch 96, loss_ca: 1.8401, adv_loss: 0.5332\n",
      "Epoch 35, Batch 97, loss_ca: 1.8210, adv_loss: 0.5393\n",
      "Epoch 35, Batch 98, loss_ca: 1.8097, adv_loss: 0.5373\n",
      "Epoch 35, Batch 99, loss_ca: 1.7001, adv_loss: 0.4901\n",
      "Epoch 35, Batch 100, loss_ca: 1.8114, adv_loss: 0.5053\n",
      "Epoch 35, Batch 101, loss_ca: 1.9460, adv_loss: 0.5110\n",
      "Epoch 35, Batch 102, loss_ca: 1.8935, adv_loss: 0.5189\n",
      "Epoch 35, Batch 103, loss_ca: 1.8819, adv_loss: 0.5259\n",
      "Epoch 35, Batch 104, loss_ca: 1.8963, adv_loss: 0.4960\n",
      "Epoch 35, Batch 105, loss_ca: 1.9381, adv_loss: 0.5502\n",
      "Epoch 35, Batch 106, loss_ca: 1.8933, adv_loss: 0.5684\n",
      "Epoch 35, Batch 107, loss_ca: 1.9778, adv_loss: 0.5835\n",
      "Epoch 35, Batch 108, loss_ca: 1.9100, adv_loss: 0.5752\n",
      "Epoch 35, Batch 109, loss_ca: 1.8860, adv_loss: 0.5485\n",
      "Epoch 35, Batch 110, loss_ca: 1.8423, adv_loss: 0.5524\n",
      "Epoch 35, Batch 111, loss_ca: 1.7989, adv_loss: 0.5633\n",
      "Epoch 35, Batch 112, loss_ca: 1.7725, adv_loss: 0.5597\n",
      "Epoch 35, Batch 113, loss_ca: 1.7408, adv_loss: 0.5480\n",
      "Epoch 35, Batch 114, loss_ca: 1.7178, adv_loss: 0.5386\n",
      "Epoch 35, Batch 115, loss_ca: 1.6742, adv_loss: 0.5255\n",
      "Epoch 35, Batch 116, loss_ca: 1.7193, adv_loss: 0.5331\n",
      "Epoch 35, Batch 117, loss_ca: 1.7447, adv_loss: 0.5122\n",
      "Epoch 35, Batch 118, loss_ca: 1.7585, adv_loss: 0.5198\n",
      "Epoch 35, Batch 119, loss_ca: 1.7192, adv_loss: 0.5091\n",
      "Epoch 35, Batch 120, loss_ca: 1.7249, adv_loss: 0.5039\n",
      "Epoch 35, Batch 121, loss_ca: 1.8954, adv_loss: 0.4936\n",
      "Epoch 35, Batch 122, loss_ca: 1.9197, adv_loss: 0.5459\n",
      "Epoch 35, Batch 123, loss_ca: 1.8971, adv_loss: 0.5479\n",
      "Epoch 35, Batch 124, loss_ca: 1.8432, adv_loss: 0.5401\n",
      "Epoch 35, Batch 125, loss_ca: 1.8248, adv_loss: 0.5441\n",
      "Epoch 35, Batch 126, loss_ca: 1.8573, adv_loss: 0.5256\n",
      "Epoch 35, Batch 127, loss_ca: 1.8683, adv_loss: 0.5373\n",
      "Epoch 35, Batch 128, loss_ca: 1.9163, adv_loss: 0.5604\n",
      "Epoch 35, Batch 129, loss_ca: 1.9086, adv_loss: 0.5571\n",
      "Epoch 35, Batch 130, loss_ca: 1.8230, adv_loss: 0.5680\n",
      "Epoch 35, Batch 131, loss_ca: 1.8563, adv_loss: 0.5386\n",
      "Epoch 35, Batch 132, loss_ca: 1.9163, adv_loss: 0.5923\n",
      "Epoch 35, Batch 133, loss_ca: 1.9699, adv_loss: 0.6126\n",
      "Epoch 35, Batch 134, loss_ca: 1.9179, adv_loss: 0.6189\n",
      "Epoch 35, Batch 135, loss_ca: 1.8834, adv_loss: 0.6146\n",
      "Epoch 35, Batch 136, loss_ca: 1.8110, adv_loss: 0.4835\n",
      "Epoch 35, Batch 137, loss_ca: 1.7058, adv_loss: 0.5041\n",
      "Epoch 35, Batch 138, loss_ca: 1.6390, adv_loss: 0.4962\n",
      "Epoch 35, Batch 139, loss_ca: 1.6095, adv_loss: 0.4973\n",
      "Epoch 35, Batch 140, loss_ca: 1.6466, adv_loss: 0.5275\n",
      "Epoch 35, Batch 141, loss_ca: 1.6578, adv_loss: 0.5148\n",
      "Epoch 35, Batch 142, loss_ca: 1.7376, adv_loss: 0.5294\n",
      "Epoch 35, Batch 143, loss_ca: 1.6872, adv_loss: 0.5229\n",
      "Epoch 35, Batch 144, loss_ca: 1.6868, adv_loss: 0.5417\n",
      "Epoch 35, Batch 145, loss_ca: 1.6846, adv_loss: 0.5355\n",
      "Epoch 35, Batch 146, loss_ca: 1.7349, adv_loss: 0.5503\n",
      "Epoch 35, Batch 147, loss_ca: 1.8321, adv_loss: 0.5600\n",
      "Epoch 35, Batch 148, loss_ca: 1.8648, adv_loss: 0.5576\n",
      "Epoch 35, Batch 149, loss_ca: 1.7912, adv_loss: 0.5011\n",
      "Epoch 35, Batch 150, loss_ca: 1.7219, adv_loss: 0.4956\n",
      "Epoch 35, Batch 151, loss_ca: 1.6532, adv_loss: 0.4905\n",
      "Epoch 35, Batch 152, loss_ca: 1.6496, adv_loss: 0.5016\n",
      "Epoch 35, Batch 153, loss_ca: 1.7677, adv_loss: 0.5349\n",
      "Epoch 35, Batch 154, loss_ca: 1.8417, adv_loss: 0.5504\n",
      "Epoch 35, Batch 155, loss_ca: 1.8219, adv_loss: 0.5623\n",
      "Epoch 35, Batch 156, loss_ca: 1.8439, adv_loss: 0.5681\n",
      "Epoch 35, Batch 157, loss_ca: 1.8166, adv_loss: 0.5423\n",
      "Epoch 35, Batch 158, loss_ca: 1.8951, adv_loss: 0.5233\n",
      "Epoch 35, Batch 159, loss_ca: 2.1074, adv_loss: 0.4834\n",
      "Epoch 35, Batch 160, loss_ca: 2.1959, adv_loss: 0.5434\n",
      "Epoch 35, Batch 161, loss_ca: 1.9586, adv_loss: 0.5248\n",
      "Epoch 35, Batch 162, loss_ca: 2.2069, adv_loss: 0.5695\n",
      "Epoch 35, Batch 163, loss_ca: 2.0422, adv_loss: 0.5417\n",
      "Epoch 35, Batch 164, loss_ca: 2.0375, adv_loss: 0.5210\n",
      "Epoch 35, Batch 165, loss_ca: 2.1935, adv_loss: 0.5181\n",
      "Epoch 35, Batch 166, loss_ca: 1.9981, adv_loss: 0.5463\n",
      "Epoch 35, Batch 167, loss_ca: 1.9034, adv_loss: 0.5252\n",
      "Epoch 35, Batch 168, loss_ca: 2.0910, adv_loss: 0.5461\n",
      "Epoch 35, Batch 169, loss_ca: 2.0427, adv_loss: 0.5599\n",
      "Epoch 35, Batch 170, loss_ca: 2.0824, adv_loss: 0.5543\n",
      "Epoch 35, Batch 171, loss_ca: 2.1737, adv_loss: 0.5633\n",
      "Epoch 35, Batch 172, loss_ca: 2.1810, adv_loss: 0.5652\n",
      "Epoch 35, Batch 173, loss_ca: 1.9326, adv_loss: 0.5454\n",
      "Epoch 35, Batch 174, loss_ca: 1.8924, adv_loss: 0.4694\n",
      "Epoch 35, Batch 175, loss_ca: 1.8054, adv_loss: 0.4739\n",
      "Epoch 35, Batch 176, loss_ca: 1.7849, adv_loss: 0.4777\n",
      "Epoch 35, Batch 177, loss_ca: 1.7300, adv_loss: 0.4826\n",
      "Epoch 35, Batch 178, loss_ca: 1.8399, adv_loss: 0.5111\n",
      "Epoch 35, Batch 179, loss_ca: 1.8060, adv_loss: 0.5059\n",
      "Epoch 35, Batch 180, loss_ca: 1.6967, adv_loss: 0.5044\n",
      "Epoch 35, Batch 181, loss_ca: 1.7063, adv_loss: 0.5074\n",
      "Epoch 35, Batch 182, loss_ca: 1.6711, adv_loss: 0.5041\n",
      "Epoch 35, Batch 183, loss_ca: 1.8213, adv_loss: 0.5305\n",
      "Epoch 35, Batch 184, loss_ca: 1.7545, adv_loss: 0.4928\n",
      "Epoch 35, Batch 185, loss_ca: 1.6998, adv_loss: 0.4747\n",
      "Epoch 35, Batch 186, loss_ca: 1.6437, adv_loss: 0.4664\n",
      "Epoch 35, Batch 187, loss_ca: 1.6892, adv_loss: 0.4674\n",
      "Epoch 35, Batch 188, loss_ca: 1.6505, adv_loss: 0.4691\n",
      "Epoch 35, Batch 189, loss_ca: 1.7664, adv_loss: 0.4941\n",
      "Epoch 35, Batch 190, loss_ca: 1.8695, adv_loss: 0.5723\n",
      "Epoch 35, Batch 191, loss_ca: 1.8346, adv_loss: 0.5605\n",
      "Epoch 35, Batch 192, loss_ca: 1.9967, adv_loss: 0.5550\n",
      "Epoch 35, Batch 193, loss_ca: 1.8819, adv_loss: 0.5759\n",
      "Epoch 35, Batch 194, loss_ca: 1.7478, adv_loss: 0.5868\n",
      "Epoch 35, Batch 195, loss_ca: 1.8763, adv_loss: 0.5474\n",
      "Epoch 35, Batch 196, loss_ca: 2.4539, adv_loss: 0.5915\n",
      "Epoch 35, Batch 197, loss_ca: 1.8876, adv_loss: 0.5311\n",
      "Epoch 35, Batch 198, loss_ca: 1.6779, adv_loss: 0.5292\n",
      "Epoch 35, Batch 199, loss_ca: 1.6687, adv_loss: 0.4809\n",
      "Epoch 35, Batch 200, loss_ca: 1.6078, adv_loss: 0.5402\n",
      "Epoch 35, Batch 201, loss_ca: 1.9870, adv_loss: 0.5627\n",
      "Epoch 35, Batch 202, loss_ca: 2.0865, adv_loss: 0.5807\n",
      "Epoch 35, Batch 203, loss_ca: 2.3018, adv_loss: 0.6002\n",
      "Epoch 35, Batch 204, loss_ca: 1.9237, adv_loss: 0.5428\n",
      "Epoch 35, Batch 205, loss_ca: 2.0294, adv_loss: 0.5392\n",
      "Epoch 35, Batch 206, loss_ca: 1.8820, adv_loss: 0.5567\n",
      "Epoch 35, Batch 207, loss_ca: 2.0827, adv_loss: 0.5628\n",
      "Epoch 35, Batch 208, loss_ca: 1.9147, adv_loss: 0.5109\n",
      "Epoch 35, Batch 209, loss_ca: 1.7109, adv_loss: 0.4989\n",
      "Epoch 35, Batch 210, loss_ca: 1.7873, adv_loss: 0.5378\n",
      "Epoch 35, Batch 211, loss_ca: 1.9629, adv_loss: 0.6062\n",
      "Epoch 35, Batch 212, loss_ca: 1.7483, adv_loss: 0.6128\n",
      "Epoch 35, Batch 213, loss_ca: 1.9457, adv_loss: 0.6365\n",
      "Epoch 35, Batch 214, loss_ca: 1.6684, adv_loss: 0.5652\n",
      "Epoch 35, Batch 215, loss_ca: 1.5367, adv_loss: 0.6312\n",
      "Epoch 35, Batch 216, loss_ca: 1.7092, adv_loss: 0.6412\n",
      "Epoch 35, Batch 217, loss_ca: 1.6930, adv_loss: 0.6415\n",
      "Epoch 35, Batch 218, loss_ca: 1.6792, adv_loss: 0.6071\n",
      "Epoch 35, Batch 219, loss_ca: 2.0862, adv_loss: 0.5513\n",
      "Epoch 35, Batch 220, loss_ca: 1.8726, adv_loss: 0.5269\n",
      "Epoch 35, Batch 221, loss_ca: 1.8571, adv_loss: 0.4907\n",
      "Epoch 35, Batch 222, loss_ca: 1.7835, adv_loss: 0.4949\n",
      "Epoch 35, Batch 223, loss_ca: 1.7109, adv_loss: 0.5091\n",
      "Epoch 35, Batch 224, loss_ca: 1.6581, adv_loss: 0.5144\n",
      "Epoch 35, Batch 225, loss_ca: 1.6573, adv_loss: 0.5148\n",
      "Epoch 35, Batch 226, loss_ca: 1.6850, adv_loss: 0.5062\n",
      "Epoch 35, Batch 227, loss_ca: 1.6609, adv_loss: 0.5240\n",
      "Epoch 35, Batch 228, loss_ca: 1.7046, adv_loss: 0.5296\n",
      "Epoch 36, Batch 36, loss_ca: 1.9955, adv_loss: 0.6161\n",
      "Epoch 36, Batch 37, loss_ca: 1.9695, adv_loss: 0.6016\n",
      "Epoch 36, Batch 38, loss_ca: 1.9806, adv_loss: 0.6192\n",
      "Epoch 36, Batch 39, loss_ca: 1.9285, adv_loss: 0.6050\n",
      "Epoch 36, Batch 40, loss_ca: 1.8947, adv_loss: 0.5839\n",
      "Epoch 36, Batch 41, loss_ca: 1.7816, adv_loss: 0.5373\n",
      "Epoch 36, Batch 42, loss_ca: 1.7789, adv_loss: 0.5206\n",
      "Epoch 36, Batch 43, loss_ca: 1.7801, adv_loss: 0.5097\n",
      "Epoch 36, Batch 44, loss_ca: 1.7573, adv_loss: 0.5090\n",
      "Epoch 36, Batch 45, loss_ca: 1.7330, adv_loss: 0.5203\n",
      "Epoch 36, Batch 46, loss_ca: 1.7223, adv_loss: 0.5267\n",
      "Epoch 36, Batch 47, loss_ca: 1.7448, adv_loss: 0.5357\n",
      "Epoch 36, Batch 48, loss_ca: 1.7267, adv_loss: 0.5806\n",
      "Epoch 36, Batch 49, loss_ca: 1.7307, adv_loss: 0.5802\n",
      "Epoch 36, Batch 50, loss_ca: 1.7036, adv_loss: 0.5684\n",
      "Epoch 36, Batch 51, loss_ca: 1.7453, adv_loss: 0.5700\n",
      "Epoch 36, Batch 52, loss_ca: 1.7455, adv_loss: 0.5274\n",
      "Epoch 36, Batch 53, loss_ca: 1.7350, adv_loss: 0.5666\n",
      "Epoch 36, Batch 54, loss_ca: 1.7234, adv_loss: 0.5598\n",
      "Epoch 36, Batch 55, loss_ca: 1.7216, adv_loss: 0.5594\n",
      "Epoch 36, Batch 56, loss_ca: 1.6923, adv_loss: 0.5561\n",
      "Epoch 36, Batch 57, loss_ca: 1.6725, adv_loss: 0.5504\n",
      "Epoch 36, Batch 58, loss_ca: 1.7727, adv_loss: 0.5393\n",
      "Epoch 36, Batch 59, loss_ca: 1.7674, adv_loss: 0.4997\n",
      "Epoch 36, Batch 60, loss_ca: 1.7837, adv_loss: 0.4998\n",
      "Epoch 36, Batch 61, loss_ca: 1.7395, adv_loss: 0.4889\n",
      "Epoch 36, Batch 62, loss_ca: 1.7352, adv_loss: 0.4937\n",
      "Epoch 36, Batch 63, loss_ca: 1.7846, adv_loss: 0.5124\n",
      "Epoch 36, Batch 64, loss_ca: 1.8078, adv_loss: 0.5142\n",
      "Epoch 36, Batch 65, loss_ca: 1.7091, adv_loss: 0.5247\n",
      "Epoch 36, Batch 66, loss_ca: 1.7446, adv_loss: 0.5630\n",
      "Epoch 36, Batch 67, loss_ca: 1.7248, adv_loss: 0.5616\n",
      "Epoch 36, Batch 68, loss_ca: 1.8002, adv_loss: 0.5616\n",
      "Epoch 36, Batch 69, loss_ca: 1.7789, adv_loss: 0.5482\n",
      "Epoch 36, Batch 70, loss_ca: 1.8625, adv_loss: 0.5762\n",
      "Epoch 36, Batch 71, loss_ca: 1.7880, adv_loss: 0.5389\n",
      "Epoch 36, Batch 72, loss_ca: 1.7977, adv_loss: 0.5172\n",
      "Epoch 36, Batch 73, loss_ca: 1.7593, adv_loss: 0.5054\n",
      "Epoch 36, Batch 74, loss_ca: 1.7260, adv_loss: 0.5017\n",
      "Epoch 36, Batch 75, loss_ca: 1.6810, adv_loss: 0.4955\n",
      "Epoch 36, Batch 76, loss_ca: 1.6512, adv_loss: 0.4933\n",
      "Epoch 36, Batch 77, loss_ca: 1.6543, adv_loss: 0.4896\n",
      "Epoch 36, Batch 78, loss_ca: 1.6811, adv_loss: 0.4751\n",
      "Epoch 36, Batch 79, loss_ca: 1.6593, adv_loss: 0.4672\n",
      "Epoch 36, Batch 80, loss_ca: 1.6462, adv_loss: 0.4690\n",
      "Epoch 36, Batch 81, loss_ca: 1.6297, adv_loss: 0.4639\n",
      "Epoch 36, Batch 82, loss_ca: 1.6336, adv_loss: 0.4674\n",
      "Epoch 36, Batch 83, loss_ca: 1.6661, adv_loss: 0.4620\n",
      "Epoch 36, Batch 84, loss_ca: 1.6585, adv_loss: 0.4681\n",
      "Epoch 36, Batch 85, loss_ca: 1.6939, adv_loss: 0.4868\n",
      "Epoch 36, Batch 86, loss_ca: 1.7955, adv_loss: 0.4966\n",
      "Epoch 36, Batch 87, loss_ca: 1.8373, adv_loss: 0.5009\n",
      "Epoch 36, Batch 88, loss_ca: 1.7913, adv_loss: 0.5040\n",
      "Epoch 36, Batch 89, loss_ca: 1.8221, adv_loss: 0.5135\n",
      "Epoch 36, Batch 90, loss_ca: 1.7632, adv_loss: 0.4946\n",
      "Epoch 36, Batch 91, loss_ca: 1.9005, adv_loss: 0.5149\n",
      "Epoch 36, Batch 92, loss_ca: 1.8136, adv_loss: 0.5243\n",
      "Epoch 36, Batch 93, loss_ca: 1.9004, adv_loss: 0.5572\n",
      "Epoch 36, Batch 94, loss_ca: 1.7988, adv_loss: 0.5288\n",
      "Epoch 36, Batch 95, loss_ca: 1.7946, adv_loss: 0.4879\n",
      "Epoch 36, Batch 96, loss_ca: 1.8403, adv_loss: 0.5137\n",
      "Epoch 36, Batch 97, loss_ca: 1.7772, adv_loss: 0.5225\n",
      "Epoch 36, Batch 98, loss_ca: 1.7417, adv_loss: 0.5076\n",
      "Epoch 36, Batch 99, loss_ca: 1.6718, adv_loss: 0.4839\n",
      "Epoch 36, Batch 100, loss_ca: 1.7770, adv_loss: 0.4898\n",
      "Epoch 36, Batch 101, loss_ca: 1.8912, adv_loss: 0.5189\n",
      "Epoch 36, Batch 102, loss_ca: 1.8321, adv_loss: 0.5178\n",
      "Epoch 36, Batch 103, loss_ca: 1.8324, adv_loss: 0.5280\n",
      "Epoch 36, Batch 104, loss_ca: 1.8302, adv_loss: 0.5365\n",
      "Epoch 36, Batch 105, loss_ca: 1.8948, adv_loss: 0.6099\n",
      "Epoch 36, Batch 106, loss_ca: 1.8867, adv_loss: 0.6016\n",
      "Epoch 36, Batch 107, loss_ca: 2.0080, adv_loss: 0.6285\n",
      "Epoch 36, Batch 108, loss_ca: 1.8947, adv_loss: 0.5456\n",
      "Epoch 36, Batch 109, loss_ca: 1.9077, adv_loss: 0.4899\n",
      "Epoch 36, Batch 110, loss_ca: 1.7989, adv_loss: 0.4813\n",
      "Epoch 36, Batch 111, loss_ca: 1.7211, adv_loss: 0.4846\n",
      "Epoch 36, Batch 112, loss_ca: 1.7502, adv_loss: 0.4830\n",
      "Epoch 36, Batch 113, loss_ca: 1.7047, adv_loss: 0.4858\n",
      "Epoch 36, Batch 114, loss_ca: 1.7185, adv_loss: 0.4918\n",
      "Epoch 36, Batch 115, loss_ca: 1.6920, adv_loss: 0.5056\n",
      "Epoch 36, Batch 116, loss_ca: 1.8239, adv_loss: 0.5343\n",
      "Epoch 36, Batch 117, loss_ca: 1.7390, adv_loss: 0.5146\n",
      "Epoch 36, Batch 118, loss_ca: 1.7292, adv_loss: 0.5101\n",
      "Epoch 36, Batch 119, loss_ca: 1.7356, adv_loss: 0.5096\n",
      "Epoch 36, Batch 120, loss_ca: 1.7422, adv_loss: 0.5157\n",
      "Epoch 36, Batch 121, loss_ca: 1.9910, adv_loss: 0.5343\n",
      "Epoch 36, Batch 122, loss_ca: 1.9715, adv_loss: 0.5640\n",
      "Epoch 36, Batch 123, loss_ca: 1.9510, adv_loss: 0.5639\n",
      "Epoch 36, Batch 124, loss_ca: 1.8637, adv_loss: 0.5736\n",
      "Epoch 36, Batch 125, loss_ca: 1.8343, adv_loss: 0.5743\n",
      "Epoch 36, Batch 126, loss_ca: 1.8519, adv_loss: 0.5554\n",
      "Epoch 36, Batch 127, loss_ca: 1.8759, adv_loss: 0.5902\n",
      "Epoch 36, Batch 128, loss_ca: 1.8973, adv_loss: 0.6040\n",
      "Epoch 36, Batch 129, loss_ca: 1.9164, adv_loss: 0.6144\n",
      "Epoch 36, Batch 130, loss_ca: 1.8938, adv_loss: 0.6165\n",
      "Epoch 36, Batch 131, loss_ca: 1.9300, adv_loss: 0.5854\n",
      "Epoch 36, Batch 132, loss_ca: 1.8039, adv_loss: 0.5726\n",
      "Epoch 36, Batch 133, loss_ca: 1.8201, adv_loss: 0.5667\n",
      "Epoch 36, Batch 134, loss_ca: 1.8556, adv_loss: 0.5508\n",
      "Epoch 36, Batch 135, loss_ca: 1.8428, adv_loss: 0.5623\n",
      "Epoch 36, Batch 136, loss_ca: 1.7884, adv_loss: 0.5339\n",
      "Epoch 36, Batch 137, loss_ca: 1.7284, adv_loss: 0.4752\n",
      "Epoch 36, Batch 138, loss_ca: 1.6926, adv_loss: 0.4819\n",
      "Epoch 36, Batch 139, loss_ca: 1.6962, adv_loss: 0.4817\n",
      "Epoch 36, Batch 140, loss_ca: 1.6721, adv_loss: 0.4964\n",
      "Epoch 36, Batch 141, loss_ca: 1.6795, adv_loss: 0.5007\n",
      "Epoch 36, Batch 142, loss_ca: 1.8184, adv_loss: 0.5347\n",
      "Epoch 36, Batch 143, loss_ca: 1.7133, adv_loss: 0.5527\n",
      "Epoch 36, Batch 144, loss_ca: 1.7067, adv_loss: 0.5610\n",
      "Epoch 36, Batch 145, loss_ca: 1.6888, adv_loss: 0.5565\n",
      "Epoch 36, Batch 146, loss_ca: 1.7201, adv_loss: 0.5336\n",
      "Epoch 36, Batch 147, loss_ca: 1.7730, adv_loss: 0.5372\n",
      "Epoch 36, Batch 148, loss_ca: 1.8431, adv_loss: 0.5442\n",
      "Epoch 36, Batch 149, loss_ca: 1.7051, adv_loss: 0.4996\n",
      "Epoch 36, Batch 150, loss_ca: 1.6493, adv_loss: 0.4890\n",
      "Epoch 36, Batch 151, loss_ca: 1.5810, adv_loss: 0.4849\n",
      "Epoch 36, Batch 152, loss_ca: 1.5825, adv_loss: 0.4874\n",
      "Epoch 36, Batch 153, loss_ca: 1.7552, adv_loss: 0.4959\n",
      "Epoch 36, Batch 154, loss_ca: 1.8354, adv_loss: 0.4978\n",
      "Epoch 36, Batch 155, loss_ca: 1.8155, adv_loss: 0.5196\n",
      "Epoch 36, Batch 156, loss_ca: 1.7708, adv_loss: 0.5505\n",
      "Epoch 36, Batch 157, loss_ca: 1.8257, adv_loss: 0.5072\n",
      "Epoch 36, Batch 158, loss_ca: 2.0167, adv_loss: 0.5666\n",
      "Epoch 36, Batch 159, loss_ca: 2.2444, adv_loss: 0.5765\n",
      "Epoch 36, Batch 160, loss_ca: 2.1020, adv_loss: 0.5645\n",
      "Epoch 36, Batch 161, loss_ca: 1.9229, adv_loss: 0.5068\n",
      "Epoch 36, Batch 162, loss_ca: 2.2084, adv_loss: 0.5420\n",
      "Epoch 36, Batch 163, loss_ca: 2.0217, adv_loss: 0.5130\n",
      "Epoch 36, Batch 164, loss_ca: 2.4376, adv_loss: 0.5654\n",
      "Epoch 36, Batch 165, loss_ca: 2.3562, adv_loss: 0.5374\n",
      "Epoch 36, Batch 166, loss_ca: 2.2237, adv_loss: 0.5865\n",
      "Epoch 36, Batch 167, loss_ca: 1.9748, adv_loss: 0.5357\n",
      "Epoch 36, Batch 168, loss_ca: 2.0142, adv_loss: 0.5496\n",
      "Epoch 36, Batch 169, loss_ca: 2.0139, adv_loss: 0.5626\n",
      "Epoch 36, Batch 170, loss_ca: 1.9726, adv_loss: 0.5420\n",
      "Epoch 36, Batch 171, loss_ca: 2.0740, adv_loss: 0.5618\n",
      "Epoch 36, Batch 172, loss_ca: 2.2182, adv_loss: 0.5893\n",
      "Epoch 36, Batch 173, loss_ca: 1.9164, adv_loss: 0.5534\n",
      "Epoch 36, Batch 174, loss_ca: 1.8318, adv_loss: 0.5341\n",
      "Epoch 36, Batch 175, loss_ca: 1.8426, adv_loss: 0.5291\n",
      "Epoch 36, Batch 176, loss_ca: 1.7789, adv_loss: 0.5265\n",
      "Epoch 36, Batch 177, loss_ca: 1.7474, adv_loss: 0.5053\n",
      "Epoch 36, Batch 178, loss_ca: 1.8685, adv_loss: 0.5089\n",
      "Epoch 36, Batch 179, loss_ca: 1.8464, adv_loss: 0.4836\n",
      "Epoch 36, Batch 180, loss_ca: 1.7709, adv_loss: 0.4799\n",
      "Epoch 36, Batch 181, loss_ca: 1.7150, adv_loss: 0.4746\n",
      "Epoch 36, Batch 182, loss_ca: 1.6677, adv_loss: 0.4815\n",
      "Epoch 36, Batch 183, loss_ca: 1.8144, adv_loss: 0.5071\n",
      "Epoch 36, Batch 184, loss_ca: 1.7663, adv_loss: 0.4950\n",
      "Epoch 36, Batch 185, loss_ca: 1.6782, adv_loss: 0.4976\n",
      "Epoch 36, Batch 186, loss_ca: 1.6768, adv_loss: 0.4997\n",
      "Epoch 36, Batch 187, loss_ca: 1.7032, adv_loss: 0.5028\n",
      "Epoch 36, Batch 188, loss_ca: 1.6712, adv_loss: 0.5002\n",
      "Epoch 36, Batch 189, loss_ca: 1.8410, adv_loss: 0.5339\n",
      "Epoch 36, Batch 190, loss_ca: 2.0977, adv_loss: 0.5534\n",
      "Epoch 36, Batch 191, loss_ca: 2.2203, adv_loss: 0.5800\n",
      "Epoch 36, Batch 192, loss_ca: 2.1133, adv_loss: 0.5673\n",
      "Epoch 36, Batch 193, loss_ca: 1.8584, adv_loss: 0.5596\n",
      "Epoch 36, Batch 194, loss_ca: 1.6314, adv_loss: 0.5480\n",
      "Epoch 36, Batch 195, loss_ca: 1.7825, adv_loss: 0.5275\n",
      "Epoch 36, Batch 196, loss_ca: 2.2065, adv_loss: 0.5179\n",
      "Epoch 36, Batch 197, loss_ca: 1.8505, adv_loss: 0.4950\n",
      "Epoch 36, Batch 198, loss_ca: 1.8048, adv_loss: 0.5545\n",
      "Epoch 36, Batch 199, loss_ca: 1.8020, adv_loss: 0.5334\n",
      "Epoch 36, Batch 200, loss_ca: 1.7119, adv_loss: 0.5274\n",
      "Epoch 36, Batch 201, loss_ca: 1.8819, adv_loss: 0.5066\n",
      "Epoch 36, Batch 202, loss_ca: 1.9438, adv_loss: 0.5331\n",
      "Epoch 36, Batch 203, loss_ca: 2.2939, adv_loss: 0.5648\n",
      "Epoch 36, Batch 204, loss_ca: 1.8428, adv_loss: 0.5015\n",
      "Epoch 36, Batch 205, loss_ca: 1.9557, adv_loss: 0.5158\n",
      "Epoch 36, Batch 206, loss_ca: 1.9504, adv_loss: 0.5292\n",
      "Epoch 36, Batch 207, loss_ca: 2.3526, adv_loss: 0.6193\n",
      "Epoch 36, Batch 208, loss_ca: 2.0031, adv_loss: 0.5660\n",
      "Epoch 36, Batch 209, loss_ca: 1.7168, adv_loss: 0.5010\n",
      "Epoch 36, Batch 210, loss_ca: 1.7759, adv_loss: 0.5659\n",
      "Epoch 36, Batch 211, loss_ca: 1.7988, adv_loss: 0.5600\n",
      "Epoch 36, Batch 212, loss_ca: 1.5333, adv_loss: 0.5682\n",
      "Epoch 36, Batch 213, loss_ca: 2.0440, adv_loss: 0.6326\n",
      "Epoch 36, Batch 214, loss_ca: 2.1158, adv_loss: 0.6463\n",
      "Epoch 36, Batch 215, loss_ca: 1.6565, adv_loss: 0.6304\n",
      "Epoch 36, Batch 216, loss_ca: 1.6788, adv_loss: 0.6162\n",
      "Epoch 36, Batch 217, loss_ca: 1.6274, adv_loss: 0.5672\n",
      "Epoch 36, Batch 218, loss_ca: 1.5939, adv_loss: 0.5647\n",
      "Epoch 36, Batch 219, loss_ca: 1.9782, adv_loss: 0.5147\n",
      "Epoch 36, Batch 220, loss_ca: 1.8261, adv_loss: 0.4775\n",
      "Epoch 36, Batch 221, loss_ca: 1.7482, adv_loss: 0.4535\n",
      "Epoch 36, Batch 222, loss_ca: 1.7413, adv_loss: 0.4552\n",
      "Epoch 36, Batch 223, loss_ca: 1.7083, adv_loss: 0.4698\n",
      "Epoch 36, Batch 224, loss_ca: 1.6317, adv_loss: 0.4767\n",
      "Epoch 36, Batch 225, loss_ca: 1.6065, adv_loss: 0.4830\n",
      "Epoch 36, Batch 226, loss_ca: 1.6416, adv_loss: 0.4966\n",
      "Epoch 36, Batch 227, loss_ca: 1.6567, adv_loss: 0.5502\n",
      "Epoch 36, Batch 228, loss_ca: 1.6522, adv_loss: 0.5243\n",
      "Epoch 37, Batch 37, loss_ca: 2.0649, adv_loss: 0.5594\n",
      "Epoch 37, Batch 38, loss_ca: 2.0094, adv_loss: 0.5767\n",
      "Epoch 37, Batch 39, loss_ca: 1.9859, adv_loss: 0.6177\n",
      "Epoch 37, Batch 40, loss_ca: 1.9930, adv_loss: 0.6000\n",
      "Epoch 37, Batch 41, loss_ca: 1.7951, adv_loss: 0.5452\n",
      "Epoch 37, Batch 42, loss_ca: 1.7746, adv_loss: 0.5536\n",
      "Epoch 37, Batch 43, loss_ca: 1.7699, adv_loss: 0.5431\n",
      "Epoch 37, Batch 44, loss_ca: 1.7360, adv_loss: 0.5539\n",
      "Epoch 37, Batch 45, loss_ca: 1.7370, adv_loss: 0.5594\n",
      "Epoch 37, Batch 46, loss_ca: 1.7488, adv_loss: 0.5637\n",
      "Epoch 37, Batch 47, loss_ca: 1.8356, adv_loss: 0.5751\n",
      "Epoch 37, Batch 48, loss_ca: 1.8009, adv_loss: 0.6066\n",
      "Epoch 37, Batch 49, loss_ca: 1.7914, adv_loss: 0.5777\n",
      "Epoch 37, Batch 50, loss_ca: 1.7646, adv_loss: 0.5585\n",
      "Epoch 37, Batch 51, loss_ca: 1.7633, adv_loss: 0.5548\n",
      "Epoch 37, Batch 52, loss_ca: 1.7983, adv_loss: 0.5345\n",
      "Epoch 37, Batch 53, loss_ca: 1.7383, adv_loss: 0.5423\n",
      "Epoch 37, Batch 54, loss_ca: 1.6879, adv_loss: 0.5246\n",
      "Epoch 37, Batch 55, loss_ca: 1.6643, adv_loss: 0.5195\n",
      "Epoch 37, Batch 56, loss_ca: 1.6642, adv_loss: 0.5234\n",
      "Epoch 37, Batch 57, loss_ca: 1.6856, adv_loss: 0.5233\n",
      "Epoch 37, Batch 58, loss_ca: 1.7970, adv_loss: 0.5402\n",
      "Epoch 37, Batch 59, loss_ca: 1.8739, adv_loss: 0.5173\n",
      "Epoch 37, Batch 60, loss_ca: 1.7446, adv_loss: 0.5365\n",
      "Epoch 37, Batch 61, loss_ca: 1.7848, adv_loss: 0.5263\n",
      "Epoch 37, Batch 62, loss_ca: 1.7803, adv_loss: 0.5319\n",
      "Epoch 37, Batch 63, loss_ca: 1.7799, adv_loss: 0.5375\n",
      "Epoch 37, Batch 64, loss_ca: 1.7978, adv_loss: 0.5423\n",
      "Epoch 37, Batch 65, loss_ca: 1.6956, adv_loss: 0.5274\n",
      "Epoch 37, Batch 66, loss_ca: 1.6933, adv_loss: 0.5427\n",
      "Epoch 37, Batch 67, loss_ca: 1.7108, adv_loss: 0.5533\n",
      "Epoch 37, Batch 68, loss_ca: 1.7587, adv_loss: 0.5592\n",
      "Epoch 37, Batch 69, loss_ca: 1.7867, adv_loss: 0.5381\n",
      "Epoch 37, Batch 70, loss_ca: 1.7930, adv_loss: 0.5457\n",
      "Epoch 37, Batch 71, loss_ca: 1.7135, adv_loss: 0.5498\n",
      "Epoch 37, Batch 72, loss_ca: 1.7547, adv_loss: 0.5268\n",
      "Epoch 37, Batch 73, loss_ca: 1.6918, adv_loss: 0.5232\n",
      "Epoch 37, Batch 74, loss_ca: 1.6696, adv_loss: 0.5150\n",
      "Epoch 37, Batch 75, loss_ca: 1.6343, adv_loss: 0.5147\n",
      "Epoch 37, Batch 76, loss_ca: 1.6076, adv_loss: 0.5083\n",
      "Epoch 37, Batch 77, loss_ca: 1.5882, adv_loss: 0.5051\n",
      "Epoch 37, Batch 78, loss_ca: 1.6619, adv_loss: 0.4827\n",
      "Epoch 37, Batch 79, loss_ca: 1.6515, adv_loss: 0.4855\n",
      "Epoch 37, Batch 80, loss_ca: 1.6427, adv_loss: 0.4819\n",
      "Epoch 37, Batch 81, loss_ca: 1.6343, adv_loss: 0.4908\n",
      "Epoch 37, Batch 82, loss_ca: 1.6620, adv_loss: 0.4923\n",
      "Epoch 37, Batch 83, loss_ca: 1.6948, adv_loss: 0.4835\n",
      "Epoch 37, Batch 84, loss_ca: 1.6768, adv_loss: 0.4985\n",
      "Epoch 37, Batch 85, loss_ca: 1.6692, adv_loss: 0.4875\n",
      "Epoch 37, Batch 86, loss_ca: 1.7679, adv_loss: 0.4926\n",
      "Epoch 37, Batch 87, loss_ca: 1.7789, adv_loss: 0.5145\n",
      "Epoch 37, Batch 88, loss_ca: 1.7965, adv_loss: 0.5003\n",
      "Epoch 37, Batch 89, loss_ca: 1.8371, adv_loss: 0.5066\n",
      "Epoch 37, Batch 90, loss_ca: 1.7988, adv_loss: 0.4924\n",
      "Epoch 37, Batch 91, loss_ca: 1.8842, adv_loss: 0.5440\n",
      "Epoch 37, Batch 92, loss_ca: 1.8322, adv_loss: 0.5379\n",
      "Epoch 37, Batch 93, loss_ca: 1.8689, adv_loss: 0.5578\n",
      "Epoch 37, Batch 94, loss_ca: 1.7934, adv_loss: 0.5253\n",
      "Epoch 37, Batch 95, loss_ca: 1.7356, adv_loss: 0.4942\n",
      "Epoch 37, Batch 96, loss_ca: 1.7523, adv_loss: 0.5110\n",
      "Epoch 37, Batch 97, loss_ca: 1.7352, adv_loss: 0.5188\n",
      "Epoch 37, Batch 98, loss_ca: 1.7092, adv_loss: 0.5133\n",
      "Epoch 37, Batch 99, loss_ca: 1.6583, adv_loss: 0.4798\n",
      "Epoch 37, Batch 100, loss_ca: 1.8180, adv_loss: 0.4860\n",
      "Epoch 37, Batch 101, loss_ca: 1.8930, adv_loss: 0.5141\n",
      "Epoch 37, Batch 102, loss_ca: 1.8739, adv_loss: 0.5176\n",
      "Epoch 37, Batch 103, loss_ca: 1.8359, adv_loss: 0.5284\n",
      "Epoch 37, Batch 104, loss_ca: 1.8180, adv_loss: 0.5022\n",
      "Epoch 37, Batch 105, loss_ca: 1.9228, adv_loss: 0.6141\n",
      "Epoch 37, Batch 106, loss_ca: 1.9538, adv_loss: 0.6270\n",
      "Epoch 37, Batch 107, loss_ca: 1.9977, adv_loss: 0.6145\n",
      "Epoch 37, Batch 108, loss_ca: 1.9552, adv_loss: 0.5869\n",
      "Epoch 37, Batch 109, loss_ca: 1.8727, adv_loss: 0.5506\n",
      "Epoch 37, Batch 110, loss_ca: 1.8319, adv_loss: 0.5376\n",
      "Epoch 37, Batch 111, loss_ca: 1.7772, adv_loss: 0.5329\n",
      "Epoch 37, Batch 112, loss_ca: 1.7246, adv_loss: 0.5285\n",
      "Epoch 37, Batch 113, loss_ca: 1.7306, adv_loss: 0.4996\n",
      "Epoch 37, Batch 114, loss_ca: 1.7130, adv_loss: 0.4829\n",
      "Epoch 37, Batch 115, loss_ca: 1.6649, adv_loss: 0.4865\n",
      "Epoch 37, Batch 116, loss_ca: 1.7096, adv_loss: 0.4857\n",
      "Epoch 37, Batch 117, loss_ca: 1.7239, adv_loss: 0.5038\n",
      "Epoch 37, Batch 118, loss_ca: 1.7022, adv_loss: 0.4983\n",
      "Epoch 37, Batch 119, loss_ca: 1.6842, adv_loss: 0.4958\n",
      "Epoch 37, Batch 120, loss_ca: 1.7064, adv_loss: 0.4910\n",
      "Epoch 37, Batch 121, loss_ca: 1.9389, adv_loss: 0.4815\n",
      "Epoch 37, Batch 122, loss_ca: 1.9184, adv_loss: 0.5356\n",
      "Epoch 37, Batch 123, loss_ca: 1.9102, adv_loss: 0.5441\n",
      "Epoch 37, Batch 124, loss_ca: 1.8790, adv_loss: 0.5661\n",
      "Epoch 37, Batch 125, loss_ca: 1.8450, adv_loss: 0.5800\n",
      "Epoch 37, Batch 126, loss_ca: 1.8574, adv_loss: 0.5679\n",
      "Epoch 37, Batch 127, loss_ca: 1.8618, adv_loss: 0.5759\n",
      "Epoch 37, Batch 128, loss_ca: 1.9042, adv_loss: 0.5830\n",
      "Epoch 37, Batch 129, loss_ca: 1.8750, adv_loss: 0.5675\n",
      "Epoch 37, Batch 130, loss_ca: 1.8207, adv_loss: 0.5759\n",
      "Epoch 37, Batch 131, loss_ca: 1.8286, adv_loss: 0.5662\n",
      "Epoch 37, Batch 132, loss_ca: 1.8774, adv_loss: 0.5939\n",
      "Epoch 37, Batch 133, loss_ca: 1.9321, adv_loss: 0.6031\n",
      "Epoch 37, Batch 134, loss_ca: 1.9792, adv_loss: 0.6159\n",
      "Epoch 37, Batch 135, loss_ca: 1.9222, adv_loss: 0.6084\n",
      "Epoch 37, Batch 136, loss_ca: 1.7729, adv_loss: 0.5388\n",
      "Epoch 37, Batch 137, loss_ca: 1.7544, adv_loss: 0.5473\n",
      "Epoch 37, Batch 138, loss_ca: 1.7784, adv_loss: 0.5343\n",
      "Epoch 37, Batch 139, loss_ca: 1.7141, adv_loss: 0.5211\n",
      "Epoch 37, Batch 140, loss_ca: 1.7056, adv_loss: 0.5154\n",
      "Epoch 37, Batch 141, loss_ca: 1.7138, adv_loss: 0.4954\n",
      "Epoch 37, Batch 142, loss_ca: 1.8400, adv_loss: 0.5063\n",
      "Epoch 37, Batch 143, loss_ca: 1.7319, adv_loss: 0.5018\n",
      "Epoch 37, Batch 144, loss_ca: 1.6974, adv_loss: 0.5201\n",
      "Epoch 37, Batch 145, loss_ca: 1.6838, adv_loss: 0.5269\n",
      "Epoch 37, Batch 146, loss_ca: 1.7169, adv_loss: 0.5281\n",
      "Epoch 37, Batch 147, loss_ca: 1.7597, adv_loss: 0.5244\n",
      "Epoch 37, Batch 148, loss_ca: 1.6649, adv_loss: 0.5319\n",
      "Epoch 37, Batch 149, loss_ca: 1.6855, adv_loss: 0.5184\n",
      "Epoch 37, Batch 150, loss_ca: 1.6708, adv_loss: 0.5120\n",
      "Epoch 37, Batch 151, loss_ca: 1.6380, adv_loss: 0.5125\n",
      "Epoch 37, Batch 152, loss_ca: 1.6225, adv_loss: 0.5092\n",
      "Epoch 37, Batch 153, loss_ca: 1.7740, adv_loss: 0.5127\n",
      "Epoch 37, Batch 154, loss_ca: 1.7973, adv_loss: 0.5005\n",
      "Epoch 37, Batch 155, loss_ca: 1.7838, adv_loss: 0.5251\n",
      "Epoch 37, Batch 156, loss_ca: 1.7611, adv_loss: 0.5342\n",
      "Epoch 37, Batch 157, loss_ca: 1.7973, adv_loss: 0.5356\n",
      "Epoch 37, Batch 158, loss_ca: 1.9182, adv_loss: 0.5625\n",
      "Epoch 37, Batch 159, loss_ca: 2.3207, adv_loss: 0.5476\n",
      "Epoch 37, Batch 160, loss_ca: 2.0584, adv_loss: 0.5529\n",
      "Epoch 37, Batch 161, loss_ca: 1.8596, adv_loss: 0.5469\n",
      "Epoch 37, Batch 162, loss_ca: 2.1384, adv_loss: 0.4950\n",
      "Epoch 37, Batch 163, loss_ca: 2.0146, adv_loss: 0.5114\n",
      "Epoch 37, Batch 164, loss_ca: 2.2284, adv_loss: 0.5277\n",
      "Epoch 37, Batch 165, loss_ca: 2.3824, adv_loss: 0.5138\n",
      "Epoch 37, Batch 166, loss_ca: 2.0872, adv_loss: 0.5576\n",
      "Epoch 37, Batch 167, loss_ca: 2.0174, adv_loss: 0.5217\n",
      "Epoch 37, Batch 168, loss_ca: 2.1579, adv_loss: 0.5248\n",
      "Epoch 37, Batch 169, loss_ca: 2.1324, adv_loss: 0.5427\n",
      "Epoch 37, Batch 170, loss_ca: 2.1020, adv_loss: 0.5356\n",
      "Epoch 37, Batch 171, loss_ca: 2.0144, adv_loss: 0.5302\n",
      "Epoch 37, Batch 172, loss_ca: 2.0719, adv_loss: 0.5729\n",
      "Epoch 37, Batch 173, loss_ca: 1.8930, adv_loss: 0.5541\n",
      "Epoch 37, Batch 174, loss_ca: 1.8114, adv_loss: 0.5059\n",
      "Epoch 37, Batch 175, loss_ca: 1.7901, adv_loss: 0.5163\n",
      "Epoch 37, Batch 176, loss_ca: 1.8643, adv_loss: 0.5243\n",
      "Epoch 37, Batch 177, loss_ca: 1.6981, adv_loss: 0.5160\n",
      "Epoch 37, Batch 178, loss_ca: 1.8142, adv_loss: 0.5008\n",
      "Epoch 37, Batch 179, loss_ca: 1.8202, adv_loss: 0.4907\n",
      "Epoch 37, Batch 180, loss_ca: 1.6827, adv_loss: 0.4837\n",
      "Epoch 37, Batch 181, loss_ca: 1.6856, adv_loss: 0.4902\n",
      "Epoch 37, Batch 182, loss_ca: 1.6373, adv_loss: 0.4926\n",
      "Epoch 37, Batch 183, loss_ca: 1.7835, adv_loss: 0.4960\n",
      "Epoch 37, Batch 184, loss_ca: 1.7264, adv_loss: 0.4989\n",
      "Epoch 37, Batch 185, loss_ca: 1.6863, adv_loss: 0.5053\n",
      "Epoch 37, Batch 186, loss_ca: 1.6214, adv_loss: 0.4972\n",
      "Epoch 37, Batch 187, loss_ca: 1.6685, adv_loss: 0.5062\n",
      "Epoch 37, Batch 188, loss_ca: 1.6415, adv_loss: 0.4955\n",
      "Epoch 37, Batch 189, loss_ca: 1.7398, adv_loss: 0.4950\n",
      "Epoch 37, Batch 190, loss_ca: 1.8622, adv_loss: 0.5465\n",
      "Epoch 37, Batch 191, loss_ca: 1.9173, adv_loss: 0.5476\n",
      "Epoch 37, Batch 192, loss_ca: 2.2449, adv_loss: 0.6073\n",
      "Epoch 37, Batch 193, loss_ca: 2.0573, adv_loss: 0.5961\n",
      "Epoch 37, Batch 194, loss_ca: 1.7361, adv_loss: 0.6014\n",
      "Epoch 37, Batch 195, loss_ca: 1.8814, adv_loss: 0.5672\n",
      "Epoch 37, Batch 196, loss_ca: 2.1730, adv_loss: 0.5800\n",
      "Epoch 37, Batch 197, loss_ca: 1.8403, adv_loss: 0.4924\n",
      "Epoch 37, Batch 198, loss_ca: 1.7155, adv_loss: 0.4744\n",
      "Epoch 37, Batch 199, loss_ca: 1.6765, adv_loss: 0.4615\n",
      "Epoch 37, Batch 200, loss_ca: 1.8241, adv_loss: 0.4838\n",
      "Epoch 37, Batch 201, loss_ca: 1.9387, adv_loss: 0.4897\n",
      "Epoch 37, Batch 202, loss_ca: 1.9855, adv_loss: 0.5101\n",
      "Epoch 37, Batch 203, loss_ca: 2.2820, adv_loss: 0.5100\n",
      "Epoch 37, Batch 204, loss_ca: 1.8435, adv_loss: 0.4940\n",
      "Epoch 37, Batch 205, loss_ca: 2.0205, adv_loss: 0.5207\n",
      "Epoch 37, Batch 206, loss_ca: 1.9160, adv_loss: 0.5188\n",
      "Epoch 37, Batch 207, loss_ca: 2.0732, adv_loss: 0.5485\n",
      "Epoch 37, Batch 208, loss_ca: 1.9105, adv_loss: 0.5582\n",
      "Epoch 37, Batch 209, loss_ca: 1.8549, adv_loss: 0.5162\n",
      "Epoch 37, Batch 210, loss_ca: 1.8492, adv_loss: 0.5446\n",
      "Epoch 37, Batch 211, loss_ca: 1.8007, adv_loss: 0.6170\n",
      "Epoch 37, Batch 212, loss_ca: 1.5717, adv_loss: 0.6090\n",
      "Epoch 37, Batch 213, loss_ca: 1.8497, adv_loss: 0.5731\n",
      "Epoch 37, Batch 214, loss_ca: 1.6713, adv_loss: 0.5752\n",
      "Epoch 37, Batch 215, loss_ca: 1.5984, adv_loss: 0.5975\n",
      "Epoch 37, Batch 216, loss_ca: 1.6579, adv_loss: 0.6261\n",
      "Epoch 37, Batch 217, loss_ca: 1.6579, adv_loss: 0.6228\n",
      "Epoch 37, Batch 218, loss_ca: 1.7788, adv_loss: 0.6213\n",
      "Epoch 37, Batch 219, loss_ca: 2.2796, adv_loss: 0.6089\n",
      "Epoch 37, Batch 220, loss_ca: 1.9635, adv_loss: 0.5787\n",
      "Epoch 37, Batch 221, loss_ca: 1.8208, adv_loss: 0.5512\n",
      "Epoch 37, Batch 222, loss_ca: 1.7505, adv_loss: 0.5339\n",
      "Epoch 37, Batch 223, loss_ca: 1.7315, adv_loss: 0.5161\n",
      "Epoch 37, Batch 224, loss_ca: 1.7139, adv_loss: 0.4903\n",
      "Epoch 37, Batch 225, loss_ca: 1.6778, adv_loss: 0.4840\n",
      "Epoch 37, Batch 226, loss_ca: 1.6701, adv_loss: 0.4722\n",
      "Epoch 37, Batch 227, loss_ca: 1.6826, adv_loss: 0.4891\n",
      "Epoch 37, Batch 228, loss_ca: 1.6873, adv_loss: 0.5000\n",
      "Epoch 38, Batch 38, loss_ca: 2.1274, adv_loss: 0.5876\n",
      "Epoch 38, Batch 39, loss_ca: 2.0768, adv_loss: 0.6056\n",
      "Epoch 38, Batch 40, loss_ca: 2.0005, adv_loss: 0.5761\n",
      "Epoch 38, Batch 41, loss_ca: 1.7811, adv_loss: 0.5117\n",
      "Epoch 38, Batch 42, loss_ca: 1.8169, adv_loss: 0.5050\n",
      "Epoch 38, Batch 43, loss_ca: 1.8226, adv_loss: 0.5082\n",
      "Epoch 38, Batch 44, loss_ca: 1.8604, adv_loss: 0.5290\n",
      "Epoch 38, Batch 45, loss_ca: 1.8478, adv_loss: 0.5444\n",
      "Epoch 38, Batch 46, loss_ca: 1.7661, adv_loss: 0.5399\n",
      "Epoch 38, Batch 47, loss_ca: 1.7907, adv_loss: 0.5705\n",
      "Epoch 38, Batch 48, loss_ca: 1.7207, adv_loss: 0.5761\n",
      "Epoch 38, Batch 49, loss_ca: 1.7192, adv_loss: 0.5780\n",
      "Epoch 38, Batch 50, loss_ca: 1.6913, adv_loss: 0.5705\n",
      "Epoch 38, Batch 51, loss_ca: 1.7113, adv_loss: 0.5707\n",
      "Epoch 38, Batch 52, loss_ca: 1.7016, adv_loss: 0.5421\n",
      "Epoch 38, Batch 53, loss_ca: 1.7530, adv_loss: 0.5573\n",
      "Epoch 38, Batch 54, loss_ca: 1.7343, adv_loss: 0.5446\n",
      "Epoch 38, Batch 55, loss_ca: 1.7110, adv_loss: 0.5419\n",
      "Epoch 38, Batch 56, loss_ca: 1.7230, adv_loss: 0.5408\n",
      "Epoch 38, Batch 57, loss_ca: 1.6688, adv_loss: 0.5328\n",
      "Epoch 38, Batch 58, loss_ca: 1.7717, adv_loss: 0.5505\n",
      "Epoch 38, Batch 59, loss_ca: 1.9168, adv_loss: 0.5122\n",
      "Epoch 38, Batch 60, loss_ca: 1.8792, adv_loss: 0.5160\n",
      "Epoch 38, Batch 61, loss_ca: 1.6908, adv_loss: 0.5187\n",
      "Epoch 38, Batch 62, loss_ca: 1.6819, adv_loss: 0.5099\n",
      "Epoch 38, Batch 63, loss_ca: 1.7708, adv_loss: 0.5061\n",
      "Epoch 38, Batch 64, loss_ca: 1.7904, adv_loss: 0.5319\n",
      "Epoch 38, Batch 65, loss_ca: 1.7524, adv_loss: 0.5172\n",
      "Epoch 38, Batch 66, loss_ca: 1.7741, adv_loss: 0.5433\n",
      "Epoch 38, Batch 67, loss_ca: 1.8608, adv_loss: 0.5468\n",
      "Epoch 38, Batch 68, loss_ca: 1.7794, adv_loss: 0.5675\n",
      "Epoch 38, Batch 69, loss_ca: 1.8187, adv_loss: 0.5775\n",
      "Epoch 38, Batch 70, loss_ca: 1.8227, adv_loss: 0.5642\n",
      "Epoch 38, Batch 71, loss_ca: 1.7618, adv_loss: 0.5500\n",
      "Epoch 38, Batch 72, loss_ca: 1.7484, adv_loss: 0.5337\n",
      "Epoch 38, Batch 73, loss_ca: 1.6849, adv_loss: 0.5249\n",
      "Epoch 38, Batch 74, loss_ca: 1.6753, adv_loss: 0.5188\n",
      "Epoch 38, Batch 75, loss_ca: 1.6205, adv_loss: 0.5116\n",
      "Epoch 38, Batch 76, loss_ca: 1.6148, adv_loss: 0.5123\n",
      "Epoch 38, Batch 77, loss_ca: 1.6186, adv_loss: 0.5159\n",
      "Epoch 38, Batch 78, loss_ca: 1.7299, adv_loss: 0.5056\n",
      "Epoch 38, Batch 79, loss_ca: 1.7083, adv_loss: 0.5031\n",
      "Epoch 38, Batch 80, loss_ca: 1.6535, adv_loss: 0.5121\n",
      "Epoch 38, Batch 81, loss_ca: 1.6681, adv_loss: 0.5103\n",
      "Epoch 38, Batch 82, loss_ca: 1.6663, adv_loss: 0.5184\n",
      "Epoch 38, Batch 83, loss_ca: 1.6758, adv_loss: 0.5177\n",
      "Epoch 38, Batch 84, loss_ca: 1.6902, adv_loss: 0.5254\n",
      "Epoch 38, Batch 85, loss_ca: 1.7881, adv_loss: 0.5122\n",
      "Epoch 38, Batch 86, loss_ca: 1.7948, adv_loss: 0.4973\n",
      "Epoch 38, Batch 87, loss_ca: 1.8122, adv_loss: 0.5129\n",
      "Epoch 38, Batch 88, loss_ca: 1.7572, adv_loss: 0.5172\n",
      "Epoch 38, Batch 89, loss_ca: 1.8032, adv_loss: 0.5277\n",
      "Epoch 38, Batch 90, loss_ca: 1.7521, adv_loss: 0.5131\n",
      "Epoch 38, Batch 91, loss_ca: 1.8309, adv_loss: 0.5300\n",
      "Epoch 38, Batch 92, loss_ca: 1.7900, adv_loss: 0.5301\n",
      "Epoch 38, Batch 93, loss_ca: 1.8546, adv_loss: 0.5530\n",
      "Epoch 38, Batch 94, loss_ca: 1.8097, adv_loss: 0.5345\n",
      "Epoch 38, Batch 95, loss_ca: 1.7929, adv_loss: 0.5317\n",
      "Epoch 38, Batch 96, loss_ca: 1.8189, adv_loss: 0.5425\n",
      "Epoch 38, Batch 97, loss_ca: 1.8033, adv_loss: 0.5386\n",
      "Epoch 38, Batch 98, loss_ca: 1.7734, adv_loss: 0.5241\n",
      "Epoch 38, Batch 99, loss_ca: 1.7028, adv_loss: 0.5006\n",
      "Epoch 38, Batch 100, loss_ca: 1.8294, adv_loss: 0.4937\n",
      "Epoch 38, Batch 101, loss_ca: 1.8841, adv_loss: 0.5016\n",
      "Epoch 38, Batch 102, loss_ca: 1.8488, adv_loss: 0.5015\n",
      "Epoch 38, Batch 103, loss_ca: 1.8674, adv_loss: 0.5064\n",
      "Epoch 38, Batch 104, loss_ca: 1.8784, adv_loss: 0.4922\n",
      "Epoch 38, Batch 105, loss_ca: 1.9821, adv_loss: 0.5501\n",
      "Epoch 38, Batch 106, loss_ca: 1.9700, adv_loss: 0.5584\n",
      "Epoch 38, Batch 107, loss_ca: 1.9802, adv_loss: 0.5718\n",
      "Epoch 38, Batch 108, loss_ca: 1.9484, adv_loss: 0.5280\n",
      "Epoch 38, Batch 109, loss_ca: 1.8056, adv_loss: 0.5112\n",
      "Epoch 38, Batch 110, loss_ca: 1.7494, adv_loss: 0.5044\n",
      "Epoch 38, Batch 111, loss_ca: 1.7124, adv_loss: 0.5007\n",
      "Epoch 38, Batch 112, loss_ca: 1.8048, adv_loss: 0.4853\n",
      "Epoch 38, Batch 113, loss_ca: 1.7146, adv_loss: 0.4867\n",
      "Epoch 38, Batch 114, loss_ca: 1.7142, adv_loss: 0.4844\n",
      "Epoch 38, Batch 115, loss_ca: 1.6638, adv_loss: 0.4847\n",
      "Epoch 38, Batch 116, loss_ca: 1.6953, adv_loss: 0.4780\n",
      "Epoch 38, Batch 117, loss_ca: 1.7060, adv_loss: 0.4947\n",
      "Epoch 38, Batch 118, loss_ca: 1.7447, adv_loss: 0.5025\n",
      "Epoch 38, Batch 119, loss_ca: 1.7411, adv_loss: 0.5015\n",
      "Epoch 38, Batch 120, loss_ca: 1.8191, adv_loss: 0.5034\n",
      "Epoch 38, Batch 121, loss_ca: 1.8970, adv_loss: 0.5110\n",
      "Epoch 38, Batch 122, loss_ca: 1.9040, adv_loss: 0.5111\n",
      "Epoch 38, Batch 123, loss_ca: 1.8859, adv_loss: 0.5084\n",
      "Epoch 38, Batch 124, loss_ca: 1.8559, adv_loss: 0.5155\n",
      "Epoch 38, Batch 125, loss_ca: 1.9135, adv_loss: 0.4942\n",
      "Epoch 38, Batch 126, loss_ca: 1.9140, adv_loss: 0.5510\n",
      "Epoch 38, Batch 127, loss_ca: 1.8800, adv_loss: 0.5923\n",
      "Epoch 38, Batch 128, loss_ca: 1.9353, adv_loss: 0.6112\n",
      "Epoch 38, Batch 129, loss_ca: 1.9006, adv_loss: 0.6160\n",
      "Epoch 38, Batch 130, loss_ca: 1.8399, adv_loss: 0.6332\n",
      "Epoch 38, Batch 131, loss_ca: 1.8424, adv_loss: 0.5715\n",
      "Epoch 38, Batch 132, loss_ca: 1.8366, adv_loss: 0.5567\n",
      "Epoch 38, Batch 133, loss_ca: 1.8457, adv_loss: 0.5661\n",
      "Epoch 38, Batch 134, loss_ca: 1.8880, adv_loss: 0.5723\n",
      "Epoch 38, Batch 135, loss_ca: 1.8620, adv_loss: 0.5975\n",
      "Epoch 38, Batch 136, loss_ca: 1.8553, adv_loss: 0.5163\n",
      "Epoch 38, Batch 137, loss_ca: 1.7964, adv_loss: 0.5509\n",
      "Epoch 38, Batch 138, loss_ca: 1.7569, adv_loss: 0.5434\n",
      "Epoch 38, Batch 139, loss_ca: 1.7177, adv_loss: 0.5364\n",
      "Epoch 38, Batch 140, loss_ca: 1.6733, adv_loss: 0.5295\n",
      "Epoch 38, Batch 141, loss_ca: 1.6470, adv_loss: 0.5138\n",
      "Epoch 38, Batch 142, loss_ca: 1.7580, adv_loss: 0.5123\n",
      "Epoch 38, Batch 143, loss_ca: 1.7359, adv_loss: 0.4983\n",
      "Epoch 38, Batch 144, loss_ca: 1.7642, adv_loss: 0.5061\n",
      "Epoch 38, Batch 145, loss_ca: 1.7253, adv_loss: 0.5117\n",
      "Epoch 38, Batch 146, loss_ca: 1.6893, adv_loss: 0.5110\n",
      "Epoch 38, Batch 147, loss_ca: 1.7481, adv_loss: 0.5242\n",
      "Epoch 38, Batch 148, loss_ca: 1.8134, adv_loss: 0.5467\n",
      "Epoch 38, Batch 149, loss_ca: 1.6944, adv_loss: 0.5170\n",
      "Epoch 38, Batch 150, loss_ca: 1.6586, adv_loss: 0.5217\n",
      "Epoch 38, Batch 151, loss_ca: 1.6223, adv_loss: 0.5192\n",
      "Epoch 38, Batch 152, loss_ca: 1.6323, adv_loss: 0.5081\n",
      "Epoch 38, Batch 153, loss_ca: 1.8068, adv_loss: 0.5163\n",
      "Epoch 38, Batch 154, loss_ca: 1.8347, adv_loss: 0.4914\n",
      "Epoch 38, Batch 155, loss_ca: 1.8061, adv_loss: 0.5025\n",
      "Epoch 38, Batch 156, loss_ca: 1.7654, adv_loss: 0.5241\n",
      "Epoch 38, Batch 157, loss_ca: 1.7855, adv_loss: 0.5008\n",
      "Epoch 38, Batch 158, loss_ca: 1.8813, adv_loss: 0.4849\n",
      "Epoch 38, Batch 159, loss_ca: 1.9812, adv_loss: 0.5258\n",
      "Epoch 38, Batch 160, loss_ca: 1.9237, adv_loss: 0.4743\n",
      "Epoch 38, Batch 161, loss_ca: 1.7988, adv_loss: 0.5167\n",
      "Epoch 38, Batch 162, loss_ca: 2.0078, adv_loss: 0.5490\n",
      "Epoch 38, Batch 163, loss_ca: 2.0474, adv_loss: 0.5645\n",
      "Epoch 38, Batch 164, loss_ca: 2.4478, adv_loss: 0.5955\n",
      "Epoch 38, Batch 165, loss_ca: 2.0923, adv_loss: 0.5394\n",
      "Epoch 38, Batch 166, loss_ca: 1.9444, adv_loss: 0.5628\n",
      "Epoch 38, Batch 167, loss_ca: 1.9101, adv_loss: 0.5116\n",
      "Epoch 38, Batch 168, loss_ca: 2.0821, adv_loss: 0.5081\n",
      "Epoch 38, Batch 169, loss_ca: 2.0559, adv_loss: 0.5192\n",
      "Epoch 38, Batch 170, loss_ca: 2.0442, adv_loss: 0.5150\n",
      "Epoch 38, Batch 171, loss_ca: 2.0037, adv_loss: 0.5380\n",
      "Epoch 38, Batch 172, loss_ca: 2.0600, adv_loss: 0.5504\n",
      "Epoch 38, Batch 173, loss_ca: 1.9295, adv_loss: 0.5527\n",
      "Epoch 38, Batch 174, loss_ca: 1.9358, adv_loss: 0.5766\n",
      "Epoch 38, Batch 175, loss_ca: 1.7975, adv_loss: 0.5361\n",
      "Epoch 38, Batch 176, loss_ca: 1.7750, adv_loss: 0.5321\n",
      "Epoch 38, Batch 177, loss_ca: 1.6714, adv_loss: 0.5223\n",
      "Epoch 38, Batch 178, loss_ca: 1.8175, adv_loss: 0.5312\n",
      "Epoch 38, Batch 179, loss_ca: 1.7679, adv_loss: 0.5106\n",
      "Epoch 38, Batch 180, loss_ca: 1.6518, adv_loss: 0.4935\n",
      "Epoch 38, Batch 181, loss_ca: 1.6819, adv_loss: 0.5018\n",
      "Epoch 38, Batch 182, loss_ca: 1.6660, adv_loss: 0.4807\n",
      "Epoch 38, Batch 183, loss_ca: 1.7921, adv_loss: 0.4963\n",
      "Epoch 38, Batch 184, loss_ca: 1.7913, adv_loss: 0.4683\n",
      "Epoch 38, Batch 185, loss_ca: 1.7717, adv_loss: 0.4773\n",
      "Epoch 38, Batch 186, loss_ca: 1.6348, adv_loss: 0.4736\n",
      "Epoch 38, Batch 187, loss_ca: 1.7287, adv_loss: 0.5002\n",
      "Epoch 38, Batch 188, loss_ca: 1.6666, adv_loss: 0.4999\n",
      "Epoch 38, Batch 189, loss_ca: 1.7212, adv_loss: 0.4983\n",
      "Epoch 38, Batch 190, loss_ca: 1.9279, adv_loss: 0.4961\n",
      "Epoch 38, Batch 191, loss_ca: 1.8959, adv_loss: 0.4902\n",
      "Epoch 38, Batch 192, loss_ca: 2.0675, adv_loss: 0.5444\n",
      "Epoch 38, Batch 193, loss_ca: 1.9677, adv_loss: 0.5977\n",
      "Epoch 38, Batch 194, loss_ca: 1.6921, adv_loss: 0.5840\n",
      "Epoch 38, Batch 195, loss_ca: 1.9655, adv_loss: 0.5897\n",
      "Epoch 38, Batch 196, loss_ca: 2.4395, adv_loss: 0.6096\n",
      "Epoch 38, Batch 197, loss_ca: 1.9874, adv_loss: 0.5757\n",
      "Epoch 38, Batch 198, loss_ca: 1.9711, adv_loss: 0.5879\n",
      "Epoch 38, Batch 199, loss_ca: 1.8623, adv_loss: 0.5716\n",
      "Epoch 38, Batch 200, loss_ca: 1.6093, adv_loss: 0.5716\n",
      "Epoch 38, Batch 201, loss_ca: 1.8598, adv_loss: 0.5092\n",
      "Epoch 38, Batch 202, loss_ca: 1.8712, adv_loss: 0.5240\n",
      "Epoch 38, Batch 203, loss_ca: 2.1490, adv_loss: 0.5183\n",
      "Epoch 38, Batch 204, loss_ca: 1.8836, adv_loss: 0.5031\n",
      "Epoch 38, Batch 205, loss_ca: 2.1170, adv_loss: 0.4534\n",
      "Epoch 38, Batch 206, loss_ca: 1.9290, adv_loss: 0.5083\n",
      "Epoch 38, Batch 207, loss_ca: 2.0767, adv_loss: 0.5370\n",
      "Epoch 38, Batch 208, loss_ca: 2.0276, adv_loss: 0.5091\n",
      "Epoch 38, Batch 209, loss_ca: 2.0021, adv_loss: 0.4874\n",
      "Epoch 38, Batch 210, loss_ca: 1.9291, adv_loss: 0.5127\n",
      "Epoch 38, Batch 211, loss_ca: 2.0472, adv_loss: 0.6096\n",
      "Epoch 38, Batch 212, loss_ca: 2.0160, adv_loss: 0.5422\n",
      "Epoch 38, Batch 213, loss_ca: 1.9425, adv_loss: 0.5640\n",
      "Epoch 38, Batch 214, loss_ca: 1.7372, adv_loss: 0.4835\n",
      "Epoch 38, Batch 215, loss_ca: 1.5231, adv_loss: 0.5473\n",
      "Epoch 38, Batch 216, loss_ca: 1.6185, adv_loss: 0.6250\n",
      "Epoch 38, Batch 217, loss_ca: 1.5959, adv_loss: 0.6195\n",
      "Epoch 38, Batch 218, loss_ca: 1.6460, adv_loss: 0.6141\n",
      "Epoch 38, Batch 219, loss_ca: 2.0493, adv_loss: 0.5755\n",
      "Epoch 38, Batch 220, loss_ca: 1.8976, adv_loss: 0.5783\n",
      "Epoch 38, Batch 221, loss_ca: 1.7354, adv_loss: 0.5543\n",
      "Epoch 38, Batch 222, loss_ca: 1.7052, adv_loss: 0.5296\n",
      "Epoch 38, Batch 223, loss_ca: 1.6915, adv_loss: 0.5202\n",
      "Epoch 38, Batch 224, loss_ca: 1.6308, adv_loss: 0.5001\n",
      "Epoch 38, Batch 225, loss_ca: 1.6485, adv_loss: 0.4866\n",
      "Epoch 38, Batch 226, loss_ca: 1.7118, adv_loss: 0.4737\n",
      "Epoch 38, Batch 227, loss_ca: 1.6753, adv_loss: 0.4894\n",
      "Epoch 38, Batch 228, loss_ca: 1.6425, adv_loss: 0.4979\n",
      "Epoch 39, Batch 39, loss_ca: 2.0169, adv_loss: 0.6351\n",
      "Epoch 39, Batch 40, loss_ca: 1.9056, adv_loss: 0.6001\n",
      "Epoch 39, Batch 41, loss_ca: 1.7752, adv_loss: 0.5526\n",
      "Epoch 39, Batch 42, loss_ca: 1.7750, adv_loss: 0.5830\n",
      "Epoch 39, Batch 43, loss_ca: 1.7738, adv_loss: 0.5786\n",
      "Epoch 39, Batch 44, loss_ca: 1.8055, adv_loss: 0.5630\n",
      "Epoch 39, Batch 45, loss_ca: 1.7699, adv_loss: 0.5548\n",
      "Epoch 39, Batch 46, loss_ca: 1.7329, adv_loss: 0.5441\n",
      "Epoch 39, Batch 47, loss_ca: 1.8409, adv_loss: 0.5445\n",
      "Epoch 39, Batch 48, loss_ca: 1.7587, adv_loss: 0.5399\n",
      "Epoch 39, Batch 49, loss_ca: 1.7580, adv_loss: 0.5538\n",
      "Epoch 39, Batch 50, loss_ca: 1.7142, adv_loss: 0.5641\n",
      "Epoch 39, Batch 51, loss_ca: 1.7589, adv_loss: 0.5664\n",
      "Epoch 39, Batch 52, loss_ca: 1.7248, adv_loss: 0.5286\n",
      "Epoch 39, Batch 53, loss_ca: 1.7631, adv_loss: 0.5514\n",
      "Epoch 39, Batch 54, loss_ca: 1.7499, adv_loss: 0.5408\n",
      "Epoch 39, Batch 55, loss_ca: 1.7296, adv_loss: 0.5433\n",
      "Epoch 39, Batch 56, loss_ca: 1.7220, adv_loss: 0.5431\n",
      "Epoch 39, Batch 57, loss_ca: 1.7293, adv_loss: 0.5364\n",
      "Epoch 39, Batch 58, loss_ca: 1.8570, adv_loss: 0.5520\n",
      "Epoch 39, Batch 59, loss_ca: 1.7852, adv_loss: 0.5114\n",
      "Epoch 39, Batch 60, loss_ca: 1.7777, adv_loss: 0.5090\n",
      "Epoch 39, Batch 61, loss_ca: 1.7025, adv_loss: 0.5048\n",
      "Epoch 39, Batch 62, loss_ca: 1.6880, adv_loss: 0.5023\n",
      "Epoch 39, Batch 63, loss_ca: 1.8526, adv_loss: 0.4749\n",
      "Epoch 39, Batch 64, loss_ca: 1.8638, adv_loss: 0.5011\n",
      "Epoch 39, Batch 65, loss_ca: 1.7462, adv_loss: 0.4988\n",
      "Epoch 39, Batch 66, loss_ca: 1.7394, adv_loss: 0.5101\n",
      "Epoch 39, Batch 67, loss_ca: 1.8613, adv_loss: 0.5472\n",
      "Epoch 39, Batch 68, loss_ca: 1.6824, adv_loss: 0.5336\n",
      "Epoch 39, Batch 69, loss_ca: 1.7565, adv_loss: 0.5567\n",
      "Epoch 39, Batch 70, loss_ca: 1.7345, adv_loss: 0.5609\n",
      "Epoch 39, Batch 71, loss_ca: 1.7425, adv_loss: 0.5458\n",
      "Epoch 39, Batch 72, loss_ca: 1.7214, adv_loss: 0.5375\n",
      "Epoch 39, Batch 73, loss_ca: 1.6390, adv_loss: 0.5254\n",
      "Epoch 39, Batch 74, loss_ca: 1.6574, adv_loss: 0.5202\n",
      "Epoch 39, Batch 75, loss_ca: 1.6276, adv_loss: 0.5141\n",
      "Epoch 39, Batch 76, loss_ca: 1.6557, adv_loss: 0.5011\n",
      "Epoch 39, Batch 77, loss_ca: 1.6699, adv_loss: 0.5002\n",
      "Epoch 39, Batch 78, loss_ca: 1.7066, adv_loss: 0.4938\n",
      "Epoch 39, Batch 79, loss_ca: 1.6880, adv_loss: 0.4941\n",
      "Epoch 39, Batch 80, loss_ca: 1.6432, adv_loss: 0.4921\n",
      "Epoch 39, Batch 81, loss_ca: 1.6339, adv_loss: 0.4947\n",
      "Epoch 39, Batch 82, loss_ca: 1.6623, adv_loss: 0.5064\n",
      "Epoch 39, Batch 83, loss_ca: 1.7166, adv_loss: 0.5063\n",
      "Epoch 39, Batch 84, loss_ca: 1.6938, adv_loss: 0.5014\n",
      "Epoch 39, Batch 85, loss_ca: 1.6882, adv_loss: 0.4965\n",
      "Epoch 39, Batch 86, loss_ca: 1.7874, adv_loss: 0.5123\n",
      "Epoch 39, Batch 87, loss_ca: 1.8061, adv_loss: 0.5079\n",
      "Epoch 39, Batch 88, loss_ca: 1.7599, adv_loss: 0.5077\n",
      "Epoch 39, Batch 89, loss_ca: 1.7948, adv_loss: 0.5151\n",
      "Epoch 39, Batch 90, loss_ca: 1.7205, adv_loss: 0.5115\n",
      "Epoch 39, Batch 91, loss_ca: 1.7792, adv_loss: 0.5382\n",
      "Epoch 39, Batch 92, loss_ca: 1.7334, adv_loss: 0.5243\n",
      "Epoch 39, Batch 93, loss_ca: 1.8150, adv_loss: 0.5652\n",
      "Epoch 39, Batch 94, loss_ca: 1.7534, adv_loss: 0.5317\n",
      "Epoch 39, Batch 95, loss_ca: 1.7725, adv_loss: 0.5269\n",
      "Epoch 39, Batch 96, loss_ca: 1.7917, adv_loss: 0.5437\n",
      "Epoch 39, Batch 97, loss_ca: 1.7711, adv_loss: 0.5323\n",
      "Epoch 39, Batch 98, loss_ca: 1.7404, adv_loss: 0.5263\n",
      "Epoch 39, Batch 99, loss_ca: 1.6742, adv_loss: 0.5130\n",
      "Epoch 39, Batch 100, loss_ca: 1.7993, adv_loss: 0.5095\n",
      "Epoch 39, Batch 101, loss_ca: 1.8783, adv_loss: 0.5191\n",
      "Epoch 39, Batch 102, loss_ca: 1.8596, adv_loss: 0.5252\n",
      "Epoch 39, Batch 103, loss_ca: 1.8353, adv_loss: 0.5225\n",
      "Epoch 39, Batch 104, loss_ca: 1.8502, adv_loss: 0.5137\n",
      "Epoch 39, Batch 105, loss_ca: 1.9717, adv_loss: 0.6129\n",
      "Epoch 39, Batch 106, loss_ca: 1.9355, adv_loss: 0.6159\n",
      "Epoch 39, Batch 107, loss_ca: 1.9509, adv_loss: 0.6178\n",
      "Epoch 39, Batch 108, loss_ca: 2.0104, adv_loss: 0.5801\n",
      "Epoch 39, Batch 109, loss_ca: 1.8740, adv_loss: 0.5250\n",
      "Epoch 39, Batch 110, loss_ca: 1.7548, adv_loss: 0.5126\n",
      "Epoch 39, Batch 111, loss_ca: 1.7288, adv_loss: 0.5036\n",
      "Epoch 39, Batch 112, loss_ca: 1.8270, adv_loss: 0.4994\n",
      "Epoch 39, Batch 113, loss_ca: 1.7110, adv_loss: 0.4973\n",
      "Epoch 39, Batch 114, loss_ca: 1.7170, adv_loss: 0.4905\n",
      "Epoch 39, Batch 115, loss_ca: 1.6777, adv_loss: 0.4847\n",
      "Epoch 39, Batch 116, loss_ca: 1.7170, adv_loss: 0.4707\n",
      "Epoch 39, Batch 117, loss_ca: 1.6931, adv_loss: 0.4807\n",
      "Epoch 39, Batch 118, loss_ca: 1.7174, adv_loss: 0.4751\n",
      "Epoch 39, Batch 119, loss_ca: 1.7117, adv_loss: 0.4807\n",
      "Epoch 39, Batch 120, loss_ca: 1.7433, adv_loss: 0.5052\n",
      "Epoch 39, Batch 121, loss_ca: 2.0670, adv_loss: 0.5533\n",
      "Epoch 39, Batch 122, loss_ca: 1.9366, adv_loss: 0.5601\n",
      "Epoch 39, Batch 123, loss_ca: 1.9076, adv_loss: 0.5742\n",
      "Epoch 39, Batch 124, loss_ca: 1.8575, adv_loss: 0.5689\n",
      "Epoch 39, Batch 125, loss_ca: 1.8396, adv_loss: 0.5702\n",
      "Epoch 39, Batch 126, loss_ca: 1.8060, adv_loss: 0.5263\n",
      "Epoch 39, Batch 127, loss_ca: 1.8342, adv_loss: 0.5537\n",
      "Epoch 39, Batch 128, loss_ca: 1.8618, adv_loss: 0.5387\n",
      "Epoch 39, Batch 129, loss_ca: 1.8692, adv_loss: 0.5551\n",
      "Epoch 39, Batch 130, loss_ca: 1.8197, adv_loss: 0.5601\n",
      "Epoch 39, Batch 131, loss_ca: 1.8336, adv_loss: 0.5191\n",
      "Epoch 39, Batch 132, loss_ca: 1.8858, adv_loss: 0.5640\n",
      "Epoch 39, Batch 133, loss_ca: 1.9453, adv_loss: 0.5879\n",
      "Epoch 39, Batch 134, loss_ca: 1.9010, adv_loss: 0.5775\n",
      "Epoch 39, Batch 135, loss_ca: 1.8917, adv_loss: 0.5709\n",
      "Epoch 39, Batch 136, loss_ca: 1.8297, adv_loss: 0.5541\n",
      "Epoch 39, Batch 137, loss_ca: 1.7403, adv_loss: 0.5162\n",
      "Epoch 39, Batch 138, loss_ca: 1.7069, adv_loss: 0.4911\n",
      "Epoch 39, Batch 139, loss_ca: 1.6957, adv_loss: 0.4973\n",
      "Epoch 39, Batch 140, loss_ca: 1.6707, adv_loss: 0.5015\n",
      "Epoch 39, Batch 141, loss_ca: 1.6646, adv_loss: 0.5025\n",
      "Epoch 39, Batch 142, loss_ca: 1.8726, adv_loss: 0.5693\n",
      "Epoch 39, Batch 143, loss_ca: 1.8149, adv_loss: 0.5734\n",
      "Epoch 39, Batch 144, loss_ca: 1.7246, adv_loss: 0.5734\n",
      "Epoch 39, Batch 145, loss_ca: 1.7089, adv_loss: 0.5424\n",
      "Epoch 39, Batch 146, loss_ca: 1.6791, adv_loss: 0.5198\n",
      "Epoch 39, Batch 147, loss_ca: 1.7401, adv_loss: 0.4983\n",
      "Epoch 39, Batch 148, loss_ca: 1.8193, adv_loss: 0.5005\n",
      "Epoch 39, Batch 149, loss_ca: 1.6781, adv_loss: 0.5001\n",
      "Epoch 39, Batch 150, loss_ca: 1.6945, adv_loss: 0.5048\n",
      "Epoch 39, Batch 151, loss_ca: 1.6374, adv_loss: 0.5189\n",
      "Epoch 39, Batch 152, loss_ca: 1.6390, adv_loss: 0.5221\n",
      "Epoch 39, Batch 153, loss_ca: 1.8133, adv_loss: 0.5558\n",
      "Epoch 39, Batch 154, loss_ca: 1.8318, adv_loss: 0.5389\n",
      "Epoch 39, Batch 155, loss_ca: 1.8268, adv_loss: 0.5431\n",
      "Epoch 39, Batch 156, loss_ca: 1.8095, adv_loss: 0.5456\n",
      "Epoch 39, Batch 157, loss_ca: 1.8335, adv_loss: 0.5262\n",
      "Epoch 39, Batch 158, loss_ca: 1.9772, adv_loss: 0.5731\n",
      "Epoch 39, Batch 159, loss_ca: 2.1842, adv_loss: 0.4990\n",
      "Epoch 39, Batch 160, loss_ca: 2.1016, adv_loss: 0.5338\n",
      "Epoch 39, Batch 161, loss_ca: 1.9012, adv_loss: 0.5748\n",
      "Epoch 39, Batch 162, loss_ca: 2.0018, adv_loss: 0.5067\n",
      "Epoch 39, Batch 163, loss_ca: 1.9695, adv_loss: 0.5345\n",
      "Epoch 39, Batch 164, loss_ca: 2.1655, adv_loss: 0.5885\n",
      "Epoch 39, Batch 165, loss_ca: 2.1487, adv_loss: 0.5678\n",
      "Epoch 39, Batch 166, loss_ca: 2.0613, adv_loss: 0.6049\n",
      "Epoch 39, Batch 167, loss_ca: 2.0247, adv_loss: 0.5477\n",
      "Epoch 39, Batch 168, loss_ca: 2.1470, adv_loss: 0.5613\n",
      "Epoch 39, Batch 169, loss_ca: 1.9972, adv_loss: 0.5706\n",
      "Epoch 39, Batch 170, loss_ca: 1.9400, adv_loss: 0.5051\n",
      "Epoch 39, Batch 171, loss_ca: 1.9809, adv_loss: 0.5130\n",
      "Epoch 39, Batch 172, loss_ca: 2.0308, adv_loss: 0.5264\n",
      "Epoch 39, Batch 173, loss_ca: 1.8781, adv_loss: 0.5135\n",
      "Epoch 39, Batch 174, loss_ca: 1.8767, adv_loss: 0.4832\n",
      "Epoch 39, Batch 175, loss_ca: 1.8876, adv_loss: 0.4698\n",
      "Epoch 39, Batch 176, loss_ca: 1.8535, adv_loss: 0.4666\n",
      "Epoch 39, Batch 177, loss_ca: 1.7407, adv_loss: 0.4954\n",
      "Epoch 39, Batch 178, loss_ca: 1.8520, adv_loss: 0.5421\n",
      "Epoch 39, Batch 179, loss_ca: 1.8143, adv_loss: 0.5438\n",
      "Epoch 39, Batch 180, loss_ca: 1.6781, adv_loss: 0.5171\n",
      "Epoch 39, Batch 181, loss_ca: 1.7100, adv_loss: 0.5151\n",
      "Epoch 39, Batch 182, loss_ca: 1.7000, adv_loss: 0.5065\n",
      "Epoch 39, Batch 183, loss_ca: 1.8194, adv_loss: 0.5217\n",
      "Epoch 39, Batch 184, loss_ca: 1.7669, adv_loss: 0.4789\n",
      "Epoch 39, Batch 185, loss_ca: 1.6852, adv_loss: 0.4869\n",
      "Epoch 39, Batch 186, loss_ca: 1.6870, adv_loss: 0.4638\n",
      "Epoch 39, Batch 187, loss_ca: 1.7119, adv_loss: 0.4717\n",
      "Epoch 39, Batch 188, loss_ca: 1.6533, adv_loss: 0.4734\n",
      "Epoch 39, Batch 189, loss_ca: 1.7784, adv_loss: 0.5032\n",
      "Epoch 39, Batch 190, loss_ca: 1.9214, adv_loss: 0.5748\n",
      "Epoch 39, Batch 191, loss_ca: 1.9278, adv_loss: 0.5775\n",
      "Epoch 39, Batch 192, loss_ca: 2.0323, adv_loss: 0.5908\n",
      "Epoch 39, Batch 193, loss_ca: 1.9129, adv_loss: 0.6257\n",
      "Epoch 39, Batch 194, loss_ca: 1.6921, adv_loss: 0.6153\n",
      "Epoch 39, Batch 195, loss_ca: 1.8650, adv_loss: 0.5673\n",
      "Epoch 39, Batch 196, loss_ca: 2.2049, adv_loss: 0.5783\n",
      "Epoch 39, Batch 197, loss_ca: 1.7927, adv_loss: 0.5160\n",
      "Epoch 39, Batch 198, loss_ca: 1.7140, adv_loss: 0.5330\n",
      "Epoch 39, Batch 199, loss_ca: 1.5679, adv_loss: 0.5610\n",
      "Epoch 39, Batch 200, loss_ca: 1.5801, adv_loss: 0.5212\n",
      "Epoch 39, Batch 201, loss_ca: 1.9724, adv_loss: 0.5295\n",
      "Epoch 39, Batch 202, loss_ca: 2.3189, adv_loss: 0.6169\n",
      "Epoch 39, Batch 203, loss_ca: 2.6659, adv_loss: 0.6218\n",
      "Epoch 39, Batch 204, loss_ca: 1.7525, adv_loss: 0.4871\n",
      "Epoch 39, Batch 205, loss_ca: 1.9595, adv_loss: 0.5008\n",
      "Epoch 39, Batch 206, loss_ca: 1.8879, adv_loss: 0.5044\n",
      "Epoch 39, Batch 207, loss_ca: 1.9994, adv_loss: 0.5694\n",
      "Epoch 39, Batch 208, loss_ca: 1.8890, adv_loss: 0.5487\n",
      "Epoch 39, Batch 209, loss_ca: 1.7689, adv_loss: 0.5319\n",
      "Epoch 39, Batch 210, loss_ca: 1.8489, adv_loss: 0.5339\n",
      "Epoch 39, Batch 211, loss_ca: 1.8242, adv_loss: 0.5591\n",
      "Epoch 39, Batch 212, loss_ca: 1.6295, adv_loss: 0.5412\n",
      "Epoch 39, Batch 213, loss_ca: 1.8974, adv_loss: 0.5622\n",
      "Epoch 39, Batch 214, loss_ca: 1.8730, adv_loss: 0.5436\n",
      "Epoch 39, Batch 215, loss_ca: 1.5816, adv_loss: 0.5801\n",
      "Epoch 39, Batch 216, loss_ca: 1.6832, adv_loss: 0.6211\n",
      "Epoch 39, Batch 217, loss_ca: 1.6208, adv_loss: 0.6072\n",
      "Epoch 39, Batch 218, loss_ca: 1.5920, adv_loss: 0.5934\n",
      "Epoch 39, Batch 219, loss_ca: 2.0897, adv_loss: 0.5580\n",
      "Epoch 39, Batch 220, loss_ca: 1.9587, adv_loss: 0.5502\n",
      "Epoch 39, Batch 221, loss_ca: 1.7991, adv_loss: 0.5026\n",
      "Epoch 39, Batch 222, loss_ca: 1.7436, adv_loss: 0.4983\n",
      "Epoch 39, Batch 223, loss_ca: 1.7293, adv_loss: 0.4957\n",
      "Epoch 39, Batch 224, loss_ca: 1.6640, adv_loss: 0.4877\n",
      "Epoch 39, Batch 225, loss_ca: 1.6429, adv_loss: 0.4783\n",
      "Epoch 39, Batch 226, loss_ca: 1.6760, adv_loss: 0.4824\n",
      "Epoch 39, Batch 227, loss_ca: 1.6937, adv_loss: 0.5219\n",
      "Epoch 39, Batch 228, loss_ca: 1.6546, adv_loss: 0.4841\n",
      "Epoch 40, Batch 40, loss_ca: 1.9046, adv_loss: 0.5718\n",
      "Epoch 40, Batch 41, loss_ca: 1.7313, adv_loss: 0.5002\n",
      "Epoch 40, Batch 42, loss_ca: 1.7602, adv_loss: 0.5329\n",
      "Epoch 40, Batch 43, loss_ca: 1.7631, adv_loss: 0.5486\n",
      "Epoch 40, Batch 44, loss_ca: 1.7523, adv_loss: 0.5394\n",
      "Epoch 40, Batch 45, loss_ca: 1.7419, adv_loss: 0.5543\n",
      "Epoch 40, Batch 46, loss_ca: 1.7415, adv_loss: 0.5604\n",
      "Epoch 40, Batch 47, loss_ca: 2.0204, adv_loss: 0.5759\n",
      "Epoch 40, Batch 48, loss_ca: 1.9486, adv_loss: 0.5883\n",
      "Epoch 40, Batch 49, loss_ca: 1.8349, adv_loss: 0.5649\n",
      "Epoch 40, Batch 50, loss_ca: 1.7933, adv_loss: 0.5700\n",
      "Epoch 40, Batch 51, loss_ca: 1.7458, adv_loss: 0.5539\n",
      "Epoch 40, Batch 52, loss_ca: 1.7401, adv_loss: 0.5343\n",
      "Epoch 40, Batch 53, loss_ca: 1.7609, adv_loss: 0.5639\n",
      "Epoch 40, Batch 54, loss_ca: 1.7611, adv_loss: 0.5559\n",
      "Epoch 40, Batch 55, loss_ca: 1.7511, adv_loss: 0.5521\n",
      "Epoch 40, Batch 56, loss_ca: 1.7259, adv_loss: 0.5566\n",
      "Epoch 40, Batch 57, loss_ca: 1.7011, adv_loss: 0.5554\n",
      "Epoch 40, Batch 58, loss_ca: 1.8112, adv_loss: 0.5464\n",
      "Epoch 40, Batch 59, loss_ca: 1.7167, adv_loss: 0.5306\n",
      "Epoch 40, Batch 60, loss_ca: 1.7268, adv_loss: 0.5122\n",
      "Epoch 40, Batch 61, loss_ca: 1.6934, adv_loss: 0.5266\n",
      "Epoch 40, Batch 62, loss_ca: 1.6965, adv_loss: 0.5405\n",
      "Epoch 40, Batch 63, loss_ca: 1.8229, adv_loss: 0.5868\n",
      "Epoch 40, Batch 64, loss_ca: 1.9046, adv_loss: 0.6049\n",
      "Epoch 40, Batch 65, loss_ca: 1.7612, adv_loss: 0.5487\n",
      "Epoch 40, Batch 66, loss_ca: 1.7514, adv_loss: 0.5304\n",
      "Epoch 40, Batch 67, loss_ca: 1.7036, adv_loss: 0.4912\n",
      "Epoch 40, Batch 68, loss_ca: 1.6898, adv_loss: 0.4777\n",
      "Epoch 40, Batch 69, loss_ca: 1.6806, adv_loss: 0.4675\n",
      "Epoch 40, Batch 70, loss_ca: 1.6732, adv_loss: 0.4758\n",
      "Epoch 40, Batch 71, loss_ca: 1.6332, adv_loss: 0.4824\n",
      "Epoch 40, Batch 72, loss_ca: 1.6960, adv_loss: 0.5019\n",
      "Epoch 40, Batch 73, loss_ca: 1.6345, adv_loss: 0.5043\n",
      "Epoch 40, Batch 74, loss_ca: 1.6462, adv_loss: 0.5070\n",
      "Epoch 40, Batch 75, loss_ca: 1.6017, adv_loss: 0.5064\n",
      "Epoch 40, Batch 76, loss_ca: 1.5919, adv_loss: 0.4916\n",
      "Epoch 40, Batch 77, loss_ca: 1.6163, adv_loss: 0.4855\n",
      "Epoch 40, Batch 78, loss_ca: 1.6901, adv_loss: 0.4813\n",
      "Epoch 40, Batch 79, loss_ca: 1.6904, adv_loss: 0.4815\n",
      "Epoch 40, Batch 80, loss_ca: 1.6714, adv_loss: 0.4842\n",
      "Epoch 40, Batch 81, loss_ca: 1.6343, adv_loss: 0.4894\n",
      "Epoch 40, Batch 82, loss_ca: 1.6356, adv_loss: 0.4894\n",
      "Epoch 40, Batch 83, loss_ca: 1.6611, adv_loss: 0.4919\n",
      "Epoch 40, Batch 84, loss_ca: 1.6621, adv_loss: 0.4998\n",
      "Epoch 40, Batch 85, loss_ca: 1.6643, adv_loss: 0.5004\n",
      "Epoch 40, Batch 86, loss_ca: 1.7231, adv_loss: 0.5145\n",
      "Epoch 40, Batch 87, loss_ca: 1.8097, adv_loss: 0.5421\n",
      "Epoch 40, Batch 88, loss_ca: 1.7948, adv_loss: 0.5464\n",
      "Epoch 40, Batch 89, loss_ca: 1.8013, adv_loss: 0.5636\n",
      "Epoch 40, Batch 90, loss_ca: 1.7170, adv_loss: 0.5195\n",
      "Epoch 40, Batch 91, loss_ca: 1.7970, adv_loss: 0.5463\n",
      "Epoch 40, Batch 92, loss_ca: 1.7566, adv_loss: 0.5256\n",
      "Epoch 40, Batch 93, loss_ca: 1.8141, adv_loss: 0.5334\n",
      "Epoch 40, Batch 94, loss_ca: 1.7437, adv_loss: 0.5213\n",
      "Epoch 40, Batch 95, loss_ca: 1.7240, adv_loss: 0.5040\n",
      "Epoch 40, Batch 96, loss_ca: 1.7709, adv_loss: 0.5163\n",
      "Epoch 40, Batch 97, loss_ca: 1.7683, adv_loss: 0.5185\n",
      "Epoch 40, Batch 98, loss_ca: 1.7346, adv_loss: 0.5068\n",
      "Epoch 40, Batch 99, loss_ca: 1.6623, adv_loss: 0.4902\n",
      "Epoch 40, Batch 100, loss_ca: 1.8095, adv_loss: 0.4920\n",
      "Epoch 40, Batch 101, loss_ca: 1.9046, adv_loss: 0.4981\n",
      "Epoch 40, Batch 102, loss_ca: 1.8662, adv_loss: 0.5077\n",
      "Epoch 40, Batch 103, loss_ca: 1.8679, adv_loss: 0.5239\n",
      "Epoch 40, Batch 104, loss_ca: 1.8221, adv_loss: 0.5122\n",
      "Epoch 40, Batch 105, loss_ca: 1.9822, adv_loss: 0.5925\n",
      "Epoch 40, Batch 106, loss_ca: 1.9527, adv_loss: 0.5959\n",
      "Epoch 40, Batch 107, loss_ca: 1.9592, adv_loss: 0.6289\n",
      "Epoch 40, Batch 108, loss_ca: 1.8452, adv_loss: 0.5609\n",
      "Epoch 40, Batch 109, loss_ca: 1.8182, adv_loss: 0.5228\n",
      "Epoch 40, Batch 110, loss_ca: 1.7303, adv_loss: 0.5106\n",
      "Epoch 40, Batch 111, loss_ca: 1.7370, adv_loss: 0.5228\n",
      "Epoch 40, Batch 112, loss_ca: 1.8093, adv_loss: 0.5280\n",
      "Epoch 40, Batch 113, loss_ca: 1.7390, adv_loss: 0.5349\n",
      "Epoch 40, Batch 114, loss_ca: 1.7829, adv_loss: 0.5322\n",
      "Epoch 40, Batch 115, loss_ca: 1.7183, adv_loss: 0.5179\n",
      "Epoch 40, Batch 116, loss_ca: 1.6828, adv_loss: 0.5212\n",
      "Epoch 40, Batch 117, loss_ca: 1.6616, adv_loss: 0.5081\n",
      "Epoch 40, Batch 118, loss_ca: 1.6897, adv_loss: 0.5119\n",
      "Epoch 40, Batch 119, loss_ca: 1.7071, adv_loss: 0.5084\n",
      "Epoch 40, Batch 120, loss_ca: 1.7091, adv_loss: 0.5073\n",
      "Epoch 40, Batch 121, loss_ca: 2.0477, adv_loss: 0.5501\n",
      "Epoch 40, Batch 122, loss_ca: 1.9399, adv_loss: 0.5297\n",
      "Epoch 40, Batch 123, loss_ca: 1.9211, adv_loss: 0.5352\n",
      "Epoch 40, Batch 124, loss_ca: 1.9075, adv_loss: 0.5323\n",
      "Epoch 40, Batch 125, loss_ca: 1.8646, adv_loss: 0.5312\n",
      "Epoch 40, Batch 126, loss_ca: 1.8553, adv_loss: 0.5130\n",
      "Epoch 40, Batch 127, loss_ca: 1.8443, adv_loss: 0.5108\n",
      "Epoch 40, Batch 128, loss_ca: 1.8852, adv_loss: 0.5180\n",
      "Epoch 40, Batch 129, loss_ca: 1.8572, adv_loss: 0.5193\n",
      "Epoch 40, Batch 130, loss_ca: 1.7843, adv_loss: 0.5395\n",
      "Epoch 40, Batch 131, loss_ca: 1.8255, adv_loss: 0.5416\n",
      "Epoch 40, Batch 132, loss_ca: 1.9075, adv_loss: 0.6085\n",
      "Epoch 40, Batch 133, loss_ca: 1.9219, adv_loss: 0.6100\n",
      "Epoch 40, Batch 134, loss_ca: 1.9122, adv_loss: 0.6278\n",
      "Epoch 40, Batch 135, loss_ca: 1.9076, adv_loss: 0.6116\n",
      "Epoch 40, Batch 136, loss_ca: 1.8956, adv_loss: 0.5750\n",
      "Epoch 40, Batch 137, loss_ca: 1.7834, adv_loss: 0.5098\n",
      "Epoch 40, Batch 138, loss_ca: 1.7665, adv_loss: 0.5083\n",
      "Epoch 40, Batch 139, loss_ca: 1.6442, adv_loss: 0.5059\n",
      "Epoch 40, Batch 140, loss_ca: 1.6274, adv_loss: 0.5042\n",
      "Epoch 40, Batch 141, loss_ca: 1.6296, adv_loss: 0.5058\n",
      "Epoch 40, Batch 142, loss_ca: 1.7507, adv_loss: 0.5060\n",
      "Epoch 40, Batch 143, loss_ca: 1.6837, adv_loss: 0.5242\n",
      "Epoch 40, Batch 144, loss_ca: 1.7182, adv_loss: 0.5284\n",
      "Epoch 40, Batch 145, loss_ca: 1.7443, adv_loss: 0.5320\n",
      "Epoch 40, Batch 146, loss_ca: 1.7566, adv_loss: 0.5543\n",
      "Epoch 40, Batch 147, loss_ca: 1.8038, adv_loss: 0.5715\n",
      "Epoch 40, Batch 148, loss_ca: 1.8500, adv_loss: 0.5563\n",
      "Epoch 40, Batch 149, loss_ca: 1.7099, adv_loss: 0.5470\n",
      "Epoch 40, Batch 150, loss_ca: 1.6707, adv_loss: 0.5314\n",
      "Epoch 40, Batch 151, loss_ca: 1.6127, adv_loss: 0.5320\n",
      "Epoch 40, Batch 152, loss_ca: 1.5824, adv_loss: 0.5179\n",
      "Epoch 40, Batch 153, loss_ca: 1.7629, adv_loss: 0.5187\n",
      "Epoch 40, Batch 154, loss_ca: 1.7729, adv_loss: 0.4978\n",
      "Epoch 40, Batch 155, loss_ca: 1.7725, adv_loss: 0.5193\n",
      "Epoch 40, Batch 156, loss_ca: 1.7867, adv_loss: 0.5135\n",
      "Epoch 40, Batch 157, loss_ca: 1.8764, adv_loss: 0.5357\n",
      "Epoch 40, Batch 158, loss_ca: 1.9680, adv_loss: 0.5686\n",
      "Epoch 40, Batch 159, loss_ca: 2.1546, adv_loss: 0.5068\n",
      "Epoch 40, Batch 160, loss_ca: 2.0150, adv_loss: 0.5434\n",
      "Epoch 40, Batch 161, loss_ca: 1.7993, adv_loss: 0.4967\n",
      "Epoch 40, Batch 162, loss_ca: 1.9659, adv_loss: 0.5425\n",
      "Epoch 40, Batch 163, loss_ca: 1.9162, adv_loss: 0.4921\n",
      "Epoch 40, Batch 164, loss_ca: 2.0338, adv_loss: 0.5119\n",
      "Epoch 40, Batch 165, loss_ca: 2.1150, adv_loss: 0.4974\n",
      "Epoch 40, Batch 166, loss_ca: 1.9697, adv_loss: 0.5467\n",
      "Epoch 40, Batch 167, loss_ca: 1.9030, adv_loss: 0.5404\n",
      "Epoch 40, Batch 168, loss_ca: 2.1837, adv_loss: 0.5807\n",
      "Epoch 40, Batch 169, loss_ca: 2.3308, adv_loss: 0.5919\n",
      "Epoch 40, Batch 170, loss_ca: 2.0091, adv_loss: 0.5340\n",
      "Epoch 40, Batch 171, loss_ca: 2.0918, adv_loss: 0.5697\n",
      "Epoch 40, Batch 172, loss_ca: 2.2575, adv_loss: 0.5787\n",
      "Epoch 40, Batch 173, loss_ca: 1.8534, adv_loss: 0.5198\n",
      "Epoch 40, Batch 174, loss_ca: 1.7666, adv_loss: 0.4936\n",
      "Epoch 40, Batch 175, loss_ca: 1.7319, adv_loss: 0.4553\n",
      "Epoch 40, Batch 176, loss_ca: 1.7414, adv_loss: 0.4506\n",
      "Epoch 40, Batch 177, loss_ca: 1.6873, adv_loss: 0.4606\n",
      "Epoch 40, Batch 178, loss_ca: 1.8137, adv_loss: 0.4711\n",
      "Epoch 40, Batch 179, loss_ca: 1.7824, adv_loss: 0.4925\n",
      "Epoch 40, Batch 180, loss_ca: 1.6667, adv_loss: 0.4898\n",
      "Epoch 40, Batch 181, loss_ca: 1.7001, adv_loss: 0.5035\n",
      "Epoch 40, Batch 182, loss_ca: 1.6604, adv_loss: 0.5052\n",
      "Epoch 40, Batch 183, loss_ca: 1.8023, adv_loss: 0.5295\n",
      "Epoch 40, Batch 184, loss_ca: 1.7223, adv_loss: 0.5126\n",
      "Epoch 40, Batch 185, loss_ca: 1.6855, adv_loss: 0.5224\n",
      "Epoch 40, Batch 186, loss_ca: 1.6346, adv_loss: 0.4876\n",
      "Epoch 40, Batch 187, loss_ca: 1.6830, adv_loss: 0.4788\n",
      "Epoch 40, Batch 188, loss_ca: 1.6743, adv_loss: 0.4757\n",
      "Epoch 40, Batch 189, loss_ca: 1.7379, adv_loss: 0.4791\n",
      "Epoch 40, Batch 190, loss_ca: 1.9919, adv_loss: 0.4682\n",
      "Epoch 40, Batch 191, loss_ca: 1.8852, adv_loss: 0.5157\n",
      "Epoch 40, Batch 192, loss_ca: 1.9578, adv_loss: 0.4798\n",
      "Epoch 40, Batch 193, loss_ca: 1.8198, adv_loss: 0.5383\n",
      "Epoch 40, Batch 194, loss_ca: 1.7135, adv_loss: 0.5558\n",
      "Epoch 40, Batch 195, loss_ca: 1.8391, adv_loss: 0.5506\n",
      "Epoch 40, Batch 196, loss_ca: 2.3039, adv_loss: 0.5733\n",
      "Epoch 40, Batch 197, loss_ca: 2.0173, adv_loss: 0.5286\n",
      "Epoch 40, Batch 198, loss_ca: 1.7363, adv_loss: 0.5192\n",
      "Epoch 40, Batch 199, loss_ca: 1.7579, adv_loss: 0.4773\n",
      "Epoch 40, Batch 200, loss_ca: 1.6050, adv_loss: 0.4829\n",
      "Epoch 40, Batch 201, loss_ca: 2.0182, adv_loss: 0.5332\n",
      "Epoch 40, Batch 202, loss_ca: 2.0476, adv_loss: 0.5517\n",
      "Epoch 40, Batch 203, loss_ca: 2.3375, adv_loss: 0.5634\n",
      "Epoch 40, Batch 204, loss_ca: 2.0424, adv_loss: 0.5166\n",
      "Epoch 40, Batch 205, loss_ca: 2.2315, adv_loss: 0.4957\n",
      "Epoch 40, Batch 206, loss_ca: 2.1028, adv_loss: 0.5300\n",
      "Epoch 40, Batch 207, loss_ca: 2.1264, adv_loss: 0.5909\n",
      "Epoch 40, Batch 208, loss_ca: 2.0393, adv_loss: 0.5884\n",
      "Epoch 40, Batch 209, loss_ca: 1.8314, adv_loss: 0.5061\n",
      "Epoch 40, Batch 210, loss_ca: 1.8575, adv_loss: 0.5185\n",
      "Epoch 40, Batch 211, loss_ca: 1.7044, adv_loss: 0.6138\n",
      "Epoch 40, Batch 212, loss_ca: 1.5227, adv_loss: 0.6186\n",
      "Epoch 40, Batch 213, loss_ca: 1.9510, adv_loss: 0.6246\n",
      "Epoch 40, Batch 214, loss_ca: 1.7234, adv_loss: 0.5858\n",
      "Epoch 40, Batch 215, loss_ca: 1.5472, adv_loss: 0.6279\n",
      "Epoch 40, Batch 216, loss_ca: 1.7152, adv_loss: 0.6367\n",
      "Epoch 40, Batch 217, loss_ca: 1.7370, adv_loss: 0.6417\n",
      "Epoch 40, Batch 218, loss_ca: 1.6357, adv_loss: 0.6158\n",
      "Epoch 40, Batch 219, loss_ca: 1.9949, adv_loss: 0.5450\n",
      "Epoch 40, Batch 220, loss_ca: 1.8635, adv_loss: 0.5427\n",
      "Epoch 40, Batch 221, loss_ca: 1.7507, adv_loss: 0.5298\n",
      "Epoch 40, Batch 222, loss_ca: 1.7233, adv_loss: 0.5304\n",
      "Epoch 40, Batch 223, loss_ca: 1.6939, adv_loss: 0.5109\n",
      "Epoch 40, Batch 224, loss_ca: 1.7136, adv_loss: 0.5091\n",
      "Epoch 40, Batch 225, loss_ca: 1.7124, adv_loss: 0.5011\n",
      "Epoch 40, Batch 226, loss_ca: 1.6693, adv_loss: 0.4882\n",
      "Epoch 40, Batch 227, loss_ca: 1.6787, adv_loss: 0.4977\n",
      "Epoch 40, Batch 228, loss_ca: 1.6512, adv_loss: 0.4962\n",
      "Training complete. Loss log saved to 'loss_log.txt'.\n",
      "Video saved to training_dataset.avi\n"
     ]
    }
   ],
   "source": [
    "from lib.models.smpl import SMPL  # Import SMPL from the VIBE library\n",
    "\n",
    "# Training parameters\n",
    "num_windows = train_sequences.shape[0]  # Total sliding windows\n",
    "print(f\"num_windows: {num_windows}\")\n",
    "\n",
    "batch_size = 32  # Batch size for training\n",
    "num_epochs = 40  # Number of epochs for training\n",
    "learning_rate = 1.5e-4\n",
    "\n",
    "\n",
    "lambda_adv = 0.03\n",
    "lambda_ca = 1.0    # Weight for reconstruction loss\n",
    "\n",
    "# Gradient Penalty parameters\n",
    "lambda_gp = 20  # Weight for the gradient penalty\n",
    "\n",
    "# # SMPL model\n",
    "# model = SMPLModel(device=device)\n",
    "\n",
    "# Initialize optimizer \n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# SMPL model directory\n",
    "SMPL_MODEL_DIR = \"/home/mkeya/VIBE/data/vibe_data\"\n",
    "\n",
    "# Initialize the SMPL model\n",
    "smpl_model = SMPL(SMPL_MODEL_DIR, batch_size=batch_size, create_transl=True).to(device)\n",
    "\n",
    "# collecting losses per epoch\n",
    "epoch_losses = []\n",
    "epoch_losses_test = []\n",
    "\n",
    "# collecting discriminator loss per epoch\n",
    "discriminator_losses = []\n",
    "discriminator_losses_test = []\n",
    "\n",
    "ca_losses = []\n",
    "adv_losses = []\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # total loss\n",
    "    epoch_loss = []\n",
    "    epoch_loss_test = []\n",
    "\n",
    "    # discriminator loss\n",
    "    discriminator_loss = []\n",
    "    discriminator_loss_test = []\n",
    "\n",
    "    ca_loss = []\n",
    "    adver_loss = []\n",
    "\n",
    "    \n",
    "    for i in range(0, num_windows, batch_size):\n",
    "        \n",
    "        batch_start = i\n",
    "        batch_end = min(i + batch_size, num_windows)\n",
    "  \n",
    "        # Extract batch of sliding windows\n",
    "        i = epoch*batch_size +i\n",
    "        \n",
    "        batch_windows = train_sequences[i:i + batch_size].to(device)  # Shape: (32, 20, 96, 96)\n",
    "\n",
    "        # Get the ground truth for the 20th frame of each sliding window in the batch\n",
    "        ground_truth_shape_batch = ground_truth_shape_train[i:i + batch_size].to(device)  # Shape: (32, 10) beta\n",
    "        ground_truth_pose_batch = ground_truth_pose_train[i:i + batch_size].to(device)    # Shape: (32, 72) theta\n",
    "        ground_truth_camera_batch = ground_truth_camera_train[i:i + batch_size].to(device)  # Shape: (32, 3) translation\n",
    "        ground_truth_vertices_batch = ground_truth_vertices_train[i:i + batch_size].to(device)  # Shape: (32, 6890, 3)\n",
    "        ground_truth_joints_batch = ground_truth_coco_train[i:i + batch_size].to(device)  # Shape: (32, 49, 3) # joints\n",
    "\n",
    "        # Ensure batch consistency\n",
    "        if batch_windows.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        beta_student, theta_student, translation_student = student_model(batch_windows)\n",
    "\n",
    "\n",
    "\n",
    "        # Forward pass: SMPL vertices and joints\n",
    "        all_vertices = []\n",
    "        all_joints = []\n",
    "        \n",
    "        \n",
    "        # Loop over the batch (one frame per sequence)\n",
    "        for b in range(batch_size):\n",
    "            # Extract beta, theta, and translation for the current frame\n",
    "            beta = beta_student[b].to(device).to(torch.float32)          # Shape: (10,)\n",
    "            theta = theta_student[b].to(device).to(torch.float32)        # Shape: (72,)\n",
    "            translation = translation_student[b].to(device).to(torch.float32)  # Shape: (3,)           \n",
    "            \n",
    "\n",
    "            # Forward pass through the SMPL model\n",
    "            output = smpl_model(\n",
    "                betas=beta.unsqueeze(0),  # Add batch dimension\n",
    "                body_pose=theta[3:].unsqueeze(0),  # Exclude global orientation\n",
    "                global_orient=translation[:3].unsqueeze(0),  # Use only global orientation\n",
    "                transl=translation.unsqueeze(0)  # Add batch dimension\n",
    "            )\n",
    "        \n",
    "            # Extract vertices and joints\n",
    "            vertices = output.vertices  # Shape: (1, 6890, 3)\n",
    "            joints = output.joints      # Shape: (1, 49, 3)\n",
    "            \n",
    "            # Collect vertices and joints for this frame\n",
    "            all_vertices.append(vertices.cpu().detach().numpy())\n",
    "            all_joints.append(joints.cpu().detach().numpy())\n",
    "        \n",
    "        \n",
    "        # Convert lists to numpy arrays\n",
    "        all_vertices = np.array(all_vertices)  # Shape: (batch_size, 1, num_vertices, 3)\n",
    "        all_joints = np.array(all_joints)      # Shape: (batch_size, 1, 49, 3)\n",
    "        \n",
    "        # print(f\"all_vertices shape before: {all_vertices.shape}\")\n",
    "        # print(f\"all_joints shape before: {all_joints.shape}\")\n",
    "\n",
    "        all_vertices = all_vertices.squeeze(1)  # Shape: (batch_size, num_vertices, 3)\n",
    "        all_joints = all_joints.squeeze(1)      # Shape: (batch_size, 49, 3)\n",
    "\n",
    "        \n",
    "        # Convert SMPL output to torch tensors (if needed)\n",
    "        all_vertices_torch = torch.tensor(all_vertices, device=device)  # Shape: (32, 6890, 3)\n",
    "        all_joints_torch = torch.tensor(all_joints, device=device)      # Shape: (32, 24, 3)\n",
    "\n",
    "            \n",
    "        # unfreeze the discriminator         \n",
    "        discriminator.train()  # Set discriminator to training mode\n",
    "        student_model.eval() # Set the student model to eval mode during discriminator training\n",
    "        \n",
    "        for param in discriminator.parameters():\n",
    "            param.requires_grad = True  # Enable gradients for discriminator\n",
    "\n",
    "            \n",
    "        # Forward pass through discriminator\n",
    "        real_outputs = discriminator(ground_truth_joints_batch.to(device).to(torch.float32))  # Real joints\n",
    "        fake_outputs = discriminator(all_joints_torch.detach().to(device).to(torch.float32))  # Generated joints (detach to prevent grad flow)\n",
    "\n",
    "        # Theoretical discriminator loss (BCE with logits to match formula)\n",
    "        d_real_loss = -torch.mean(torch.log(real_outputs + 1e-8))  # Avoid log(0)\n",
    "        d_fake_loss = -torch.mean(torch.log(1 - fake_outputs + 1e-8))  # Avoid log(0)\n",
    "        d_loss = d_real_loss + d_fake_loss  # Total discriminator loss\n",
    "\n",
    "\n",
    "        # Gradient penalty\n",
    "        batch_size = ground_truth_joints_batch.size(0)\n",
    "        num_joints = ground_truth_joints_batch.size(1)\n",
    "        alpha = torch.rand(batch_size, 1, 1, device=device)  # Add extra dimensions for broadcasting\n",
    "        \n",
    "        interpolates = alpha * ground_truth_joints_batch + (1 - alpha) * all_joints_torch.detach()\n",
    "        interpolates.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass through discriminator for interpolated samples\n",
    "        interpolated_outputs = discriminator(interpolates.to(torch.float32).to(device))\n",
    "        \n",
    "        # Compute gradients of the outputs w.r.t. inputs\n",
    "        grad_outputs = torch.ones_like(interpolated_outputs, device=device)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=interpolated_outputs,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        # Calculate the gradient penalty\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradient_norm = gradients.norm(2, dim=1)  # L2 norm\n",
    "        gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "        \n",
    "        # Add the gradient penalty to the discriminator loss\n",
    "        d_loss += gradient_penalty\n",
    "\n",
    "\n",
    "        discriminator_loss.append(d_loss.item())\n",
    "        \n",
    "        # Backpropagation for discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        # print(f\"Discriminator Loss: {d_loss.item():.4f}\")\n",
    "    \n",
    "        # freeze the discriminator for generator training\n",
    "        discriminator.eval()\n",
    "        for param in discriminator.parameters():\n",
    "            param.requires_grad = False # Freeze the discriminator's weight\n",
    "\n",
    "        # Student model or generator training\n",
    "        student_model.train()  # Set to training mode\n",
    "\n",
    "        \n",
    "        # Calculate the individual MAE losses for each component\n",
    "        beta_loss = mae_loss(beta_student, ground_truth_shape_batch)  # Shape: (32, 10)\n",
    "        camera_loss = mae_loss(translation_student, ground_truth_camera_batch)  # Shape: (32, 3)\n",
    "        pose_loss = mae_loss(theta_student, ground_truth_pose_batch)  # Shape: (32, 72)\n",
    "        vertices_loss = mae_loss(all_vertices_torch, ground_truth_vertices_batch)  # Shape: (32, 6890, 3)\n",
    "        joints_loss = mae_loss(all_joints_torch, ground_truth_joints_batch)  # Shape: (32, 24, 3)\n",
    "        \n",
    "        # Now compute the total loss by summing all individual losses\n",
    "        loss_ca = (torch.mean(beta_loss) + torch.mean(camera_loss) + \n",
    "                      torch.mean(pose_loss) + torch.mean(vertices_loss) + torch.mean(joints_loss))\n",
    "\n",
    "        ca_loss.append(loss_ca.item())\n",
    "\n",
    "        # Adversarial loss for generator with no grad mode\n",
    "        with torch.no_grad(): # Discriminator forward pass should not compute gradients\n",
    "            # adv_loss = -torch.mean(torch.log(discriminator(all_joints_torch.to(torch.float32)) + 1e-8))  # L_adv\n",
    "            adv_loss = torch.mean((discriminator(all_joints_torch.to(torch.float32)) -1) **2)  # L_adv\n",
    "\n",
    "        adver_loss.append(adv_loss.item())\n",
    "\n",
    "        # Log the values of loss_ca and adv_loss\n",
    "        print(f\"Epoch {epoch+1}, Batch {i//batch_size+1}, loss_ca: {loss_ca.item():.4f}, adv_loss: {adv_loss.item():.4f}\")\n",
    "       \n",
    "        # Compute total loss\n",
    "        total_loss = lambda_ca * loss_ca + lambda_adv * adv_loss\n",
    "\n",
    "        # Testing logic for this batch\n",
    "        total_loss_test, d_loss_test = test_model(\n",
    "            smpl_model, student_model, discriminator, test_sequences, ground_truth_shape_test,\n",
    "            ground_truth_pose_test, ground_truth_camera_test, ground_truth_vertices_test,\n",
    "            ground_truth_coco_test, batch_start, batch_end, batch_size, lambda_adv, lambda_ca\n",
    "        )\n",
    "        \n",
    "\n",
    "        if d_loss_test is not None:\n",
    "            discriminator_loss_test.append(d_loss_test.item())\n",
    "        else:\n",
    "            discriminator_loss_test.append(0.0) \n",
    "        \n",
    "\n",
    "        # BackpropagationL\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(total_loss.item())\n",
    "        \n",
    "        if total_loss_test is not None:\n",
    "            epoch_loss_test.append(total_loss_test)\n",
    "        else:\n",
    "            epoch_loss_test.append(0.0)\n",
    "\n",
    "\n",
    "    #avg total loss per epoch\n",
    "    avg_loss_per_epoch = sum(epoch_loss)/len(epoch_loss)\n",
    "    epoch_losses.append(avg_loss_per_epoch)\n",
    "    \n",
    "\n",
    "    # Filter out 0.0 values\n",
    "    filtered_epoch_loss_test = [loss for loss in epoch_loss_test if loss != 0.0]\n",
    "    \n",
    "    # Calculate average if the filtered list is not empty\n",
    "    if filtered_epoch_loss_test:\n",
    "        avg_loss_per_epoch_test = sum(filtered_epoch_loss_test) / len(filtered_epoch_loss_test)\n",
    "    else:\n",
    "        avg_loss_per_epoch_test = 0.0  # Handle case when all values are 0.0 or the list is empty\n",
    "    \n",
    "    epoch_losses_test.append(avg_loss_per_epoch_test)\n",
    "\n",
    "    \n",
    "    #avg discriminator loss per epoch\n",
    "    if len(discriminator_loss) > 0:\n",
    "        avg_discriminator_loss_per_epoch = sum(discriminator_loss)/len(discriminator_loss)\n",
    "        discriminator_losses.append(avg_discriminator_loss_per_epoch)\n",
    "        \n",
    "    else:\n",
    "        discriminator_losses.append(None)\n",
    "\n",
    "\n",
    "    # Filter out 0.0 values\n",
    "    filtered_discriminator_loss_test = [loss for loss in discriminator_loss_test if loss != 0.0]\n",
    "    \n",
    "    # Calculate average if the filtered list is not empty\n",
    "    if filtered_discriminator_loss_test:\n",
    "        avg_discriminator_loss_per_epoch_test = sum(filtered_discriminator_loss_test) / len(filtered_discriminator_loss_test)\n",
    "    else:\n",
    "        avg_discriminator_loss_per_epoch_test = 0.0  # Handle case when all values are 0.0 or the list is empty\n",
    "    \n",
    "    discriminator_losses_test.append(avg_discriminator_loss_per_epoch_test)\n",
    "\n",
    "    ca_loss_per_epoch = sum(ca_loss)/len(ca_loss)\n",
    "    ca_losses.append(ca_loss_per_epoch)\n",
    "\n",
    "    adver_loss_per_epoch = sum(adver_loss)/len(adver_loss)\n",
    "    adv_losses.append(adver_loss_per_epoch)\n",
    "\n",
    "\n",
    "\n",
    "# Save loss trend to file\n",
    "with open(\"loss_log.txt\", \"w\") as f:\n",
    "    for epoch, (loss, d_loss, loss_test, d_loss_test, adv_l) in enumerate(zip(epoch_losses, discriminator_losses, epoch_losses_test, discriminator_losses_test, adv_losses)):\n",
    "        d_loss_str = f\"{d_loss:.4f}\" if d_loss is not None else \"N/A\"\n",
    "        f.write(f\"Epoch {epoch+1}, Total Loss: {loss:.4f}, Discriminator Loss: {d_loss_str}, Test Loss: {loss_test:.4f}, Test Discrimination Loss: {d_loss_test:.4f}, Adv Loss: {adv_l:.4f}\\n\")\n",
    "\n",
    "print(\"Training complete. Loss log saved to 'loss_log.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "51eed5f8-c268-4175-83f5-19176af5d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store epochs and losses\n",
    "epochs = []\n",
    "adv_loss_plot = []\n",
    "total_losses = []\n",
    "discriminator_losses = []\n",
    "test_losses = []\n",
    "test_discriminator_losses = []\n",
    "\n",
    "# Read the log file and parse the values\n",
    "with open(\"loss_log.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\", \")\n",
    "        if len(parts) == 6:  # Ensure proper formatting\n",
    "            \n",
    "            epoch = int(parts[0].split(\" \")[1])  # Extract epoch number\n",
    "            total_loss = float(parts[1].split(\": \")[1])  # Extract Total Loss\n",
    "            \n",
    "            # Extract and handle Discriminator Loss\n",
    "            d_loss_str = parts[2].split(\": \")[1]\n",
    "            d_loss = float(d_loss_str) if d_loss_str != \"N/A\" else None\n",
    "            \n",
    "            # Extract Test Loss\n",
    "            test_loss = float(parts[3].split(\": \")[1])  # Extract Test Loss\n",
    "            \n",
    "            # Extract and handle Test Discriminator Loss\n",
    "            d_loss_test_str = parts[4].split(\": \")[1]\n",
    "            test_d_loss = float(d_loss_test_str) if d_loss_test_str != \"N/A\" else None\n",
    "            \n",
    "            # Extract Adv Loss\n",
    "            adv_loss = float(parts[5].split(\": \")[1])\n",
    "\n",
    "            # Append values to lists\n",
    "            epochs.append(epoch)\n",
    "            total_losses.append(total_loss)\n",
    "            discriminator_losses.append(d_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            test_discriminator_losses.append(test_d_loss)\n",
    "            adv_loss_plot.append(adv_loss)\n",
    "\n",
    "# Plot the losses\n",
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot total loss\n",
    "# ax1.plot(epochs, total_losses, label=\"Generator Loss\", color=\"blue\", marker=\"o\")\n",
    "\n",
    "ax1.plot(epochs, total_losses, label=f\"Generator Loss, LR = {learning_rate}, L_ca = {lambda_ca}, l_adv = {lambda_adv}\", color=\"blue\", marker=\"o\")\n",
    "\n",
    "# Plot discriminator loss, skipping epochs with no discriminator loss\n",
    "valid_d_losses = [(epoch, d_loss) for epoch, d_loss in zip(epochs, discriminator_losses) if d_loss is not None]\n",
    "if valid_d_losses:\n",
    "    d_epochs, d_losses = zip(*valid_d_losses)\n",
    "    ax1.plot(d_epochs, d_losses, label=\"Discriminator Loss\", color=\"red\", marker=\"o\")\n",
    "\n",
    "# Plot test loss\n",
    "ax1.plot(epochs, test_losses, label=\"Test Loss\", color=\"green\", linestyle=\"--\", marker=\"x\")\n",
    "\n",
    "# Plot test discriminator loss, skipping epochs with no discriminator test loss\n",
    "valid_test_d_losses = [(epoch, d_loss) for epoch, d_loss in zip(epochs, test_discriminator_losses) if d_loss is not None]\n",
    "if valid_test_d_losses:\n",
    "    d_test_epochs, d_test_losses = zip(*valid_test_d_losses)\n",
    "    ax1.plot(d_test_epochs, d_test_losses, label=\"Test Discrimination Loss\", color=\"purple\", linestyle=\"--\", marker=\"x\")\n",
    "\n",
    "# Plot adv loss\n",
    "ax1.plot(epochs, adv_loss_plot, label=\"Adv Loss\", color=\"black\", linestyle=\"--\", marker=\"x\")\n",
    "\n",
    "# Add labels, title, and legend for the loss curves\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(f\"Losses Over {len(epochs)} Epochs\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Save the plot as an image (optional)\n",
    "plt.savefig(\"loss_dis_train_first_5epoch_test_dataset.png\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
